- date: "2020-09-07"
  version: "1"
  last_updated: "2020-09-07"
  expired: "false"
  valid: "yes"
  valid_date: "2025-07-28"
  name: "MMLU (Massive Multitask Language Understanding)"
  url: "https://huggingface.co/datasets/cais/mmlu"
  doi: "10.48550/arXiv.2009.03300"
  domain:
  - Computational Science & AI
  focus: "Academic knowledge and reasoning across 57 subjects"
  keywords:
  - "multitask"
  - "multiple-choice"
  - "zero-shot"
  - "few-shot"
  - "knowledge probing"
  summary: |
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  licensing: "MIT License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "General reasoning, subject-matter understanding"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4o"
  - "Gemini 1.5 Pro"
  - "o1"
  - "DeepSeek-R1"
  ml_motif:
  - "Reasoning & Generalization"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "1"
  notes: "Good"
  contact:
    name: "Dan Hendrycks"
    email: "dan (at) safe.ai"
  cite:
  - |
    @misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav},
      journal={arXiv preprint arXiv:2009.03300},
      year={2021},
      url={https://arxiv.org/abs/2009.03300}
    }
  datasets:
    links:
    - name: "Huggingface Dataset"
      url: "https://huggingface.co/datasets/cais/mmlu"
  results:
    links:
    - name: "Measuring Massive Multitask Language Understanding - Test Leaderboard"
      url: "https://github.com/hendrycks/test?tab=readme-ov-file#test-leaderboard"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 2
      reason: |
        Some code is available on github to reproduce results via OpenAI API, but not well documented
    specification:
      rating: 4
      reason: |
        No system constraints
    dataset:
      rating: 5
      reason: |
        Meets all FAIR principles and properly versioned.
    metrics:
      rating: 5
      reason: |
        Fully defined, represents a solution's performance.
    reference_solution:
      rating: 2
      reason: |
        Reference models are available (i.e. GPT-3), but are not trainable or publicly documented
    documentation:
      rating: 5
      reason: |
        Well-explained in a provided paper.

- date: "2023-07-19"
  version: "1"
  last_updated: "2023-07-19"
  expired: "false"
  valid: "yes"
  valid_date: "2023-07-19"
  name: "ClimateLearn - Weather Forcasting"
  url: "https://arxiv.org/abs/2307.01909"
  doi: "10.48550/arXiv.2307.01909"
  domain:
  - Climate & Earth Science
  focus: "ML for weather and climate modeling"
  keywords:
  - "medium-range forecasting"
  - "ERA5"
  - "data-driven"
  summary: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  licensing: "CC-BY-4.0"
  task_types:
  - "Forecasting"
  ai_capability_measured:
  - "Global weather prediction (3-5 days)"
  metrics:
  - "RMSE"
  - "Anomaly correlation"
  models:
  - "CNN baselines"
  - "ResNet variants"
  ml_motif:
  - Sequence Prediction/Forecasting
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Multiple baseline models provided"
  notes: "Includes physical and ML baselines."
  contact:
    name: "Jason Jewik"
    email: "jason.jewik@ucla.edu"
  cite:
  - |
    @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
      title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
      author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
      year={2023}, eprint={2307.01909}, 
      archivePrefix={arXiv}, 
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.01909}
    }
  datasets:
    links:
    - name: "ClimateLearn GitHub Repository (data loaders and processing)"
      url: "https://github.com/aditya-grover/climate-learn"
  results:
    links:
    - name: "ClimateLearn Paper (results section)"
      url: "https://arxiv.org/abs/2307.01909"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Quickstart notebook makes for easy usage
    specification:
      rating: 5
      reason: |
        Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    dataset:
      rating: 5
      reason: |
        Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    metrics:
      rating: 5
      reason: |
        ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    reference_solution:
      rating: 5
      reason: |
        A Quickstart notebook is provided that uses ResNet as a baseline model
    documentation:
      rating: 5
      reason: |
        Explained in the benchmark's paper. 

- date: "2023-07-19"
  version: "1"
  last_updated: "2023-07-19"
  expired: "false"
  valid: "yes"
  valid_date: "2023-07-19"
  name: "ClimateLearn - Downscaling"
  url: "https://arxiv.org/abs/2307.01909"
  doi: "10.48550/arXiv.2307.01909"
  domain:
  - Climate & Earth Science
  focus: "ML for weather and climate modeling"
  keywords:
  - "medium-range forecasting"
  - "ERA5"
  - "data-driven"
  summary: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  licensing: "CC-BY-4.0"
  task_types:
  - "Forecasting"
  ai_capability_measured:
  - "Global weather prediction (3-5 days)"
  metrics:
  - "RMSE"
  - "Anomaly correlation"
  models:
  - "CNN baselines"
  - "ResNet variants"
  ml_motif:
  - Regression
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Multiple baseline models provided"
  notes: "Includes physical and ML baselines."
  contact:
    name: "Jason Jewik"
    email: "jason.jewik@ucla.edu"
  cite:
  - |
    @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
      title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
      author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
      year={2023}, eprint={2307.01909}, 
      archivePrefix={arXiv}, 
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.01909}
    }
  datasets:
    links:
    - name: "ClimateLearn GitHub Repository (data loaders and processing)"
      url: "https://github.com/aditya-grover/climate-learn"
  results:
    links:
    - name: "ClimateLearn Paper (results section)"
      url: "https://arxiv.org/abs/2307.01909"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Quickstart notebook makes for easy usage
    specification:
      rating: 5
      reason: |
        Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    dataset:
      rating: 5
      reason: |
        Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    metrics:
      rating: 5
      reason: |
        ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    reference_solution:
      rating: 5
      reason: |
        A Quickstart notebook is provided that uses ResNet as a baseline model
    documentation:
      rating: 5
      reason: |
        Explained in the benchmark's paper. 

- date: "2023-07-19"
  version: "1"
  last_updated: "2023-07-19"
  expired: "false"
  valid: "yes"
  valid_date: "2023-07-19"
  name: "ClimateLearn - Climate Projection"
  url: "https://arxiv.org/abs/2307.01909"
  doi: "10.48550/arXiv.2307.01909"
  domain:
  - Climate & Earth Science
  focus: "ML for weather and climate modeling"
  keywords:
  - "medium-range forecasting"
  - "ERA5"
  - "data-driven"
  summary: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  licensing: "CC-BY-4.0"
  task_types:
  - "Forecasting"
  ai_capability_measured:
  - "Global weather prediction (3-5 days)"
  metrics:
  - "RMSE"
  - "Anomaly correlation"
  models:
  - "CNN baselines"
  - "ResNet variants"
  ml_motif:
  - Regression
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Multiple baseline models provided"
  notes: "Includes physical and ML baselines."
  contact:
    name: "Jason Jewik"
    email: "jason.jewik@ucla.edu"
  cite:
  - |
    @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
      title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
      author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
      year={2023}, eprint={2307.01909}, 
      archivePrefix={arXiv}, 
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.01909}
    }
  datasets:
    links:
    - name: "ClimateLearn GitHub Repository (data loaders and processing)"
      url: "https://github.com/aditya-grover/climate-learn"
  results:
    links:
    - name: "ClimateLearn Paper (results section)"
      url: "https://arxiv.org/abs/2307.01909"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Quickstart notebook makes for easy usage
    specification:
      rating: 5
      reason: |
        Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    dataset:
      rating: 5
      reason: |
        Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    metrics:
      rating: 5
      reason: |
        ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    reference_solution:
      rating: 5
      reason: |
        A Quickstart notebook is provided that uses ResNet as a baseline model
    documentation:
      rating: 5
      reason: |
        Explained in the benchmark's paper. 

- date: "2024-10-01"
  version: "1"
  last_updated: "2024-10-01"
  expired: "false"
  valid: "yes"
  valid_date: "2024-10-01"
  name: "CFDBench (Fluid Dynamics)"
  url: "https://arxiv.org/abs/2310.05963"
  doi: "10.48550/arXiv.2310.05963"
  domain:
  - Mathematics
  focus: "Neural operator surrogate modeling"
  keywords:
  - "neural operators"
  - "CFD"
  - "FNO"
  - "DeepONet"
  summary: |
    CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
    assessing neural operators' ability to generalize to unseen PDE parameters and domains.
  licensing: "CC-BY-4.0"
  task_types:
  - "Surrogate modeling"
  ai_capability_measured:
  - "Generalization of neural operators for PDEs"
  metrics:
  - "L2 error"
  - "MAE"
  models:
  - "FNO"
  - "DeepONet"
  - "U-Net"
  ml_motif:
  - Regression
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Numerous, as it's a benchmark for ML models"
  notes: "302K frames across 739 cases"
  contact:
    name: "Yining Luo"
    email: "yining.luo@mail.utoronto.ca"
  cite:
  - |
    @misc{luo2024cfdbenchlargescalebenchmarkmachine,
      title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},
      author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},
      year={2024},
      url={https://arxiv.org/abs/2310.05963}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation
    specification:
      rating: 0
      reason: |
        Not listed
    dataset:
      rating: 0
      reason: |
        Not given
    metrics:
      rating: 5
      reason: |
        Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.
    reference_solution:
      rating: 5
      reason: |
        Baseline models like FNO and DeepONet are implemented, hardware specified.
    documentation:
      rating: 5
      reason: |
        Associated paper gives all necessary information.

- date: "2023-04-23"
  version: "1"
  last_updated: "2023-04-23"
  expired: "false"
  valid: "yes"
  valid_date: "2023-04-23"
  name: "SatImgNet"
  url: "https://satinbenchmark.github.io/"
  doi: "10.48550/arXiv.2304.11619"
  domain:
  - Climate & Earth Science
  focus: "Satellite imagery classification"
  keywords:
  - "land-use"
  - "zero-shot"
  - "multi-task"
  summary: |
    SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
    imagery classification datasets evaluating zero-shot transfer of vision-language models
    across diverse remote sensing tasks.
  licensing: "CC-BY-4.0"
  task_types:
  - "Image classification"
  ai_capability_measured:
  - "Zero-shot land-use classification"
  metrics:
  - "Accuracy"
  models:
  - "CLIP"
  - "BLIP"
  - "ALBEF"
  ml_motif:
  - Multimodal Reasoning
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Numerous, evaluated via leaderboard"
  notes: "Public leaderboard available"
  contact:
    name: "Jonathan Roberts"
    email: "j.roberts@cs.ox.ac.uk"
  cite:
  - |
    @article{roberts2023satin,
    author = "Roberts, Jonathan and Han, Kai and Albanie, Samuel",
    title = "Satin: A multi-task metadataset for classifying satellite imagery using vision-language models",
    year = "2023",
    month = "3",
    journal = "ICCV Workshop: Towards the Next Generation of Computer Vision Datasets",
    doi = "10.48550/arXiv.2304.11619"
    }
  datasets:
    links:
    - name: "SatImgNet on Hugging Face"
      url: "https://huggingface.co/datasets/jonathan-roberts1/SATIN"
  results:
    links:
    - name: "SatImgNet Leaderboard"
      url: "https://satinbenchmark.github.io/_pages/leaderboard/"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        No scripts or environment information provided
    specification:
      rating: 4
      reason: |
        Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.
    dataset:
      rating: 5
      reason: |
        Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.
    metrics:
      rating: 5
      reason: |
        Accuracy of classification is an appropriate metric
    reference_solution:
      rating: 4
      reason: |
        Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified
    documentation:
      rating: 5
      reason: |
        Paper provides all required information


- date: "2022-02-22"
  version: "1"
  last_updated: "2022-02-22"
  expired: "false"
  valid: "yes"
  valid_date: "2022-02-22"
  name: "Quantum Computing Benchmarks (QML)"
  url: "https://github.com/XanaduAI/qml-benchmarks"
  doi: "10.48550/arXiv.2403.07059"
  domain:
  - Computational Science & AI
  focus: "Quantum algorithm performance evaluation"
  keywords:
  - "quantum circuits"
  - "state preparation"
  - "error correction"
  summary: |
    A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
    preparation, circuit optimization, and error correction across multiple platforms.
  licensing: "Apache-2.0"
  task_types:
  - "Circuit benchmarking"
  - "State classification"
  ai_capability_measured:
  - "Quantum algorithm performance and fidelity"
  metrics:
  - "Fidelity"
  - "Success probability"
  models:
  - "IBM Q"
  - "IonQ"
  - "AQT@LBNL"
  ml_motif:
  - Classification
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Varies per benchmark"
  notes: "Hardware-agnostic, application-level metrics. The citation may not be correct."
  contact:
    name: "Xanadu AI"
    email: "support@xanadu.ai"
  cite:
  - |
    @misc{bowles2024betterclassicalsubtleart,
      title={Better than classical? The subtle art of benchmarking quantum machine learning models}, 
      author={Joseph Bowles and Shahnawaz Ahmed and Maria Schuld},
      year={2024},
      eprint={2403.07059},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2403.07059}, 
    }
  datasets:
    links:
    - name: "PennyLane QML Benchmarks Datasets"
      url: "https://pennylane.ai/datasets/collection/qml-benchmarks"
  results:
    links:
    - name: "QML Benchmarks GitHub Repository (Results section)"
      url: "https://github.com/XanaduAI/qml-benchmarks#results-and-leaderboards"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Software is built upon multiple common frameworks for simulation, training, and benchmarking workflows.
    specification:
      rating: 3
      reason: |
        No system constraints. Task clarity and dataset format are not clearly specified.
    dataset:
      rating: 4
      reason: |
        Datasets are accessible, but not split.
    metrics:
      rating: 3
      reason: |
        Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured.
    reference_solution:
      rating: 0
      reason: |
        Not provided
    documentation:
      rating: 5
      reason: |
        Paper is available with all required information. 


- date: "2020-10-20"
  version: "1"
  last_updated: "2020-10-20"
  expired: "false"
  valid: "yes"
  valid_date: "2020-10-20"
  name: "OCP (Open Catalyst Project)"
  url: "https://opencatalystproject.org/"
  doi: "unknown"
  domain:
  - Chemistry
  - Materials Science
  focus: "Catalyst adsorption energy prediction"
  keywords:
  - "DFT relaxations"
  - "adsorption energy"
  - "graph neural networks"
  summary: |
    The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate 
    relaxation datasets, challenging ML models to predict energies and forces for 
    renewable energy applications.
  licensing: "OCP Terms of Use"
  task_types:
  - "Energy prediction"
  - "Force prediction"
  ai_capability_measured:
  - "Prediction of adsorption energies and forces"
  metrics:
  - "MAE (energy)"
  - "MAE (force)"
  models:
  - "CGCNN"
  - "SchNet"
  - "DimeNet++"
  - "GemNet-OC"
  ml_motif:
  - Regression
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Public leaderboards; active community development"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @article{chanussot2021oc20,
      title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},
      author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
      journal   = {ACS Catalysis},
      volume    = {11},
      number    = {10},
      pages     = {6059--6072},
      year      = {2021},
      doi       = {10.1021/acscatal.0c04525},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}
    }
  - |
    @article{tran2023oc22,
      title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
      author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, F\'{e}lix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
      journal   = {ACS Catalysis},
      volume    = {13},
      number    = {5},
      pages     = {3066--3084},
      year      = {2023},
      doi       = {10.1021/acscatal.2c05426},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}
    }
  datasets:
    links:
    - name: "OCP Dataset"
      url: "https://fair-chem.github.io/catalysts/datasets/summary"
  results:
    links:
    - name: "OCP Pretrained Models"
      url: "https://fair-chem.github.io/catalysts/models.html"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Data provided in Github links
    specification:
      rating: 5
      reason: |
        Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.
    dataset:
      rating: 5
      reason: |
        Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.
    metrics:
      rating: 5
      reason: |
        MAE (energy and force) are standard and reproducible.
    reference_solution:
      rating: 4
      reason: |
        Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed.
    documentation:
      rating: 1
      reason: |
        Paper exists, but content is behind a paywall.


- date: "2020-09-28"
  version: "1"
  last_updated: "2020-09-28"
  expired: "false"
  valid: "yes"
  valid_date: "2020-09-28"
  name: "MedQA"
  url: "https://arxiv.org/abs/2009.13081"
  doi: "10.48550/arXiv.2009.13081"
  domain:
  - Biology & Medicine
  focus: "Medical board exam QA"
  keywords:
  - "USMLE"
  - "diagnostic QA"
  - "medical knowledge"
  - "multilingual"
  summary: |
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  licensing: "Under Association for the Advancement of Artificial Intelligence"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Medical diagnosis and knowledge retrieval"
  metrics:
  - "Accuracy"
  models:
  - "Neural reader"
  - "Retrieval-based QA systems"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Multilingual (English, Simplified and Traditional Chinese)"
  contact:
    name: "Di Jin"
    email: "jindi15@mit.edu"
  cite:
  - |
    @misc{jin2020diseasedoespatienthave,
        archiveprefix = {arXiv},
        author        = {Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
        eprint        = {2009.13081},
        primaryclass  = {cs.CL},
        title         = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
        url           = {https://arxiv.org/abs/2009.13081},
        year          = {2020}
      }
  datasets:
    links:
    - name: "Github"
      url: "https://github.com/jind11/MedQA"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        All code available on the github
    specification:
      rating: 3
      reason: |
        Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.
    dataset:
      rating: 4
      reason: |
        Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.
    metrics:
      rating: 5
      reason: |
        Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.
    reference_solution:
      rating: 0
      reason: |
        No reference solution mentioned.
    documentation:
      rating: 4
      reason: |
        Paper is available. Evaluation criteria are not mentioned.

- date: "2011-10-01"
  version: "1"
  last_updated: "2011-10-01"
  expired: "false"
  valid: "yes"
  valid_date: "2011-10-01"
  name: "Materials Project"
  url: "https://materialsproject.org/"
  doi: "unknown"
  domain:
  - Materials Science
  focus: "DFT-based property prediction"
  keywords:
  - "DFT"
  - "materials genome"
  - "high-throughput"
  summary: |
    The Materials Project provides an open-access database of computed properties for
    inorganic materials via high-throughput density functional theory (DFT), accelerating 
    materials discovery.
  licensing: "https://next-gen.materialsproject.org/about/terms"
  task_types:
  - "Property prediction"
  ai_capability_measured:
  - "Prediction of inorganic material properties"
  metrics:
  - "MAE"
  - "R^2"
  models:
  - "Automatminer"
  - "Crystal Graph Neural Networks"
  ml_motif:
  - Regression
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Core component of the Materials Genome Initiative"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @article{jain2013materials,
      title={The Materials Project: A materials genome approach},
      author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},
      journal={APL Materials},
      volume    = {1},
      number    = {1},
      year={2013},
      doi       = {10.1063/1.4812323},
      url={https://materialsproject.org/}
    }
  datasets:
    links:
    - name: "Materials Project Catalysis Explorer"
      url: "https://next-gen.materialsproject.org/catalysis"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        No instructions available
    specification:
      rating: 1.5
      reason: |
        The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.
    dataset:
      rating: 3
      reason: |
        API key required to access data. No predefined splits.
    metrics:
      rating: 5
      reason: |
        Uses numerical metrics like MAE and $R^2$
    reference_solution:
      rating: 2
      reason: |
        Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed.
    documentation:
      rating: 0
      reason: |
        No explanations or paper provided


- date: "2023-11-20"
  version: "1"
  last_updated: "2023-11-20"
  expired: "false"
  valid: "yes"
  valid_date: "2023-11-20"
  name: "GPQA Diamond"
  url: "https://arxiv.org/abs/2311.12022"
  doi: "10.48550/arXiv.2311.12022"
  domain:
  - Biology & Medicine
  - Chemistry
  - High Energy Physics
  focus: "Graduate-level scientific reasoning"
  keywords:
  - "Google-proof"
  - "graduate-level"
  - "science QA"
  - "chemistry"
  - "physics"
  summary: |
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is Google-proof - experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  licensing: "unknown"
  task_types:
  - "Multiple choice"
  - "Multi-step QA"
  ai_capability_measured:
  - "Scientific reasoning, deep knowledge"
  metrics:
  - "Accuracy"
  models:
  - "o1"
  - "DeepSeek-R1"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Julian Michael"
    email: "julianjm@nyu.edu"
  cite:
  - |
    @misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
      author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},
      year={2023},
      url={https://arxiv.org/abs/2311.12022}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Python version and requirements specified on Github site
    specification:
      rating: 2
      reason: |
        No system constraints or I/O specified
    dataset:
      rating: 5
      reason: |
        Easily able to access dataset. Comes with predefined splits as mentioned in the paper
    metrics:
      rating: 5
      reason: |
        Each question has a correct answer, representing the tested model's performance.
    reference_solution:
      rating: 1
      reason: |
        Common models such as GPT-3.5 were compared. They are not open and don't provide requirements
    documentation:
      rating: 5
      reason: |
        All information is listed in the associated paper

- date: "2018-03-14"
  version: "1"
  last_updated: "2018-03-14"
  expired: "false"
  valid: "yes"
  valid_date: "2018-03-14"
  name: "ARC-Challenge (Advanced Reasoning Challenge)"
  url: "https://allenai.org/data/arc"
  doi: "10.48550/arXiv.1803.05457"
  domain:
  - Computational Science & AI
  focus: "Grade-school science with reasoning emphasis"
  keywords:
  - "grade-school"
  - "science QA"
  - "challenge set"
  - "reasoning"
  summary: |
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  licensing: "Apache 2.0 License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Commonsense and scientific reasoning"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4"
  - "Claude"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @article{allenai:arc,
      author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
                    Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
      journal   = {arXiv:1803.05457v1},
      year      = {2018},
      doi       = {10.48550/arXiv.1803.05457}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/allenai/ai2_arc"
  results:
    links:
    - name: "ARC-Solvers"
      url: "https://github.com/allenai/arc-solvers"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Code is available and well documented for evaluation.
    specification:
      rating: 4
      reason: |
        Task is clear and inputs/outputs are provided along with format on dataset card.
    dataset:
      rating: 5
      reason: |
        Data accessible, offers instructions on how to download the data via CLI tools. Splits provided on Huggingface
    metrics:
      rating: 5
      reason: |
        All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 5
      reason: |
        Reference solution is available and containerized
    documentation:
      rating: 5
      reason: |
        Explains all necessary information inside a paper




- date: "2024-11-07"
  version: "1"
  last_updated: "2024-11-07"
  expired: "false"
  valid: "yes"
  valid_date: "2024-11-07"
  name: "FrontierMath"
  url: "https://arxiv.org/abs/2411.04872"
  doi: "10.48550/arXiv.2411.04872"
  domain:
  - Mathematics
  focus: "Challenging advanced mathematical reasoning"
  keywords:
  - "symbolic reasoning"
  - "number theory"
  - "algebraic geometry"
  - "category theory"
  summary: |
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs 
    ability to solve problems requiring deep abstract reasoning.
  licensing: "unknown"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Symbolic and abstract mathematical reasoning"
  metrics:
  - "Accuracy"
  models:
  - unknown
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "More information available at https://epoch.ai/frontiermath/about"
  contact:
    name: "FrontierMath team"
    email: "math_evals@epochai.org"
  cite:
  - |
    @misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
      archiveprefix = {arXiv},
      author        = {Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli J\"{a}rviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},
      eprint        = {2411.04872},
      primaryclass  = {cs.AI},
      title         = {FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
      url           = {https://arxiv.org/abs/2411.04872},
      year          = {2024}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "No"
    benchmark_ready: "No"
  ratings:
    software:
      rating: 0
      reason: |
        No publicaly available code to run the benchmark
    specification:
      rating: 3
      reason: |
        Well-specified process for asking questions and receiving answers. No software or hardware constraints
    dataset:
      rating: 0
      reason: |
        Only samples of dataset exist, not publicly available
    metrics:
      rating: 5
      reason: |
        All questions in the dataset have a correct answer
    reference_solution:
      rating: 2
      reason: |
        Displays result of leading models on the benchmark, but none are trainable or list constraints
    documentation:
      rating: 5
      reason: |
        All necessary information is in the paper and website

- date: "2024-07-18"
  version: "1"
  last_updated: "2024-07-18"
  expired: "false"
  valid: "yes"
  valid_date: "2024-07-18"
  name: "SciCode"
  url: "https://scicode-bench.github.io/"
  doi: "10.48550/arXiv.2407.13168"
  domain:
  - Computational Science & AI
  focus: "Scientific code generation and problem solving"
  keywords:
  - "code synthesis"
  - "scientific computing"
  - "programming benchmark"
  summary: |
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  licensing: "unknown"
  task_types:
  - "Coding"
  ai_capability_measured:
  - "Program synthesis, scientific computing"
  metrics:
  - "Solve rate (%)"
  models:
  - "Claude3.5-Sonnet"
  ml_motif:
  - Generative
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "unknown"
  notes: "Good"
  contact:
    name: "Minyang Tian"
    email: "mtian8@illinois.edu"
  cite:
  - |
    @misc{tian2024scicoderesearchcodingbenchmark,
      archiveprefix = {arXiv},
      author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
      eprint        = {2407.13168},
      primaryclass  = {cs.AI},
      title         = {SciCode: A Research Coding Benchmark Curated by Scientists},
      url           = {https://arxiv.org/abs/2407.13168},
      year          = {2024}
    }
  datasets:
    links:
    - name: "SciCode on Huggingface"
      url: "https://huggingface.co/datasets/SciCode1/SciCode"
  results:
    links:
    - name: "SciCode Learderboard"
      url: "https://scicode-bench.github.io/leaderboard/"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Code to run exists on github repo
    specification:
      rating: 4
      reason: |
        Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.
    dataset:
      rating: 5
      reason: |
        Dataset meets all FAIR principles, test and validation splits are available (no train split)
    metrics:
      rating: 4
      reason: |
        Metrics stated, grading guidelines are provided in repo (problems are pass/fail)
    reference_solution:
      rating: 5
      reason: |
        Code to evaluate is available and well documented. Baseline models include closed and open weight models
    documentation:
      rating: 4
      reason: |
        Paper containing all needed info except for evlauation criteria

- date: "2025-03-13"
  version: "1"
  last_updated: "2025-03-13"
  expired: "false"
  valid: "yes"
  valid_date: "2025-03-13"
  name: "AIME (American Invitational Mathematics Examination)"
  url: "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  doi: "NA"
  domain:
  - Mathematics
  focus: "Pre-college advanced problem solving"
  keywords:
  - "algebra"
  - "combinatorics"
  - "number theory"
  - "geometry"
  summary: |
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  licensing: "unknown"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Mathematical problem-solving and reasoning"
  metrics:
  - "Accuracy"
  models:
  - "unknown"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Designed for human test-takers"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{www-aime,
      author = {TBD},
      title = {AIME},
      url = {https://www.vals.ai/benchmarks/aime-2025-03-13},
      month = mar,
      year = 2025,
      note = {[Online accessed 2025-06-24]}
    }
  datasets:
    links:
    - name: "AoPS website"
      url: "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        No code available
    specification:
      rating: 3
      reason: |
        Task and Inputs/Outputs are well specified. No system constraints or dataset format is mentioned
    dataset:
      rating: 4
      reason: |
        Easily accessible data with problems and solutions, but no splits
    metrics:
      rating: 4
      reason: |
        Correctness is measured, but no grading guidelines are provided.
    reference_solution:
      rating: 0
      reason: |
        Not given. Human performance stats exist, but no mentions of AI performance
    documentation:
      rating: 3
      reason: |
        Some background and other information is provided, but it is not comprehensive. No info on how to run an evaluation

- date: "2023-05-30"
  version: "1"
  last_updated: "2023-05-30"
  expired: "false"
  valid: "yes"
  valid_date: "2023-05-30"
  name: "PRM800K"
  url: "https://github.com/openai/prm800k/tree/main"
  doi: "10.48550/arXiv.2305.20050"
  domain:
  - Mathematics
  focus: "Math reasoning generalization"
  keywords:
  - "calculus"
  - "algebra"
  - "number theory"
  - "geometry"
  summary: |
    PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset.
  licensing: "MIT License"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Math reasoning and generalization"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Reasoning"
  solutions: "0"
  notes: "Math problems & Annotated reasoning steps based off of Dan Hendrycks' MATH
    dataset"
  contact:
    name: "Karl Cobbe"
    email: "karl@openai.com"
  cite:
  - |
    @article{lightman2023lets,
          title={Let's Verify Step by Step}, 
          author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
          journal={arXiv preprint arXiv:2305.20050},
          year={2023},
          Eprint = {arXiv:2305.20050},
          doi = {10.48550/arXiv.2305.20050}
    }
  datasets:
    links:
    - name: "PRM800K: A Process Supervision Dataset"
      url: "https://github.com/openai/prm800k/tree/main"
  results:
    links:
    - name: "Let's Verify Step by Step"
      url: "https://arxiv.org/abs/2305.20050"

  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Code is provided in the PRM800K Repo for evaluation and grading, documentation is present but no environment details, baseline model, or training code is given
    specification:
      rating: 4
      reason: |
        Task is well specified, format, inputs, and outputs are mentioned. No system constraints are provided.
    dataset:
      rating: 5
      reason: |
        Dataset follows all FAIR Principles. Train/Test splits are available in the PRM800K repo
    metrics:
      rating: 4
      reason: |
        Correctness is used as the primary metric, with grading guidelines provided.
    reference_solution:
      rating: 2
      reason: |
        A reference solution is mentioned in the "Lets Verify Step by Step" paper, but the model is not open-sourced.
    documentation:
      rating: 5
      reason: |
        Documentation is present in the PRM800K repo and "Lets Verify Step by Step" paper.

- date: "2024-04-02"
  version: "1"
  last_updated: "2024-04-02"
  expired: "false"
  valid: "yes"
  valid_date: "2024-04-02"
  name: "CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)"
  url: "https://arxiv.org/abs/2503.13517"
  doi: "10.48550/arXiv.2503.13517"
  domain:
  - Materials Science
  - High Energy Physics
  - Biology & Medicine
  - Chemistry
  - Climate & Earth Science
  focus: "Long-context scientific reasoning"
  keywords:
  - "long-context"
  - "information extraction"
  - "multimodal"
  summary: |
    CURIE is a benchmark of 580 problems across six scientific disciplines-materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics-
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  licensing: "Apache 2.0 License"
  task_types:
  - "Information extraction"
  - "Reasoning"
  - "Concept tracking"
  - "Aggregation"
  - "Algebraic manipulation"
  - "Multimodal comprehension"
  ai_capability_measured:
  - "Long-context understanding and scientific reasoning"
  metrics:
  - "Accuracy"
  models:
  - "unkown"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Subhashini Venugopalan"
    email: "vsubhashini@google.com"
  cite:
  - |
    @misc{cui2025curieevaluatingllmsmultitask,
      title={CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning}, 
      author={Hao Cui and Zahra Shamsi and Gowoon Cheon and Xuejian Ma and Shutong Li and Maria Tikhanovskaya and Peter Norgaard and Nayantara Mudur and Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and Victor V. Albert and Pranesh Srinivasan and Haining Pan and Philippe Faist and Brian Rohr and Ekin Dogus Cubuk and Muratahan Aykol and Amil Merchant and Michael J. Statt and Dan Morris and Drew Purves and Elise Kleeman and Ruth Alcantara and Matthew Abraham and Muqthar Mohammad and Ean Phing VanLee and Chenfei Jiang and Elizabeth Dorfman and Eun-Ah Kim and Michael P Brenner and Viren Jain and Sameera Ponda and Subhashini Venugopalan},
      year={2025},
      eprint={2503.13517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.13517}, 
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Code is available, but not well documented
    specification:
      rating: 1
      reason: |
        Explains types of problems in detail, but does not state exactly how to administer them.
    dataset:
      rating: 4
      reason: |
        Dataset is available via Github, but hard to find
    metrics:
      rating: 5
      reason: |
        Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem.
    reference_solution:
      rating: 1
      reason: |
        Exists, but is not open
    documentation:
      rating: 5
      reason: |
        Associated paper explains all criteria

- date: "2023-01-26"
  version: "1"
  last_updated: "2023-01-26"
  expired: "false"
  valid: "no"
  valid_date: "2023-01-26"
  name: "FEABench (Finite Element Analysis Benchmark): Evaluating Language Models on Multiphysics Reasoning Ability"
  url: "https://github.com/google/feabench"
  doi: "unknown"
  domain:
  - Mathematics
  focus: "FEA simulation accuracy and performance"
  keywords:
  - "finite element"
  - "simulation"
  - "PDE"
  summary: |
    N/A
  licensing: "unknown"
  task_types:
  - "Simulation"
  - "Performance evaluation"
  ai_capability_measured:
  - "Numerical simulation accuracy and efficiency"
  metrics:
  - "Solve time"
  - "Error norm"
  models:
  - "FEniCS"
  - "deal.II"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "unknown"
  notes: "OK"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{mudur2025feabenchevaluatinglanguagemodels,
      title={FEABench: Evaluating Language Models on Multiphysics Reasoning Ability}, 
      author={Nayantara Mudur and Hao Cui and Subhashini Venugopalan and Paul Raccuglia and Michael P. Brenner and Peter Norgaard},
      year={2025},
      eprint={2504.06260},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.06260}, 
      }
  datasets:
    links:
    - name: "FEABench Github"
      url: "https://github.com/google/feabench?tab=readme-ov-file#datasets"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Code is available, but poorly documented
    specification:
      rating: 1
      reason: |
        Output is defined and task clarity is questionable
    dataset:
      rating: 4
      reason: |
        Available, but not split into sets
    metrics:
      rating: 5
      reason: |
        Fully defined metrics
    reference_solution:
      rating: 4
      reason: |
        Three open-source models were used. No system constraints.
    documentation:
      rating: 5
      reason: |
        In associated paper

- date: "2024-07-12"
  version: "1"
  last_updated: "2024-07-12"
  expired: "false"
  valid: "yes"
  valid_date: "2024-07-12"
  name: "SPIQA (Scientific Paper Image Question Answering)"
  url: "https://arxiv.org/abs/2407.09413"
  doi: "10.48550/arXiv.2407.09413"
  domain:
  - Computational Science & AI
  focus: "Multimodal QA on scientific figures"
  keywords:
  - "multimodal QA"
  - "figure understanding"
  - "table comprehension"
  - "chain-of-thought"
  summary: |
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  licensing: "Apache 2.0 License"
  task_types:
  - "Question answering"
  - "Multimodal QA"
  - "Chain-of-Thought evaluation"
  ai_capability_measured:
  - "Visual-textual reasoning in scientific contexts"
  metrics:
  - "Accuracy"
  - "F1 score"
  models:
  - "Chain-of-Thought models"
  - "Multimodal QA systems"
  ml_motif:
  - Multimodal Reasoning
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Subhashini Venugopalan"
    email: "vsubhashini@google.com"
  cite:
  - |
    @misc{zhong2024spiqa,
      title={SPIQA: Scientific Paper Image Question Answering},
      author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
      year={2024},
      url={https://arxiv.org/abs/2407.09413}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/google/spiqa"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        Not provided
    specification:
      rating: 5
      reason: |
        Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.
    dataset:
      rating: 5
      reason: |
        Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.
    metrics:
      rating: 5
      reason: |
        Uses quantitative metrics (Accuracy, F1) aligned with the task
    reference_solution:
      rating: 2
      reason: |
        Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.
    documentation:
      rating: 5
      reason: |
        All information provided in paper

- date: "2025-05-13"
  version: "1"
  last_updated: "2025-05-13"
  expired: "false"
  valid: "yes"
  valid_date: "2025-05-13"
  name: "BaisBench (Biological AI Scientist Benchmark) - Question Answering"
  url: "https://arxiv.org/abs/2505.08341"
  doi: "10.48550/arXiv.2505.08341"
  domain:
  - Biology & Medicine
  focus: "Omics-driven AI research tasks"
  keywords:
  - "single-cell annotation"
  - "biological QA"
  - "autonomous discovery"
  summary: |
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  licensing: "MIT License"
  task_types:
  - "Cell type annotation"
  - "Multiple choice"
  ai_capability_measured:
  - "Autonomous biological research capabilities"
  metrics:
  - "Annotation accuracy"
  - "QA accuracy"
  models:
  - "LLM-based AI scientist agents"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Underperforms human experts; aims to advance AI-driven discovery"
  contact:
    name: "Xuegong Zhang"
    email: "zhangxg@mail.tsinghua.edu.cn"
  cite:
  - |
    @misc{luo2025benchmarkingaiscientistsomics,
      archiveprefix = {arXiv},
      author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},
      eprint        = {2505.08341},
      primaryclass  = {cs.AI},
      title         = {Benchmarking AI scientists in omics data-driven biological research},
      url           = {https://arxiv.org/abs/2505.08341},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Github"
      url: "https://github.com/EperLuo/BaisBench"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Instructions for environment setup available
    specification:
      rating: 4
      reason: |
        Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.
    dataset:
      rating: 5
      reason: |
        Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.
    metrics:
      rating: 5
      reason: |
        Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.
    reference_solution:
      rating: 0
      reason: |
        Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.
    documentation:
      rating: 5
      reason: |
        Dataset and paper accessible; IPYNB files for setup are available on the github repo.

- date: "2025-05-13"
  version: "1"
  last_updated: "2025-05-13"
  expired: "false"
  valid: "yes"
  valid_date: "2025-05-13"
  name: "BaisBench (Biological AI Scientist Benchmark) - Cell Type Annotation"
  url: "https://arxiv.org/abs/2505.08341"
  doi: "10.48550/arXiv.2505.08341"
  domain:
  - Biology & Medicine
  focus: "Omics-driven AI research tasks"
  keywords:
  - "single-cell annotation"
  - "biological QA"
  - "autonomous discovery"
  summary: |
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  licensing: "MIT License"
  task_types:
  - "Cell type annotation"
  - "Multiple choice"
  ai_capability_measured:
  - "Autonomous biological research capabilities"
  metrics:
  - "Annotation accuracy"
  - "QA accuracy"
  models:
  - "LLM-based AI scientist agents"
  ml_motif:
  - Classification
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Underperforms human experts; aims to advance AI-driven discovery"
  contact:
    name: "Xuegong Zhang"
    email: "zhangxg@mail.tsinghua.edu.cn"
  cite:
  - |
    @misc{luo2025benchmarkingaiscientistsomics,
      archiveprefix = {arXiv},
      author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},
      eprint        = {2505.08341},
      primaryclass  = {cs.AI},
      title         = {Benchmarking AI scientists in omics data-driven biological research},
      url           = {https://arxiv.org/abs/2505.08341},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Github"
      url: "https://github.com/EperLuo/BaisBench"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Instructions for environment setup available
    specification:
      rating: 4
      reason: |
        Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.
    dataset:
      rating: 5
      reason: |
        Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.
    metrics:
      rating: 5
      reason: |
        Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.
    reference_solution:
      rating: 0
      reason: |
        Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.
    documentation:
      rating: 5
      reason: |
        Dataset and paper accessible; IPYNB files for setup are available on the github repo.

- date: "2024-12-17"
  version: "1"
  last_updated: "2023-01-26"
  expired: "false"
  valid: "yes"
  valid_date: "2023-01-26"
  name: "MOLGEN"
  url: "https://github.com/zjunlp/MolGen"
  doi: "10.48550/arXiv.2301.11259"
  domain:
  - Chemistry
  focus: "Molecular generation and optimization"
  keywords:
  - "SELFIES"
  - "GAN"
  - "property optimization"
  summary: |
    MolGen is a pre-trained molecular language model that generates chemically valid
    molecules using SELFIES and reinforcement learning, guided by chemical feedback 
    to optimize properties such as logP, QED, and docking score.
  licensing: "MIT License"
  task_types:
  - "Distribution learning"
  - "Goal-oriented generation"
  ai_capability_measured:
  - "Generation of valid and optimized molecular structures"
  metrics:
  - "Validity%"
  - "Novelty%"
  - "QED"
  - "Docking score"
  - "penalized logP"
  models:
  - "MolGen"
  ml_motif:
  - Generative
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: ""
  contact:
    name: "zhangningyu@zju.edu.cn"
    email: "Ningyu Zhang"
  cite:
  - |
    @misc{fang2024domainagnosticmoleculargenerationchemical,
      archiveprefix = {arXiv},
      author        = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
      eprint        = {2301.11259},
      primaryclass  = {cs.LG},
      title         = {Domain-Agnostic Molecular Generation with Chemical Feedback},
      url           = {https://arxiv.org/abs/2301.11259},
      year          = {2024}
    }
  datasets:
    links:
    - name: "MolGen: A Pre-trained Molecular Language Model"
      url: "https://github.com/zjunlp/MolGen/tree/main"
  results:
    links:
    - name: "Domain-Agnostic Molecular Generation with Chemical Feedback"
      url: "https://arxiv.org/abs/2301.11259"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Code is available on the github repo, along with instructions to run the model and reproduce results.
    specification:
      rating: 4
      reason: |
        Task, datset format, and input/output formats are well specified. No system constraints are mentioned.
    dataset:
      rating: 5
      reason: |
        Dataset and train/test splits are available through the github repo, as well as mentions of source datasets in the paper.
    metrics:
      rating: 5
      reason: |
        Metrics are well defined and appropriate for the task
    reference_solution:
      rating: 5
      reason: |
        A pretrained model is provided, as well as training code and instructions
    documentation:
      rating: 5
      reason: |
        All necessary information is provided in the paper and github repo

- date: "2020-05-02"
  version: "1"
  last_updated: "2020-05-02"
  expired: "false"
  valid: "yes"
  valid_date: "2020-05-02"
  name: "Open Graph Benchmark (OGB) - Biology"
  url: "https://ogb.stanford.edu/docs/home/"
  doi: "10.48550/arXiv.2005.00687"
  domain:
  - Biology & Medicine
  focus: "Biological graph property prediction"
  keywords:
  - "node prediction"
  - "link prediction"
  - "graph classification"
  summary: |
    OGB-Biology is a suite of large-scale biological network datasets (protein-protein
    interaction, drug-target, etc.) with standardized splits and evaluation protocols 
    for node, link, and graph property prediction tasks.
  licensing: "MIT License"
  task_types:
  - "Node property prediction"
  - "Link property prediction"
  - "Graph property prediction"
  ai_capability_measured:
  - "Scalability and generalization in graph ML for biology"
  metrics:
  - "Accuracy"
  - "ROC-AUC"
  models:
  - "GCN"
  - "GraphSAGE"
  - "GAT"
  ml_motif:
  - Sequence Prediction/Forecasting
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Community-driven updates"
  contact:
    name: "OGB Team"
    email: "ogb@cs.stanford.edu"
  cite:
  - |
    @misc{hu2021opengraphbenchmarkdatasets,
        archiveprefix = {arXiv},
        author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
        eprint        = {2005.00687},
        primaryclass  = {cs.LG},
        title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
        url           = {https://arxiv.org/abs/2005.00687},
        year          = {2021}
    }
  datasets:
    links:
    - name: "OGB Webpage"
      url: "https://ogb.stanford.edu/docs/dataset_overview/"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        All necessary information is provided on the Github
    specification:
      rating: 4
      reason: |
        Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; splits are well-defined.
    dataset:
      rating: 5
      reason: |
        Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.
    metrics:
      rating: 5
      reason: |
        Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.
    reference_solution:
      rating: 5
      reason: |
        Multiple baselines implemented and documented (GCN, GAT, GraphSAGE).
    documentation:
      rating: 5
      reason: |
        All necessary information is included in a paper.
