- 
  - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Jet Classification
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time classification of particle jets using HL-LHC simulation features
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - classification
    - real-time ML
    - jet tagging
    - QKeras
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      This benchmark evaluates ML models for real-time classification of
      particle jets using high-level features derived from simulated LHC data. It
      includes both full-precision \nand quantized models optimized for FPGA deployment.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time inference, model compression performance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - AUC
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Keras DNN
    - QKeras quantized DNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes both float and quantized models using QKeras
    description: Additional notes or context.
    condition: optional
  - contact: Jules Muhizi
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{duarte2022fastmlsciencebenchmarksaccelerating,
      title={FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning}, 
      author={Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},
      year={2022},
      eprint={2207.07958},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.07958}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'OpenML: hls4ml_lhc_jets_hlf (https://www.openml.org/d/42468), JetClass (https://zenodo.org/record/6619768)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1Mr7J4F8PDAIBXJ2vrfVssxLekEVW7ahJ4wpSe6FN5yw
      ChatGPT: https://docs.google.com/document/d/1runrcij-eoH3_lgGZ8wm2z1YbL1Qf5cSNbVbHyWFDs4
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: true
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent
          structure, though no hardware/system constraints are specified.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace
          and other ML libraries.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos,
          but not bundled as an official starter kit.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 7.0
        reason: Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually
          connecting models to dataset.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional


- 
  - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Irregular Sensor Data Compression
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time compression of sparse sensor data with autoencoders
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - compression
    - autoencoder
    - sparse data
    - irregular sampling
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      This benchmark addresses lossy compression of irregularly sampled
      sensor data from \nparticle detectors using real-time autoencoder architectures,
      targeting latency-critical \napplications in physics experiments.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Compression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Reconstruction quality, compression efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MSE
    - Compression ratio
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - Quantized autoencoder
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Unsupervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Based on synthetic but realistic physics sensor data
    description: Additional notes or context.
    condition: optional
  - contact: Ben Hawks, Nhan Tran
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{duarte2022fastmlsciencebenchmarksaccelerating2,
      title={FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning}, 
      author={Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},
      year={2022},
      eprint={2207.07958},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.07958}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Custom synthetic irregular sensor dataset (see GitHub repo)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1Q_kENN-Lxod5_BmqUZuqC7yT0tG1KObU9mjS1AV3zK0
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: true
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Classification is clearly defined for real-time inference on simulated LHC jets. Input features (HLFs) are
          documented, though exact latency or resource constraints are not numerically specified.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Two datasets (OpenML and Zenodo) are public, well-formatted, and documented; FAIR principles are followed,
          though richer metadata would raise confidence to a 10.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: AUC and Accuracy are standard, quantitative, and well-aligned with goals of jet tagging and inference efficiency.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Float and quantized Keras/QKeras models are provided with results. Reproducibility is good, though full automation
          and documentation could be improved.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: GitHub contains baseline code, data loaders, and references, but setup for deployment (e.g., FPGA pipeline)
          requires familiarity with the tooling.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Beam Control
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Accelerators and Magnets
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Reinforcement learning control of accelerator beam position
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - RL
    - beam stabilization
    - control systems
    - simulation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: "Beam Control explores real-time reinforcement learning strategies for maintaining \nstable beam trajectories\
      \ in particle accelerators. The benchmark is based on the \nBOOSTR environment for accelerator simulation.\n"
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Control
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Policy performance in simulated accelerator control
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Stability
    - Control loss
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - DDPG
    - PPO (planned)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, RL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Reinforcement Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Environment defined, baseline RL implementation is in progress
    description: Additional notes or context.
    condition: optional
  - contact: Ben Hawks, Nhan Tran
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{kafkes2021boostrdatasetacceleratorcontrol,
      title={BOOSTR: A Dataset for Accelerator Control Systems}, 
      author={Diana Kafkes and Jason St. John},
      year={2021},
      eprint={2101.08359},
      archivePrefix={arXiv},
      primaryClass={physics.acc-ph},
      url={https://arxiv.org/abs/2101.08359}, 
      }
    - |
      @misc{duarte2022fastmlsciencebenchmarksaccelerating3,
      title={FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning}, 
      author={Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},
      year={2022},
      eprint={2207.07958},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.07958}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'BOOSTR: https://arxiv.org/pdf/2101.08359'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1dqOsPNlp7oLix6uDsqXi-j9xHq50DGf5wnQi-Jms2DQ
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: in progress
      benchmark_ready: in progress
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Task is well defined (real-time compression of sparse, irregular sensor data using autoencoders); latency
          constraints are implied but not fully quantified.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Dataset is custom and synthetic but described well; FAIR-compliance is partial (reusable and accessible, but
          not externally versioned with rich metadata).
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Uses standard quantitative metrics (MSE, compression ratio) clearly aligned with compression and reconstruction
          goals.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: Baseline (autoencoder and quantized variant) is provided, but training/inference pipeline is minimally documented
          and needs user setup.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: GitHub repo contains core components, but more structured setup instructions and pretrained weights would
          improve usability.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional
- 
  - date: '2024-07-08'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Ultrafast jet classification at the HL-LHC
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2402.01876
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: FPGA-optimized real-time jet origin classification at the HL-LHC
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - jet classification
    - FPGA
    - quantization-aware training
    - Deep Sets
    - Interaction Networks
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Demonstrates three ML models (MLP, Deep Sets, Interaction Networks) optimized for FPGA deployment with O(100
      ns) inference using quantized models and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the
      high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time inference under FPGA constraints
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    - Resource utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - MLP
    - Deep Sets
    - Interaction Network
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '3'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Uses quantization-aware training; hardware synthesis evaluated via hls4ml
    description: Additional notes or context.
    condition: optional
  - contact: Patrick Odagiu
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
      - |
       @article{odiagiu2024,
        title={Ultrafast jet classification at the HL-LHC},
        volume={5},
        ISSN={2632-2153},
        url={http://dx.doi.org/10.1088/2632-2153/ad5f10},
        DOI={10.1088/2632-2153/ad5f10},
        number={3},
        journal={Machine Learning: Science and Technology},
        publisher={IOP Publishing},
        author={Odagiu, Patrick and Que, Zhiqiang and Duarte, Javier and Haller, Johannes and Kasieczka, Gregor and Lobanov, Artur and Loncar, Vladimir and Luk, Wayne and Ngadiuba, Jennifer and Pierini, Maurizio and Rincke, Philipp and Seksaria, Arpita and Summers, Sioni and Sznajder, Andre and Tapper, Alexander and Årrestad, Thea K},
        year={2024},
        month=jul, pages={035017} 
        }

    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Zenodo DOI:10.5281/zenodo.3602260 (constituent-level jets)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1Hk2zHauNv6BcRH4ZY5RH6v_oKDfeKzyjhoYyP0Xw4h4
      ChatGPT: https://docs.google.com/document/d/1gDf1CIYtfmfZ9urv1jCRZMYz_3WwEETkugUC65OZBdw
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task is clear (RL control of beam stability), with BOOSTR-based simulator; control objectives are well motivated,
          but system constraints and reward structure are still under refinement.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: BOOSTR dataset exists and is cited, but integration into the benchmark is in early stages; metadata and FAIR
          structure are limited.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Stability and control loss are mentioned, but metrics are not yet formalized with clear definitions or baselines.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 5.5
        reason: DDPG baseline mentioned; PPO planned; implementation is still in progress with no reproducible results available
          yet.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: GitHub has a defined structure but is incomplete; setup and execution instructions for training/evaluation
          are not fully established.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-10-15'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Quench detection
    description: The name of the benchmark.
    condition: required
  - url: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Accelerators and Magnets
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time detection of superconducting magnet quenches using ML
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - quench detection
    - autoencoder
    - anomaly detection
    - real-time
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Exploration of real-time quench detection using unsupervised and RL approaches, combining multi-modal sensor
      data (BPM, power supply, acoustic), operating on kHz-MHz streams with anomaly detection and frequency-domain features.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    - Quench localization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time anomaly detection with multi-modal sensors
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC-AUC
    - Detection latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - RL agents (in development)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, RL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Reinforcement + Unsupervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 1 (autoencoder)
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Precursor detection in progress; multi-modal and dynamic weighting methods
    description: Additional notes or context.
    condition: optional
  - contact: Maira Khan
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite: []
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: BPM and power supply data from BNL (HDF5 preprocessed, ~67k BPM + 32k PS windows)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1O7NGfSIKpXqFM1D_y0DWRueYHGm5Sqj0MaWNZzMzb6w
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: in progress
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 10.0
        reason: Real-time jet origin classification under FPGA constraints is clearly defined, with explicit latency targets
          (~100 ns) and I/O formats.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Data available on Zenodo with DOI, includes constituent-level jets; accessible and well-documented, though
          not deeply versioned with full FAIR metadata.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 10.0
        reason: Accuracy, latency, and hardware resource usage (LUTs, DSPs) are rigorously measured and aligned with real-time
          goals.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Includes models (MLP, Deep Sets, Interaction Networks) with quantization-aware training and synthesis results
          via hls4ml; reproducible but tightly coupled with specific toolchains.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper and code (via hls4ml) are sufficient, but a centralized, standalone repo for reproducing all models
          would enhance accessibility.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-10-15'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: DUNE
    description: The name of the benchmark.
    condition: required
  - url: https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time ML for DUNE DAQ time-series data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - DUNE
    - time-series
    - real-time
    - trigger
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Applying real-time ML methods to time-series data from DUNE detectors, exploring trigger-level anomaly detection
      and event selection with low latency constraints.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Trigger selection
    - Time-series anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Low-latency event detection
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Detection efficiency
    - Latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - LSTM (planned)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark (in progress)
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Prototype models demonstrated on SONIC platform
    description: Additional notes or context.
    condition: optional
  - contact: Andrew J. Morgan
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite: []
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: DUNE SONIC data (via internal FNAL systems)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1_xI6kpeb3zSCMY_rzKV9s-MCMi7kHAdsLLV0eHxG9kM
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: in progress
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task (quench detection via anomaly detection) is clearly described; multi-modal sensors, streaming rates,
          and objective are provided, but constraints (latency thresholds) are qualitative.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: Custom dataset using real data from BNL; HDF5 formatted and structured, but access may be internal or limited,
          and not versioned for public FAIR use.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: ROC-AUC and detection latency are defined; relevant and quantitative but not yet paired with benchmark baselines.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 6.0
        reason: Autoencoder prototype exists; RL methods are in development; no fully reproducible pipeline is available yet.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 7.0
        reason: Slides and GDocs outline results; implementation is in progress with limited setup/code release.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-01-08'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Intelligent experiments through real-time AI
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2501.04845
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Instrumentation and Detectors; Nuclear Physics; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time FPGA-based triggering and detector control for sPHENIX and future EIC
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - FPGA
    - Graph Neural Network
    - hls4ml
    - real-time inference
    - detector control
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Resaerch and Development demonstrator for real-time processing of high-rate tracking data from the sPHENIX detector
      (RHIC) and future EIC systems. Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events (heavy
      flavor, DIS electrons) within 10 µs latency. Demonstrated improved accuracy and latency on Alveo/FELIX platforms.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Trigger classification
    - Detector control
    - Real-time inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Low-latency GNN inference on FPGA
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy (charm and beauty detection)
    - Latency (µs)
    - Resource utilization (LUT/FF/BRAM/DSP)
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Bipartite Graph Network with Set Transformers (BGN-ST)
    - GarNet (edge-classifier)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Achieved ~97.4% accuracy for beauty decay triggers; sub-10 µs latency on Alveo U280; hit-based FPGA design via
      hls4ml and FlowGNN.
    description: Additional notes or context.
    condition: optional
  - contact: Jakub Kvapil (lanl.gov)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{kvapil2025intelligentexperimentsrealtimeai,
      title={Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors}, 
      author={J. Kvapil and G. Borca-Tasciuc and H. Bossi and K. Chen and Y. Chen and Y. Corrales Morales and H. Da Costa and C. Da Silva and C. Dean and J. Durham and S. Fu and C. Hao and P. Harris and O. Hen and H. Jheng and Y. Lee and P. Li and X. Li and Y. Lin and M. X. Liu and V. Loncar and J. P. Mitrevski and A. Olvera and M. L. Purschke and J. S. Renck and G. Roland and J. Schambach and Z. Shi and N. Tran and N. Wuerfel and B. Xu and D. Yu and H. Zhang},
      year={2025},
      eprint={2501.04845},
      archivePrefix={arXiv},
      primaryClass={physics.ins-det},
      url={https://arxiv.org/abs/2501.04845}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Internal simulated tracking data (sPHENIX and EIC DIS-electron tagger)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: ''
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task (trigger-level anomaly detection) is clearly defined for low-latency streaming input, but the problem
          framing lacks complete architectural/system specs.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Internal DUNE SONIC data; not publicly released and no formal FAIR support; replicability is institutionally
          gated.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Metrics include detection efficiency and latency, which are relevant, but only lightly supported by baselines
          or formal eval scripts.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 5.0
        reason: One CNN prototype demonstrated; LSTM planned. No public implementation or ready-to-run example yet.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: Slides and some internal documentation exist, but no full pipeline or public GitHub repo yet.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-01-09'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Neural Architecture Codesign for Fast Physics Applications
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2501.05515
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Physics; Materials Science; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Automated neural architecture search and hardware-efficient model codesign for fast physics applications
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - neural architecture search
    - FPGA deployment
    - quantization
    - pruning
    - hls4ml
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Introduces a two-stage neural architecture codesign (NAC) pipeline combining global and local search,

      quantization-aware training, and pruning to design efficient models for fast Bragg peak finding and

      jet classification, synthesized for FPGA deployment with hls4ml. Achieves >30x reduction in BOPs

      and sub-100 ns inference latency on FPGA.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    - Peak finding
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Hardware-aware model optimization; low-latency inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    - Resource utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NAC-based BraggNN
    - NAC-optimized Deep Sets (jet)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 2 (BraggNN, Jet DS)
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced.
    description: Additional notes or context.
    condition: optional
  - contact: Jason Weitz (UCSD), Nhan Tran (FNAL)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{weitz2025neuralarchitecturecodesignfast,
      title={Neural Architecture Codesign for Fast Physics Applications}, 
      author={Jason Weitz and Dmitri Demler and Luke McDermott and Nhan Tran and Javier Duarte},
      year={2025},
      eprint={2501.05515},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.05515}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Internal Bragg microscopy and HEP jet datasets
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1X6RvGHaF1rZGYSorZSEEAxlwGMYau9RQHVOn82vWv2I/edit?usp=sharing
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: Yes (nac-opt, hls4ml)
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 10.0
        reason: Task is clearly defined (triggering on rare events with sub-10 µs latency); architecture, constraints, and
          system context (FPGA, Alveo) are well detailed.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: Simulated tracking data from sPHENIX and EIC; internally structured but not yet released in a public FAIR-compliant
          format.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 10.0
        reason: Accuracy, latency, and hardware resource utilization (LUTs, DSPs) are clearly defined and used in evaluation.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Graph-based models (BGN-ST, GarNet) are implemented and tested on real hardware; reproducibility possible
          with hls4ml but full scripts not bundled.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper is detailed and tool usage (FlowGNN, hls4ml) is described, but repo release and dataset access remain
          in progress.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-06-24'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Smart Pixels for LHC
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2406.14860
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics; Instrumentation and Detectors
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - smart pixel
    - on-sensor inference
    - data reduction
    - trigger
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Presents a 256x256-pixel ROIC in 28 nm CMOS with embedded 2-layer NN for cluster filtering

      at 25 ns, achieving 54-75% data reduction while maintaining noise and latency constraints. Prototype

      consumes ~300 µW/pixel and operates in combinatorial digital logic.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    - Data filtering
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: On-chip, low-power inference; data reduction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Data rejection rate
    - Power per pixel
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - 2-layer pixel NN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades.
    description: Additional notes or context.
    condition: optional
  - contact: Lindsey Gray; Jennet Dickinson
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{parpillon2024smartpixelsinpixelai,
      title={Smart Pixels: In-pixel AI for on-sensor data filtering}, 
      author={Benjamin Parpillon and Chinar Syal and Jieun Yoo and Jennet Dickinson and Morris Swartz and Giuseppe Di Guglielmo and Alice Bean and Douglas Berry and Manuel Blanco Valentin and Karri DiPetrillo and Anthony Badea and Lindsey Gray and Petar Maksimovic and Corrinne Mills and Mark S. Neubauer and Gauri Pradhan and Nhan Tran and Dahai Wen and Farah Fahim},
      year={2024},
      eprint={2406.14860},
      archivePrefix={arXiv},
      primaryClass={physics.ins-det},
      url={https://arxiv.org/abs/2406.14860}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: In-pixel charge cluster data
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1Fevo7IGGAFC8pHrGGGA4t9V-nUwZkDezncAKDHN4v0E/edit?usp=sharing
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: Yes (Zenodo:7331128)
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Task (automated neural architecture search for real-time physics) is well formulated with clear latency, model
          compression, and deployment goals.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant, though mentioned in the paper.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 10.0
        reason: BOP reduction, latency, and accuracy are all quantitatively evaluated.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: NAC-generated models for Bragg peak and jet classification are described, but pipeline requires integration
          of several tools and is not fully packaged.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 7.0
        reason: NAC pipeline, hls4ml usage, and results are discussed; code (e.g., nac-opt) referenced, but replication requires
          stitching together toolchain and data.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-10-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HEDM BraggNN
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2008.08198
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Material Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast Bragg peak analysis using deep learning in diffraction microscopy
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - BraggNN
    - diffraction
    - peak finding
    - HEDM
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Uses BraggNN, a deep neural network, for rapid Bragg peak localization in high-energy diffraction microscopy,

      achieving ~13x speedup compared to Voigt-based methods while maintaining sub-pixel accuracy.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Peak detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: High-throughput peak localization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Localization accuracy
    - Inference time
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - BraggNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Peak finding
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Enables real-time HEDM workflows; basis for NAC case study.
    description: Additional notes or context.
    condition: optional
  - contact: Jason Weitz (UCSD)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{liu2021braggnnfastxraybragg,
      title={BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning}, 
      author={Zhengchun Liu and Hemant Sharma and Jun-Sang Park and Peter Kenesei and Antonino Miceli and Jonathan Almer and Rajkumar Kettimuthu and Ian Foster},
      year={2021},
      eprint={2008.08198},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2008.08198}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Simulated HEDM diffraction images
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1wdUwyMyOi00QzQmkI8VBfwseTVXndxPAurwGsuvoQmQ/edit?usp=sharing
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: true
      benchmark_ready: true
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 10.0
        reason: 'Fully specified: describes task (data filtering/classification), system design (on-sensor inference), latency
          (25 ns), and power constraints.'
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: In-pixel charge cluster data used, but dataset release info is minimal; FAIR metadata/versioning limited.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Data rejection rate and power per pixel are clearly defined and directly tied to hardware goals.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: 2-layer NN implementation is evaluated in hardware; reproducible via hls4ml flow with results in paper.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper is clear; Zenodo asset is referenced, but additional GitHub or setup repo would improve reproducibility.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-12-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: 4D-STEM
    description: The name of the benchmark.
    condition: required
  - url: https://openreview.net/pdf?id=7yt3N0o0W9
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Material Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time ML for scanning transmission electron microscopy
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - 4D-STEM
    - electron microscopy
    - real-time
    - image processing
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Proposes ML methods for real-time analysis of 4D scanning transmission electron microscopy
      datasets; framework details in progress.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    - Streamed data inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time large-scale microscopy inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Classification accuracy
    - Throughput
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN models (prototype)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '0'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: In-progress; model design under development.
    description: Additional notes or context.
    condition: optional
  - contact: —
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - | 
      @inproceedings{qin2023extremely,
      title={Extremely Noisy 4D-TEM Strain Mapping Using Cycle Consistent Spatial Transforming Autoencoders},
      author={Shuyu Qin and Joshua Agar and Nhan Tran},
      booktitle={AI for Accelerated Materials Design - NeurIPS 2023 Workshop},
      year={2023},
      url={https://openreview.net/forum?id=7yt3N0o0W9}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: none
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1RhoGej2LmTOb0ZF3mPzhPqV2aCct805dF40LARh_YZE/edit?usp=sharing
      ChatGPT: ''
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: in progress
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Peak localization task is well-defined for diffraction images; input/output described clearly, but no system
          constraints.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Simulated diffraction images provided; reusable and downloadable, but not externally versioned or FAIR-structured.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Inference speed and localization accuracy are standard and quantitatively reported.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: BraggNN model and training pipeline exist, but need stitching from separate repositories.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper and codebase are available and usable, though not fully turnkey.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-12-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: In-Situ High-Speed Computer Vision
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2312.00128
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Fusion/Plasma
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time image classification for in-situ plasma diagnostics
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - plasma
    - in-situ vision
    - real-time ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Applies low-latency CNN models for image classification of plasma diagnostics streams; supports deployment on
      embedded platforms.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time diagnostic inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - FPS
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Embedded/deployment details in progress.
    description: Additional notes or context.
    condition: optional
  - contact: —
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @article{wei2024,
      title={Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak},
      volume={95},
      ISSN={1089-7623},
      url={http://dx.doi.org/10.1063/5.0190354},
      DOI={10.1063/5.0190354},
      number={7},
      journal={Review of Scientific Instruments},
      publisher={AIP Publishing},
      author={Wei, Y. and Forelli, R. F. and Hansen, C. and Levesque, J. P. and Tran, N. and Agar, J. C. and Di Guglielmo, G. and Mauel, M. E. and Navratil, G. A.},
      year={2024},
      month=jul 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: In-situ sensor imagery streams
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1OcPX1eQpCcQpwZ19oOoUdzY3gcIxLCHA5R_JrCPVt2A/edit?usp=sharing
      ChatGPT: https://docs.google.com/document/d/1EqkRHuQs1yQqMvZs_L6p9JAy2vKX5OCTubzttFBuRoQ/edit?usp=sharing
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: in progress
      benchmark_ready: false
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: General task defined (real-time microscopy inference), but no standardized I/O format, latency constraint,
          or complete problem framing yet.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 0.0
        reason: Dataset not provided or described in any formal way.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 6.0
        reason: Mentions throughput and accuracy, but metrics are not formally defined or benchmarked.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 2.0
        reason: Prototype CNNs described; no baseline or implementation released.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 5.0
        reason: OpenReview paper and Gemini doc give some insight, but no working code, environment, or example.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2020-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2020-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: BenchCouncil AIBench
    description: The name of the benchmark.
    condition: required
  - url: https://www.benchcouncil.org/AIBench/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: End-to-end AI benchmarking across micro, component, and application levels
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - benchmarking
    - AI systems
    - application-level evaluation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component,
      application) across hardware systems—covering image generation, object detection, translation, recommendation, video
      prediction, etc.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Training
    - Inference
    - End-to-end AI workloads
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: System-level AI workload performance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Throughput
    - Latency
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - ResNet
    - BERT
    - GANs
    - Recommendation systems
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: General
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '4'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Covers scenario-distilling, micro, component, and end-to-end benchmarks.
    description: Additional notes or context.
    condition: optional
  - contact: Wanling Gao (BenchCouncil)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{gao2019aibenchindustrystandardinternet,
      title={AIBench: An Industry Standard Internet Service AI Benchmark Suite}, 
      author={Wanling Gao and Fei Tang and Lei Wang and Jianfeng Zhan and Chunxin Lan and Chunjie Luo and Yunyou Huang and Chen Zheng and Jiahui Dai and Zheng Cao and Daoyi Zheng and Haoning Tang and Kunlin Zhan and Biao Wang and Defei Kong and Tong Wu and Minghe Yu and Chongkang Tan and Huan Li and Xinhui Tian and Yatao Li and Junchao Shao and Zhenyu Wang and Xiaoyu Wang and Hainan Ye},
      year={2019},
      eprint={1908.08998},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.08998}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1scxhARd4vzEaWpVfwKPF_nTSxv4DirlQqcGlSG0yzJc/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task (plasma diagnostic classification) and real-time deployment described; system specs (FPS targets) implied
          but not fully quantified.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Dataset is sensor stream-based but not shared or FAIR-documented.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: FPS and classification accuracy reported and relevant.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: CNN model described and evaluated, but public implementation and benchmarks are not available yet.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: Paper and Gemini doc exist, but full setup instructions and tools are still in progress.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2020-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2020-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: BenchCouncil BigDataBench
    description: The name of the benchmark.
    condition: required
  - url: https://www.benchcouncil.org/BigDataBench/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - big data
    - AI benchmarking
    - data analytics
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources)
      and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Data preprocessing
    - Inference
    - End-to-end data pipelines
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Data processing and AI model inference performance at scale
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Data throughput
    - Latency
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - LSTM
    - SVM
    - XGBoost
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: General
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
    description: Additional notes or context.
    condition: optional
  - contact: Jianfeng Zhan (BenchCouncil)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{gao2018bigdatabenchscalableunifiedbig,
      title={BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite}, 
      author={Wanling Gao and Jianfeng Zhan and Lei Wang and Chunjie Luo and Daoyi Zheng and Xu Wen and Rui Ren and Chen Zheng and Xiwen He and Hainan Ye and Haoning Tang and Zheng Cao and Shujie Zhang and Jiahui Dai},
      year={2018},
      eprint={1802.08254},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/1802.08254}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1FlvWeGm_J5QabOL7J0RWN3udzl0QFDs7wafptXx8sRU/edit?usp=sharing
      ChatGPT: https://docs.google.com/document/d/1VFRxhR2G5A83S8PqKBrP99LLVgcCGvX2WW4vTtwxmQ4/edit?usp=sharing
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Evaluates AI at multiple levels (micro to end-to-end); tasks and workloads are clearly defined, though specific
          I/O formats and constraints vary.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Realistic datasets across diverse domains; FAIR structure for many components, but individual datasets may
          not all be versioned or richly annotated.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Latency, throughput, and accuracy clearly defined for end-to-end tasks; consistent across models and setups.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Reference implementations for several tasks exist, but setup across all tasks is complex and not fully streamlined.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Central documentation exists, with detailed component breakdowns; environment setup across platforms (e.g.,
          hardware variations) can require manual adjustment.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2021-10-20'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2021-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLPerf HPC
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/hpc
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Cosmology, Climate, Protein Structure, Catalysis
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Scientific ML training and inference on HPC systems
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - HPC
    - training
    - inference
    - scientific ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation
      with >10x performance scaling through system-level optimizations.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Training
    - Inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scaling efficiency, training time, model accuracy on HPC
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Training time
    - Accuracy
    - GPU utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CosmoFlow
    - DeepCAM
    - OpenCatalyst
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference, HPC/training
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '4'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Shared framework with MLCommons Science; reference implementations included.
    description: Additional notes or context.
    condition: optional
  - contact: Steven Farrell (MLCommons)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{farrell2021mlperfhpcholisticbenchmark,
      title={MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems}, 
      author={Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendonça and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},
      year={2021},
      eprint={2110.11466},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.11466}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: See MLCommons Science entry below
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Focused on structured/unstructured data pipelines; clearly defined tasks spanning analytics to AI; some scenarios
          lack hardware constraint modeling.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Built from 13 real-world sources; structured for realistic big data scenarios; partially FAIR-compliant with
          documented data motifs.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Covers data throughput, latency, and accuracy; quantitative and benchmark-ready.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Many pipeline and model examples provided using Hadoop/Spark/Flink; setup effort varies by task and platform.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Strong documentation with examples and task specifications; centralized support exists, but task-specific
          tuning may require domain expertise.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-06-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLCommons Science
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/science
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: AI benchmarks for scientific applications including time-series, imaging, and simulation
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - science AI
    - benchmark
    - MLCommons
    - HPC
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting,
      satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series analysis
    - Image classification
    - Simulation surrogate modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Inference accuracy, simulation speed-up, generalization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MAE
    - Accuracy
    - Speedup vs simulation
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - GNN
    - Transformer
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series, Image/CV, HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Joint national-lab effort under Apache-2.0 license.
    description: Additional notes or context.
    condition: optional
  - contact: MLCommons Science Working Group
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{mlcommons_science2023, title={MLCommons Science Working Group Benchmarks}, author={MLCommons Science Working
      Group}, year={2023}, url={https://github.com/mlcommons/science}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 10.0
        reason: Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly defined with HPC system-level constraints and targets.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Public scientific datasets (e.g., cosmology, weather); used consistently, though FAIR-compliance of individual
          datasets varies slightly.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 10.0
        reason: Training time, GPU utilization, and accuracy are all directly measured and benchmarked across HPC systems.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Reference implementations available and actively maintained; HPC setup may require domain-specific environment.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: GitHub repo and papers provide detailed instructions; reproducibility supported across multiple institutions.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2021-07-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2021-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LHC New Physics Dataset
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2107.02157
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics; Real-time Triggering
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time LHC event filtering for anomaly detection using proton collision data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - proton collision
    - real-time inference
    - event filtering
    - unsupervised ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: A dataset of proton-proton collision events emulating a 40 MHz real-time data stream from LHC detectors, pre-filtered
      on electron or muon presence. Designed for unsupervised new-physics detection algorithms under latency/bandwidth constraints.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    - Event classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Unsupervised signal detection under latency and bandwidth constraints
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC-AUC
    - Detection efficiency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - Variational autoencoder
    - Isolation forest
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '3'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box.
    description: Additional notes or context.
    condition: optional
  - contact: Ema Puljak (ema.puljak@cern.ch)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{thea_aarrestad_2021_5046428,
      author= {Thea Aarrestad and Ekaterina Govorkova and Jennifer Ngadiuba and Ema Puljak and Maurizio Pierini and Kinga Anna Wozniak},
      title = {Unsupervised New Physics detection at 40 MHz: Training Dataset},
      month = jun,
      year = 2021,
      publisher = {Zenodo},
      version = {v2},
      doi = {10.5281/zenodo.5046428},
      url = {https://doi.org/10.5281/zenodo.5046428},
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'Zenodo stores: background + 3 black-box signal sets (1M events each)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1BnX67GfTQxHbDuUsH-MuHIl1uKxCIjrXHSoxvIaB72g/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but
          lacks a formal task specification or constraints.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 5.0
        reason: Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: Publicly available papers and datasets with descriptions, but no unified README or training setup.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-07-17'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLCommons Medical AI
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/medical
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Healthcare; Medical AI
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - medical AI
    - federated evaluation
    - privacy-preserving
    - fairness
    - healthcare benchmarks
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF,
      COFE)

      to accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical

      models on diverse datasets, improving generalizability and equity while keeping data onsite.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Federated evaluation
    - Model validation
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Clinical accuracy, fairness, generalizability, privacy compliance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC AUC
    - Accuracy
    - Fairness metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - MedPerf-validated CNNs
    - GaNDLF workflows
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Open-source platform under Apache-2.0; used across 20+ institutions and hospitals.
    description: Additional notes or context.
    condition: optional
  - contact: Alex Karargyris (MLCommons Medical AI)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @article{karargyris2023federated, title={Federated benchmarking of medical artificial intelligence with MedPerf},
      author={Karargyris, Alex and Sheller, Micah J and others}, journal={Nature Machine Intelligence}, year={2023},
      url={https://www.nature.com/articles/s42256-023-00652-2}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-institutional clinical datasets (radiology, EHR)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Diverse scientific tasks (earthquake, CFD, microscopy) with detailed problem statements and goals; system
          constraints not uniformly applied.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Domain-specific datasets (e.g., microscopy, climate); mostly public and structured, but FAIR annotations are
          not always explicit.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Task-specific metrics (MAE, speedup, accuracy) are clear and reproducible.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Reference models (CNN, GNN, Transformer) provided with training/evaluation pipelines.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Well-documented, open-sourced, and maintained with examples; strong community support and reproducibility
          focus.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-10-28'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: CaloChallenge 2022
    description: The name of the benchmark.
    condition: required
  - url: http://arxiv.org/abs/2410.21611
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LHC Calorimeter; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast generative-model-based calorimeter shower simulation evaluation
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - calorimeter simulation
    - generative models
    - surrogate modeling
    - LHC
    - fast simulation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative-model submissions (VAEs, GANs, Flows,
      Diffusion)

      on four calorimeter shower datasets; benchmarking shower quality, generation speed, and model complexity.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Surrogate modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Simulation fidelity, speed, efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Histogram similarity
    - Classifier AUC
    - Generation latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - VAE variants
    - GAN variants
    - Normalizing flows
    - Diffusion models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Surrogate
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Surrogate Modeling
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '31'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset
      sizes.
    description: Additional notes or context.
    condition: optional
  - contact: Claudius Krause (CaloChallenge Lead)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{krause2024calochallenge2022communitychallenge,
      title={CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation}, 
      author={Claudius Krause and Michele Faucci Giannelli and Gregor Kasieczka and Benjamin Nachman and Dalila Salamani and David Shih and Anna Zaborowska and Oz Amram and Kerstin Borras and Matthew R. Buckley and Erik Buhmann and Thorsten Buss and Renato Paulo Da Costa Cardoso and Anthony L. Caterini and Nadezda Chernyavskaya and Federico A. G. Corchia and Jesse C. Cresswell and Sascha Diefenbacher and Etienne Dreyer and Vijay Ekambaram and Engin Eren and Florian Ernst and Luigi Favaro and Matteo Franchini and Frank Gaede and Eilam Gross and Shih-Chieh Hsu and Kristina Jaruskova and Benno Käch and Jayant Kalagnanam and Raghav Kansal and Taewoo Kim and Dmitrii Kobylianskii and Anatolii Korol and William Korcari and Dirk Krücker and Katja Krüger and Marco Letizia and Shu Li and Qibin Liu and Xiulong Liu and Gabriel Loaiza-Ganem and Thandikire Madula and Peter McKeown and Isabell-A. Melzer-Pellmann and Vinicius Mikuni and Nam Nguyen and Ayodele Ore and Sofia Palacios Schweitzer and Ian Pang and Kevin Pedro and Tilman Plehn and Witold Pokorski and Huilin Qu and Piyush Raikwar and John A. Raine and Humberto Reyes-Gonzalez and Lorenzo Rinaldi and Brendan Leigh Ross and Moritz A. W. Scham and Simon Schnake and Chase Shimmin and Eli Shlizerman and Nathalie Soybelman and Mudhakar Srivatsa and Kalliopi Tsolaki and Sofia Vallecorsa and Kyongmin Yeo and Rui Zhang},
      year={2024},
      eprint={2410.21611},
      archivePrefix={arXiv},
      primaryClass={physics.ins-det},
      url={https://arxiv.org/abs/2410.21611}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Four LHC calorimeter shower datasets (various voxel resolutions)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1JBH3WTDp2jpSt_utc1p5Dv3-MBX4xY-NVzzfXCd9xhA/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: 'Task is clearly defined: real-time anomaly detection from high-rate LHC collisions. Latency and bandwidth
          constraints are mentioned, though not numerically enforced.'
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Publicly available via Zenodo, with structured signal/background splits, and rich metadata; nearly fully FAIR.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: ROC-AUC and detection efficiency are clearly defined and appropriate for unsupervised anomaly detection.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Several baseline methods (autoencoder, VAE, isolation forest) are evaluated; runnable versions available via
          community repos but not tightly bundled.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper and data documentation are clear, and the dataset is widely reused. Setup requires some manual effort
          to reproduce full pipelines.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: ongoing
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Papers With Code- SOTA Platform
    description: The name of the benchmark.
    condition: required
  - url: https://paperswithcode.com/sota
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General ML; All domains
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - leaderboard
    - benchmarking
    - reproducibility
    - open-source
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research:

      12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple (Classification, Detection, NLP, etc.)
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model performance across tasks (accuracy, F1, BLEU, etc.)
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Task-specific (Accuracy, F1, BLEU, etc.)
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - All published models with code
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '154766'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Community-driven open platform; automatic data extraction and versioning.
    description: Additional notes or context.
    condition: optional
  - contact: Papers With Code Team
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{pwc2025, title={Papers With Code: Open machine learning benchmarks and leaderboards}, author={Papers With
      Code}, year={2025}, url={https://paperswithcode.com}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Curated benchmark-task pairs from literature
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1pbn_K20t6Kr0zxdUSAL68ChruOu6B_x5sZbp-dcrG_g/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Evaluation setting (federated clinical benchmarking) is well-defined; I/O interfaces vary slightly by task
          but are standardized in MedPerf platform.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Uses distributed, real-world clinical datasets across institutions; FAIR compliance varies across hospitals
          and data hosts.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: ROC AUC, accuracy, and fairness metrics are explicitly defined and task-dependent; consistently tracked across
          institutions.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Validated CNNs and GaNDLF pipelines are used and shared via the MedPerf tool, but some implementations are
          abstracted behind the platform.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Excellent documentation across MedPerf, GaNDLF, and COFE; reproducibility handled via containerized flows
          and task templates.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2022-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Codabench
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General ML; Multiple
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Open-source platform for organizing reproducible AI benchmarks and competitions
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - benchmark platform
    - code submission
    - competitions
    - meta-benchmark
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks

      and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model reproducibility, performance across datasets
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Submission count
    - Leaderboard ranking
    - Task-specific metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Arbitrary code submissions
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 98 071
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Hosts 51 public competitions, ~26k users, 177k submissions
    description: Additional notes or context.
    condition: optional
  - contact: Isabelle Guyon (Université Paris-Saclay)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @article{xu2021codabench, title={Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},
      author={Xu, Zhen and Escalera, Sergio and others}, journal={Patterns}, volume={3}, number={7}, pages={100543},
      year={2022}, doi={10.1016/j.patter.2022.100543}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: N/A
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1sIwNDCs01s790DApVt5leiG8UaBDFVINA4bOixZ1CUw/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 0
        reason: Not a benchmark. It's a hosting site for benchmarks.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 0
        reason: Not a benchmark. It's a hosting site for benchmarks.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 0
        reason: Not a benchmark. It's a hosting site for benchmarks.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 0
        reason: Not a benchmark. It's a hosting site for benchmarks.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 0
        reason: Not a benchmark. It's a hosting site for benchmarks.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2021-09-27'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Sabath - SBI-FAIR
    description: The name of the benchmark.
    condition: required
  - url: https://sbi-fair.github.io/docs/software/sabath/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Systems; Metadata
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: FAIR metadata framework for ML-driven surrogate workflows in HPC systems
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - meta-benchmark
    - metadata
    - HPC
    - surrogate modeling
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Sabath is a metadata framework from the SBI-FAIR group (UTK, Argonne, Virginia) facilitating

      FAIR-compliant benchmarking and surrogate execution logging across HPC systems

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Systems benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Metadata tracking, reproducible HPC workflows
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Metadata completeness
    - FAIR compliance
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - N/A
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Systems
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: N/A
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc.
    description: Additional notes or context.
    condition: optional
  - contact: Piotr Luszczek (luszczek@utk.edu)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @techreport{luszczek2021sabath, title={SABATH: FAIR Metadata Technology for Surrogate Benchmarks}, author={Luszczek,
      Piotr and others}, year={2021},  institution={University of Tennessee}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: N/A
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: (none)
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: N/A
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle
          physics datasets.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Data is well-structured for SBI and publicly available with clear licensing.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: Includes likelihood and posterior accuracy; metrics well-matched to SBI.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: Baseline SBI models are implemented and reproducible.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2022-10-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: PDEBench
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/pdebench/PDEBench
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: CFD; Weather Modeling
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite for ML-based surrogates solving time-dependent PDEs
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - PDEs
    - CFD
    - scientific ML
    - surrogate modeling
    - NeurIPS
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'PDEBench offers forward/inverse PDE tasks with large ready-to-use datasets and baselines (FNO, U-Net, PINN),
      packaged via a unified API. It won the SimTech Best Paper Award 2023.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Supervised Learning
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Time-dependent PDE modeling; physical accuracy
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - boundary RMSE
    - Fourier RMSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - FNO
    - U-Net
    - PINN
    - Gradient-Based inverse methods
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 'Yes'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email
    description: Additional notes or context.
    condition: optional
  - contact: Makoto Takamoto (makoto.takamoto@neclab.eu)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{takamoto2024pdebenchextensivebenchmarkscientific,
      title={PDEBENCH: An Extensive Benchmark for Scientific Machine Learning}, 
      author={Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pflüger and Mathias Niepert},
      year={2024},
      eprint={2210.07182},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.07182}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: DaRUS repository via DOI:10.18419/darus-2986
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1MvXdFub0PxUDtB49wqli6mmSCdLErv2nLdOJUtMylOo/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Clearly defined PDE-solving tasks with well-specified constraints and solution formats.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Includes synthetic and real-world PDE datasets with detailed format descriptions.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: Uses L2 error and other norms relevant to PDE solutions.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: Includes baseline solvers and trained models across multiple PDE tasks.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Well-organized GitHub with examples, dataset loading scripts, and training configs.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: The Well
    description: The name of the benchmark.
    condition: required
  - url: https://polymathic-ai.org/the_well/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: biological systems, fluid dynamics, acoustic scattering, astrophysical MHD
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Foundation model + surrogate dataset spanning 16 physical simulation domains
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - surrogate modeling
    - foundation model
    - physics simulations
    - spatiotemporal dynamics
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains—from biology to astrophysical
      magnetohydrodynamic simulations—with unified API and metadata. Ideal for training surrogate and foundation models on
      scientific data.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Supervised Learning
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Surrogate modeling, physics-based prediction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Dataset size
    - Domain breadth
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - FNO baselines
    - U-Net baselines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Foundation model, Surrogate
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '16'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: 'Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB.'
    description: Additional notes or context.
    condition: optional
  - contact: Wes Brewer
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @article{ohana2024well, title={The well: a large-scale collection of diverse physics simulations for machine learning},
      author={Ohana, Ruben and McCabe, Michael and Meyer, Lucas and others}, journal={NeurIPS}, volume={37}, pages={44989--45037},
      year={2024}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 16 simulation datasets (HDF5) via PyPI/GitHub
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1axQvD_aY9O71A2nxWaoFvsfE8HvZ4TmwDoZ4cQQwW58/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: Explores LLM understanding of mental health scenarios; framing is creative but loosely defined.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Dataset is described in concept but not released; privacy limits public access though synthetic proxies are
          referenced.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Uses manual annotation and quality scores, but lacks standardized automatic metrics.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 6.0
        reason: Provides few-shot prompt examples and human rating calibration details.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 5.0
        reason: Paper gives use cases, but code and data are not yet public.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-10-31'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-11
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LLM-Inference-Bench
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/argonne-lcf/LLM-Inference-Bench
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Hardware performance benchmarking of LLMs on AI accelerators
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM
    - inference benchmarking
    - GPU
    - accelerator
    - throughput
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA,
      AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Inference Benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Inference throughput, latency, hardware utilization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Token throughput (tok/s)
    - Latency
    - Framework-hardware mix performance
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA-2-7B
    - LLaMA-2-70B
    - Mistral-7B
    - Qwen-7B
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Inference Benchmarking
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ''
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators.
    description: Additional notes or context.
    condition: optional
  - contact: Krishna Teja Chitty-Venkata (Argonne LCF)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{chittyvenkata2024llminferencebenchinferencebenchmarkinglarge,
      title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, 
      author={Krishna Teja Chitty-Venkata and Siddhisanket Raskar and Bharat Kale and Farah Ferdaus and Aditya Tanikanti and Ken Raffenetti and Valerie Taylor and Murali Emani and Venkatram Vishwanath},
      year={2024},
      eprint={2411.00136},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.00136}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Performance logs, model-hardware pairs
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1I3UvByGn4KaruQC1pi6XcfoAOzt4iiA61S0nR9ovC94/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: PDE tasks (forward/inverse) and I/O structures are clearly specified with detailed PDE context and constraints.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 10.0
        reason: Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Uses RMSE variants and Fourier-based errors.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 10.0
        reason: Baselines (FNO, U-Net, PINN) implemented and ready-to-run; strong community adoption.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Clean GitHub with usage, dataset links, and tutorial notebooks.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

-
  - date: '2023-12-12'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SGLang Framework
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/sgl-project/sglang/tree/main/benchmark
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM Vision
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast serving framework for LLMs and vision-language models
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM serving
    - vision-language
    - RadixAttention
    - performance
    - JSON decoding
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching,
      quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Model serving framework
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Serving throughput, JSON/task-specific latency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - Time-to-first-token
    - Throughput gain vs baseline
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaVA
    - DeepSeek
    - Llama
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: LLM Vision
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Model serving
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ''
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025.
    description: Additional notes or context.
    condition: optional
  - contact: SGLang Team
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{zheng2024sglangefficientexecutionstructured,
      title={SGLang: Efficient Execution of Structured Language Model Programs}, 
      author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      year={2024},
      eprint={2312.07104},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.07104}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Benchmark configs (dummy or real)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: (none)
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Clearly framed around surrogate learning across 16 domains, but not all tasks are formally posed or constrained
          in a unified benchmark protocol. Paper mentions performance on NVIDIA H100.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: FAIR-compliant physics simulation dataset, structured in HDF5 with unified metadata.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Metrics like dataset size and domain coverage are listed, but standardized quantitative model evaluation metrics
          (e.g., RMSE, MAE) are not enforced.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: FNO and U-Net baselines available; full benchmarking implementations pending NeurIPS paper code release.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 10.0
        reason: Site and GitHub offer a unified API, metadata standards, and dataset loading tools; NeurIPS paper adds detailed
          design context.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

-
  - date: '2023-09-12'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: vLLM Inference and Serving Engine
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/vllm-project/vllm/tree/main/benchmarks
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: High-throughput, memory-efficient inference and serving engine for LLMs
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM inference
    - PagedAttention
    - CUDA graph
    - streaming API
    - quantization
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary:
      vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large language models,
      featuring PagedAttention, continuous batching, and support for quantized and pipelined model execution. Benchmarks
      compare it to TensorRT-LLM, SGLang, and others
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Inference Benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Throughput, latency, memory efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - Time to First Token (TTFT)
    - Memory footprint
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA
    - Mixtral
    - FlashAttention-based models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Inference
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers
    description: Additional notes or context.
    condition: optional
  - contact: Woosuk Kwon (vLLM Team)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - | 
      @inproceedings{kwon2023efficient, title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
      author={Woosuk Kwon and others}, booktitle={SOSP 2023}, year={2023}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Benchmark scripts and model configurations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1nPZyRZEZHciGXYNJShR9FCJbre7ImJLKf2MG6g4-3gQ/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Benchmarks hardware performance of LLM inference across multiple platforms with well-defined input/output
          and platform constraints.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: Uses structured log files and configs instead of conventional datasets; suitable for inference benchmarking.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Clear throughput, latency, and utilization metrics; platform comparison dashboard enhances evaluation.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Includes reproducible scripts and example runs; models like LLaMA and Mistral are referenced with platform-specific
          configs.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: GitHub contains clear instructions, platform details, and framework comparisons.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2022-06-22'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: vLLM Performance Dashboard
    description: The name of the benchmark.
    condition: required
  - url: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Interactive dashboard showing inference performance of vLLM
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Dashboard
    - Throughput visualization
    - Latency analysis
    - Metric tracking
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and
      hardware configurations.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Performance visualization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Throughput, latency, hardware utilization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - TTFT
    - Memory usage
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA-2
    - Mistral
    - Qwen
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Visualization
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Built using ObservableHQ; integrates live data from vLLM benchmarks.
    description: Additional notes or context.
    condition: optional
  - contact: Simon Mo
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@misc{mo2024vllm_dashboard, title={vLLM Performance Dashboard}, author={Mo, Simon}, year={2024}, url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Dashboard configurations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: (none)
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Framed as a model-serving tool rather than a benchmark, but includes benchmark configurations and real model
          tasks.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Mostly uses dummy configs or external model endpoints for evaluation; not designed around a formal dataset.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: 'Well-defined serving metrics: tokens/sec, time-to-first-token, and gain over baselines.'
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Core framework includes full reproducible serving benchmarks and code; multiple deployment case studies.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: High-quality usage guides, examples, and performance tuning docs.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2022-04-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla NeuralForecast
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series forecasting; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: High-performance neural forecasting library with >30 models
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - time-series
    - neural forecasting
    - NBEATS, NHITS, TFT
    - probabilistic forecasting
    - usability
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'NeuralForecast offers scalable, user-friendly implementations of over 30 neural forecasting models (NBEATS,
      NHITS, TFT, DeepAR, etc.),

      emphasizing quality, usability, interpretability, and performance.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Forecast accuracy, interpretability, speed
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    - CRPS
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NBEATS
    - NHITS
    - TFT
    - DeepAR
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. First official NHITS implementation.
    description: Additional notes or context.
    condition: optional
  - contact: Kin G. Olivares (Nixtla)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{olivares2022library_neuralforecast, author={Olivares, Kin G. and Challú, Cristian and others}, title={NeuralForecast:
      User friendly state-of-the-art neural forecasting models},  year={2022},  howpublished={{PyCon} US}, url={https://github.com/Nixtla/neuralforecast}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: M4, electricity, standard TS benchmarks
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1VzhaUubIm-SHK7cfKWyoi8GtykpCuOH2qPM-k_g8bKU/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Targets high-throughput LLM inference via PagedAttention and memory-optimized serving; benchmarks cover many
          configs.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: Focuses on model configs and streaming input/output pipelines rather than classical datasets.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Strong token/sec, memory usage, and TTFT metrics; comparative plots and logs included.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Benchmarks reproducible via script with support for multiple models and hardware types.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Excellent GitHub docs, CLI/API usage, and deployment walkthroughs.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

-
  - date: '2023-06-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast NHITS
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Official NHITS implementation for long-horizon time series forecasting
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - NHITS
    - long-horizon forecasting
    - neural interpolation
    - time-series
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art model that

      improved accuracy by ~25% and reduced compute by 50x compared to Transformer baselines,

      using hierarchical interpolation and multi-rate sampling

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Accuracy, compute efficiency for long series
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NHITS
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Official implementation in NeuralForecast, included since its AAAI 2023 release.
    description: Additional notes or context.
    condition: optional
  - contact: Kin G. Olivares (Nixtla)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @inproceedings{challu2023nhits, title={NHITS: Neural Hierarchical Interpolation for Time Series Forecasting},
      author={Challu, Cristian and Olivares, Kin G. and others},  booktitle={AAAI 2023},  year={2023}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Standard forecast datasets (M4, etc.)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/15Hm5ekGu99aQWsdtiUIwX6JMoaoFpRbIhDylrWqSoHY/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: Primarily a visualization frontend; underlying benchmark definitions come from vLLM project.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: No traditional dataset; displays live or logged benchmark metrics.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Live throughput, memory, latency, and TTFT displayed interactively; highly informative for performance analysis.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: Dashboard built on vLLM benchmarks but not itself a complete experiment package.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Observable notebooks are intuitive; customization instructions are minimal but UI is self-explanatory.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-10-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast TimeLLM
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Reprogramming LLMs for time series forecasting
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Time-LLM
    - language model
    - time-series
    - reprogramming
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Time-LLM uses reprogramming layers to adapt frozen LLMs for time series forecasting, treating
      forecasting as a language task.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model reuse via LLM, few-shot forecasting
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Time-LLM
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Fully open-source; transforms forecasting using LLM text reconstruction.
    description: Additional notes or context.
    condition: optional
  - contact: Ming Jin (Nixtla)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{jin2024timellmtimeseriesforecasting,
      title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models}, 
      author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},
      year={2024},
      eprint={2310.01728},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01728}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Standard forecast datasets (M4, etc.)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1xXGzRt-qhUFTvnBGQi2IbcoBdYyo-ZrAn3IOkswd3fw/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Uses open time series datasets, but lacks a consolidated data release or splits.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Reports metrics like MASE and SMAPE, standard in forecasting.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 6.0
        reason: Provides TimeLLM with open source, but no other baselines included.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-10-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast TimeGPT
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Time-series foundation model "TimeGPT" for forecasting and anomaly detection
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - TimeGPT
    - foundation model
    - time-series
    - generative model
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for
      zero-shot forecasting and anomaly detection via API.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Zero-shot forecasting, anomaly detection
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - Anomaly detection metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - TimeGPT
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Offered via Nixtla API and Azure Studio; enterprise-grade support available.
    description: Additional notes or context.
    condition: optional
  - contact: Azul Garza (Nixtla)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{garza2024timegpt1,
      title={TimeGPT-1}, 
      author={Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},
      year={2024},
      eprint={2310.03589},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03589}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Pretrained on 100 B+ time series via Nixtla
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://docs.google.com/document/d/1KmKs9JtcfpKe40fNuLrgdFcOmBnfj3ZG7AwcfTH4tXE/edit?usp=sharing
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 7.0
        reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Uses open time series datasets, but lacks a consolidated data release or splits.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Reports metrics like MASE and SMAPE, standard in forecasting.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 6.0
        reason: Provides TimeLLM with open source, but no other baselines included.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge- Gravitational Waves
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/2626/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Astrophysics; Time-series
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - gravitational waves
    - astrophysics
    - time-series
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 
      A benchmark for detecting anomalous transient gravitational-wave signals, including "unknown-unknowns," using
      preprocessed LIGO time-series at 4096 Hz. Competitors submit inference models on Codabench for continuous 50 ms segments
      from dual interferometers.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Novel event detection in physical signals
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC-AUC
    - Precision/Recall
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Deep latent CNNs
    - Autoencoders
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench.
    description: Additional notes or context.
    condition: optional
  - contact: HDR A3D3 Team
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{campolongo2025buildingmachinelearningchallenges,
      title={Building Machine Learning Challenges for Anomaly Detection in Science}, 
      author={Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      year={2025},
      eprint={2503.02112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.02112}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Preprocessed LIGO/Hanford and Livingston waveforms
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://www.codabench.org/competitions/2626/
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Novel approach treating forecasting as text generation is explained; framing is less conventional.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Compatible with standard forecasting datasets (e.g., M4, electricity).
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: RMSE and MAPE are included, but less emphasis on interpretability or time-series domain constraints.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Open-source with reprogramming layers, LLM interface scripts provided.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Model and architecture overview present, though usability guide is slightly lighter than others.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge- Butterfly
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/3764/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Genomics; Image/CV
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - computer vision
    - genomics
    - butterfly hybrids
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
     Image-based challenge for detecting butterfly hybrids in microscopy-driven species data. Participants evaluate
      models on Codabench using image segmentation/classification.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Hybrid detection in biological systems
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Classification accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN-based detectors
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Hybrid detection benchmarks hosted on Codabench.
    description: Additional notes or context.
    condition: optional
  - contact: Imageomics/HDR Team
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{campolongo2025buildingmachinelearningchallenges2,
      title={Building Machine Learning Challenges for Anomaly Detection in Science}, 
      author={Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      year={2025},
      eprint={2503.02112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.02112}, 
      }
    condition: '>=1'
  - dataset: Butterfly hybrid image dataset
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://www.codabench.org/competitions/3764/
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task of detecting rare anomalies in butterfly physics is well-described with physics motivation.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 7.0
        reason: Real detector data with injected anomalies is available, but requires NDA for full access.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Uses ROC, F1, and anomaly precision, standard in challenge evaluations.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 4.0
        reason: Partial baselines described, but no codebase or reproducible runs.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.0
        reason: Challenge site includes overview and metrics, but limited in walkthrough or examples.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge- Sea Level Rise
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/3223/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Climate Science; Time-series, Image/CV
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - climate science
    - sea-level rise
    - time-series
    - remote sensing
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      A challenge combining North Atlantic sea-level time-series and satellite imagery to detect flooding anomalies.
      Models submitted via Codabench.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Detection of environmental anomalies
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC-AUC
    - Precision/Recall
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNNs, RNNs, Transformers
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Sponsored by NSF HDR; integrates sensor and satellite data.
    description: Additional notes or context.
    condition: optional
  - contact: HDR A3D3 Team
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{campolongo2025buildingmachinelearningchallenges3,
      title={Building Machine Learning Challenges for Anomaly Detection in Science}, 
      author={Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      year={2025},
      eprint={2503.02112},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.02112}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Sea-level time-series and satellite imagery
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: https://www.codabench.org/competitions/3223/
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Clear anomaly detection objective framed for physical signal discovery (LIGO/Virgo).
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 10.0
        reason: Preprocessed waveform data from dual interferometers, public and well-structured.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: ROC-AUC, Precision/Recall, and confusion-based metrics are standardized.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 1.0
        reason: No starter model or baseline code linked
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Codabench page, GitHub starter kit, and related papers provide strong guidance.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2025-01-24'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2025-02
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Single Qubit Readout on QICK System
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/ml-quantum-readout
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Quantum Computing
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time single-qubit state classification using FPGA firmware
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - qubit readout
    - hls4ml
    - FPGA
    - QICK
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using
      hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Single-shot fidelity, inference latency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - hls4ml quantized NN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization.
    description: Additional notes or context.
    condition: optional
  - contact: Javier Campos / Giuseppe Di Guglielmo
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{diguglielmo2025endtoendworkflowmachinelearningbased,
      title={End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml}, 
      author={Giuseppe Di Guglielmo and Botao Du and Javier Campos and Alexandra Boltasseva and Akash V. Dixit and Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and Ruichao Ma and Gabriel N. Perdue and Nhan Tran and Omer Yesilyurt and Daniel Bowring},
      year={2025},
      eprint={2501.14663},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2501.14663}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'Zenodo: ml-quantum-readout dataset (zenodo.org/records/14427490)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: (none)
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Task clearly framed around detecting hybrid species via images, but exact labeling methods and hybrid definitions
          may need elaboration.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Dataset hosted on Codabench; appears structured but details on image sourcing and labeling pipeline are limited.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Classification accuracy and F1 are standard and appropriate.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 1.0
        reason: No starter model or baseline code linked
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 7.5
        reason: Codabench task page describes dataset and evaluation method but lacks full API/docs.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2023-11-20'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2023-11
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: 'GPQA A Graduate Level Google Proof Question and Answer Benchmark'
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2311.12022
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Science (Biology, Physics, Chemistry)
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Graduate-level, expert-validated multiple-choice questions hard even with web access
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Google-proof
    - multiple-choice
    - expert reasoning
    - science QA
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      Contains 448 challenging questions written by domain experts, with expert accuracy at 65% (74% discounting clear
      errors) and non-experts reaching just 34%. GPT-4 baseline scores ~39%—designed for scalable oversight evaluation.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple choice
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scientific reasoning, knowledge probing
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GPT-4 baseline
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple choice
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple choice
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 448 questions
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: “Google-proof”; supports oversight research.
    description: Additional notes or context.
    condition: optional
  - contact: David Rein (NYU)
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark}, 
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: GPQA dataset (zip/HuggingFace)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: (none)
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Clear dual-modality task (image + time-series); environmental focus is well described.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Time-series and satellite imagery data provided; sensor info and collection intervals are explained.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: ROC-AUC, Precision/Recall are appropriate and robust.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 1.0
        reason: No starter model or baseline code linked
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 6.5
        reason: Moderate Codabench documentation with climate context; lacks pipeline-level walkthrough.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SeafloorAI
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97432
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Marine Science; Vision-Language
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Large-scale vision-language dataset for seafloor mapping and geological classification
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - sonar imagery
    - vision-language
    - seafloor mapping
    - segmentation
    - QA
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      A first-of-its-kind dataset covering 17,300 sq km of seafloor with 696K sonar images, 827K segmentation masks,
      and 696K natural-language descriptions plus ~7M QA pairs—designed for both vision and language-based ML models in marine
      science
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image segmentation
    - Vision-language QA
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Geospatial understanding, multimodal reasoning
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Segmentation pixel accuracy
    - QA accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - SegFormer
    - ViLT-style multimodal models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Vision-Language
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Segmentation, QA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ~696K images
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Data processing code publicly available, covering five geological layers; curated with marine scientists
    description: Additional notes or context.
    condition: optional
  - contact: Kien X. Nguyen
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{nguyen2024seafloorailargescalevisionlanguagedataset,
      title={SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey}, 
      author={Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},
      year={2024},
      eprint={2411.00172},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.00172}, 
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Sonar imagery + annotations (~15 TB)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Real-time qubit classification task clearly defined in quantum instrumentation context.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Dataset available on Zenodo with signal traces; compact and reproducible.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Accuracy and latency are well defined and crucial in this setting.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: GitHub repo has reproducible code and HLS firmware targeting FPGA.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Good setup instructions, but no interactive visualization or starter notebook.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SuperCon3D
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97553
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Materials Science; Superconductivity
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - superconductivity
    - crystal structures
    - equivariant GNN
    - generative models
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning
      models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc
      candidates.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Regression (Tc prediction)
    - Generative modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Structure-to-property prediction, structure generation
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MAE (Tc)
    - Validity of generated structures
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - SODNet
    - DiffCSP-SC
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Materials Modeling
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Models
    description: The type of this benchmark.
    condition: required
  - ml_task: Regression, Generation
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrates advantage of combining ordered and disordered structural data in model design
    description: Additional notes or context.
    condition: optional
  - contact: Zhong Zuo
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{zhuang2024supercon3d, title={SuperCon3D: Learning Superconductivity from Ordered and Disordered Material
      Structures}, author={Zuo, Zhong and others}, year={2024}, note={NeurIPS Poster}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 3D crystal + Tc records
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 10.0
        reason: Multimodal task (segmentation + natural language QA pairs);.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 10.0
        reason: sonar imagery + masks + descriptions, georeferenced and labeled with QA
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Pixel accuracy and QA metrics clearly defined; tasks split by modality.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Baseline models (SegFormer, ViLT) are cited, partial configs likely available.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.5
        reason: Paper + GitHub metadata and processing details are comprehensive, though full dataset is not yet available.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: GeSS
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97816
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Scientific ML; Geometric Deep Learning
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite evaluating geometric deep learning models under real-world distribution shifts
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - geometric deep learning
    - distribution shift
    - OOD robustness
    - scientific applications
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating
      3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    - Regression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: OOD performance in scientific settings
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - RMSE
    - OOD robustness delta
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GCN
    - EGNN
    - DimeNet++
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Geometric DL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Classification, Regression
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 30 settings x 11 algos
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes no-OOD, unlabeled-OOD, and few-label scenarios
    description: Additional notes or context.
    condition: optional
  - contact: Deyu Zou
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{zou2024gess, title={GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution
      Shifts},  author={Zou, Deyu and Liu, Shikun and others},  year={2024},  note={NeurIPS Poster}}
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Scientific graph datasets with shift splits
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Well-defined problem (Tc prediction, generation) with strong scientific motivation (high-Tc materials), but
          no formal hardware constraints.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Includes curated 3D crystal structures and Tc data; readily downloadable and used in paper models.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: MAE and structural validity used, well-established in materials modeling.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Provides two reference models (SODNet, DiffCSP-SC) with results. Code likely available post-conference.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Paper and poster explain design choices well; software availability confirms reproducibility but limited external
          documentation.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Vocal Call Locator
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97470
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Neuroscience; Bioacoustics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmarking sound-source localization of rodent vocalizations from multi-channel audio
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - source localization
    - bioacoustics
    - time-series
    - SSL
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized
      audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms
      in bioacoustics
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Sound source localization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Source localization accuracy in bioacoustic settings
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Localization error (cm)
    - Recall/Precision
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN-based SSL models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection / localization
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 767,295 sounds
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Dataset spans real, simulated, and mixed audio; supports benchmarking across data types
    description: Additional notes or context.
    condition: optional
  - contact: Ralph Peterson
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{peterson2024vcl, title={Vocal Call Locator Benchmark for localizing rodent vocalizations}, author={Peterson,
      Ralph and Tanelus, Aramis and others}, year={2024}, note={NeurIPS Poster}, url={https://neurips.cc/virtual/2024/poster/97470}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-channel audio + annotations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Clear benchmark scenarios across GDL tasks under multiple real-world shift settings; OOD settings precisely
          categorized.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 8.0
        reason: Scientific graph datasets provided in multiple shift regimes; standardized splits across domains. Exact format
          of data not specified.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Includes base metrics (accuracy, RMSE) plus OOD delta robustness for evaluation under shifts.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 9.0
        reason: Multiple baselines (11 algorithms x 3 backbones) evaluated; setup supports reproducible comparison.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 2.0
        reason: Paper, poster, and source code provide thorough access to methodology and implementation. Setup instructions
          and accompanying code not present.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MassSpecGym
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97823
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Cheminformatics; Molecular Discovery
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite for discovery and identification of molecules via MS/MS
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - mass spectrometry
    - molecular structure
    - de novo generation
    - retrieval
    - dataset
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      MassSpecGym curates the largest public MS/MS dataset with three standardized tasks—de novo structure generation,
      molecule retrieval, and spectrum simulation—using challenging generalization splits to propel ML-driven molecule discovery
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - De novo generation
    - Retrieval
    - Simulation
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Molecular identification and generation from spectral data
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Structure accuracy
    - Retrieval precision
    - Simulation MSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Graph-based generative models
    - Retrieval baselines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Benchmark
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Generation, retrieval, simulation
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ~1M spectra
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks
    description: Additional notes or context.
    condition: optional
  - contact: Roman Bushuiev
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{bushuiev2024massspecgym, title={MassSpecGym: A benchmark for the discovery and identification of molecules},
      author={Bushuiev, Roman and Bushuiev, Anton and others}, year={2024}, note={NeurIPS Spotlight Poster}, url={https://neurips.cc/virtual/2024/poster/97823}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Public MS/MS spectra with structure annotations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Focused on sound source localization for rodent vocalizations in lab settings; well-scoped.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.5
        reason: 767000 annotated audio segments across diverse conditions. Minor deduction for no train/test/valid split.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.5
        reason: Localization error, precision/recall used
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: CNN-based baselines referenced but unclear whether pretrained models or training code are available.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 2.0
        reason: Poster and paper outline benchmark intent and setup; repo expected but not confirmed in dataset card.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Urban Data Layer
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97837
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Urban Computing; Data Engineering
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Unified data pipeline for multi-modal urban science research
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - data pipeline
    - urban science
    - multi-modal
    - benchmark
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: |
      UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality
      prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks.
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Prediction
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Multi-modal urban inference, standardization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Task-specific accuracy or RMSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Baseline regression/classification pipelines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Data engineering
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Prediction, classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 4 tasks
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models
    description: Additional notes or context.
    condition: optional
  - contact: Yiheng Wang
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{wang2024urbandatalayer, title={UrbanDataLayer: A unified data pipeline for urban science}, author={Wang,
      Yiheng and Wang, Tianyu and others}, year={2024}, note={NeurIPS Poster}, url={https://neurips.cc/virtual/2024/poster/97837}
      }"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-modal urban datasets, standardized
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: Three tasks (de novo generation, retrieval, simulation) are clearly defined for MS/MS molecule discovery.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 10.0
        reason: Over 1 million spectra with structure annotations; dataset is open-source and well-documented.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 9.0
        reason: Task-appropriate metrics (structure accuracy, precision, MSE) are specified and used consistently.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Baseline models are available (graph-based and retrieval), though not exhaustive.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: GitHub repo and poster provide code and reproducibility guidance.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Delta Squared-DFT
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97788
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Computational Chemistry; Materials Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - density functional theory
    - Delta Squared-ML correction
    - reaction energetics
    - quantum chemistry
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Introduces the Delta Squared-ML paradigm—using ML corrections to DFT to predict reaction energies with accuracy
      comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and
      organometallic transformations.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Regression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: High-accuracy energy prediction, DFT correction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Mean Absolute Error (eV)
    - Energy ranking accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Delta Squared-ML correction networks
    - Kernel ridge regression
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Scientific ML
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Regression
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 10 datasets
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility.
    description: Additional notes or context.
    condition: optional
  - contact: Wei Liu
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{liu2024delta2dft, title={Delta Squared-DFT: Machine-Learning Corrected Density Functional Theory for Reaction
      \ Energetics}, author={Liu, Wei and Chen, Rong and others}, year={2024}, note={NeurIPS Poster}, url={https://neurips.cc/virtual/2024/poster/97788}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Reaction energy sets with DFT and high-level references
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 8.0
        reason: Clear goals around unifying urban data formats and tasks (e.g., air quality prediction), though some specifics
          could be more formal.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: Multi-modal data is standardized and accessible; GitHub repo available.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: Uses common task metrics like accuracy/RMSE, though varies by task.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 7.0
        reason: Baseline regression/classification models included.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 8.0
        reason: Source code supports pipeline reuse, but formal evaluation splits may vary.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LLMs for Crop Science
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97570
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Agricultural Science; NLP
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - crop science
    - prompt engineering
    - domain adaptation
    - question answering
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs covering crop traits, growth stages,
      and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought,
      and retrieval-augmented prompts.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Question Answering
    - Inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scientific knowledge, crop reasoning
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GPT-4
    - LLaMA-2-13B
    - T5-XXL
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: NLP
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: QA, inference
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 3,500 prompts
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation.
    description: Additional notes or context.
    condition: optional
  - contact: Deepak Patel
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{patel2024llmcropsci, title={Large Language Models for Crop Science: Benchmarking Domain Reasoning and QA},
      author={Patel, Deepak and Zhao, Lan and others}, year={2024}, note={NeurIPS Poster}, url={https://neurips.cc/virtual/2024/poster/97570}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Crop science QA dataset
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 9.0
        reason: The task of ML correction to DFT energy predictions is well-specified.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 9.0
        reason: 10 public reaction datasets with DFT and CC references; well-documented.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 8.0
        reason: Uses MAE and ranking accuracy, suitable for this task.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 8.0
        reason: Includes both Δ²-ML and KRR baselines.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 9.0
        reason: Public benchmarks and clear reproducibility via datasets and model code.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional

- 
  - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of
      adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd'
    condition: optional
  - expired: null
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SPIQA LLM
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97575
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Multimodal Scientific QA; Computer Vision
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance)
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - multimodal QA
    - scientific figures
    - image+text
    - chain-of-thought prompting
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: 'A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions.
      Highlights performance differences between chain-of-thought and end-to-end adapter models.

      '
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multimodal QA
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Visual reasoning, scientific figure understanding
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaVA
    - MiniGPT-4
    - Owl-LLM adapter variants
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multimodal QA
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Multimodal QA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 10 adapter variants
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs.
    description: Additional notes or context.
    condition: optional
  - contact: Xiaoyan Zhong
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - |
      @misc{zhong2024spiqa_llm, title={SPIQA-LLM: Evaluating LLM Adapters on Scientific Figure QA}, author={Zhong,
      Xiaoyan and Gao, Yijian and others}, year={2024}, note={NeurIPS Poster}, url={https://neurips.cc/virtual/2024/poster/97575}
      }
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: SPIQA image-question set
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results:
      Gemini: TODO
      ChatGPT: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair:
      reproducible: 'Yes'
      benchmark_ready: 'Yes'
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - specification:
        rating: 6.0
        reason: Task of QA over scientific figures is interesting but not fully formalized in input/output terms.
        description: Evaluates the maturity of the specification and explanations for the existence of this benchmark.
        condition: optional
    - dataset:
        rating: 6.0
        reason: Uses SPIQA dataset with ~10 adapters; figures and questions are included, but not fully open.
        description: "Evaluates the dataset, including attributes such as:\n  - public availability\n  - versioning\n  - metadata\n\
          \  - licensing"
        condition: optional
    - metrics:
        rating: 7.0
        reason: Reports accuracy and F1; fair but no visual reasoning-specific metric.
        description: The existence and maturity of an accuracy value relevant for the domain community.
        condition: optional
    - reference_solution:
        rating: 6.0
        reason: 10 LLM adapter baselines; results included.
        description: The availability of a reference solution.
        condition: optional
    - documentation:
        rating: 5.0
        reason: Poster paper and limited documentation; no reproducibility instructions.
        description: The maturity of the documentation, including how to replicate the benchmark.
        condition: optional
    - software:
        rating: -1
        reason: Not yet evaluated
        description: Evaluates maturity and software engineering aspects used to define this benchmark.
        condition: optional
    description: Structured benchmark ratings.
    condition: optional


