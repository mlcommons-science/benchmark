- date: '2024-05-01'
  last_updated: 2024-05
  expired: unkown
  valid: 'yes'
  name: Jet Classification
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify
  domain: Particle Physics
  focus: Real-time classification of particle jets using HL-LHC simulation features
  keywords:
  - classification
  - real-time ML
  - jet tagging
  - QKeras
  description: |-
    This benchmark evaluates ML models for real-time classification of
    particle jets using high-level features derived from simulated LHC data. It
    includes both full-precision and quantized models optimized for FPGA deployment.
  task_types:
  - Classification
  ai_capability_measured:
  - Real-time inference
  - model compression performance
  metrics:
  - Accuracy
  - AUC
  models:
  - Keras DNN
  - QKeras quantized DNN
  ml_motif:
  - Real-time
  type: Benchmark
  ml_task: Supervised Learning
  notes: |-
    Includes both float and quantized models using QKeras
  contact:
    name: Jules Muhizi
    email: unkown
  cite:
  - |-
    @misc{duarte2022fastml,
      archiveprefix = {arXiv},
      author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},
      eprint        = {2207.07958},
      primaryclass  = {cs.LG},
      title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
      url           = {https://arxiv.org/abs/2207.07958},
      year          = {2022}
    }
  dataset:
  - name: 'OpenML: hls4ml_lhc_jets_hlf'
    url: https://www.openml.org/d/42468
  - name: JetClass
    url: https://zenodo.org/record/6619768
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1Mr7J4F8PDAIBXJ2vrfVssxLekEVW7ahJ4wpSe6FN5yw
  - name: ChatGPT LLM
    url: https://docs.google.com/document/d/1runrcij-eoH3_lgGZ8wm2z1YbL1Qf5cSNbVbHyWFDs4
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.
    dataset:
      rating: 9.0
      reason: |-
        Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.
    metrics:
      rating: 9.0
      reason: |-
        Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
    reference_solution:
      rating: 8.0
      reason: |-
        Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not bundled as an official starter kit.
    documentation:
      rating: 7.0
      reason: |-
        Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually connecting models to dataset.
- date: '2024-05-01'
  last_updated: 2024-05
  expired: unkown
  valid: 'yes'
  name: Irregular Sensor Data Compression
  url: |-
    https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression
  domain: Particle Physics
  focus: Real-time compression of sparse sensor data with autoencoders
  keywords:
  - compression
  - autoencoder
  - sparse data
  - irregular sampling
  description: |-
    This benchmark addresses lossy compression of irregularly sampled
    sensor data from particle detectors using real-time autoencoder architectures,
    targeting latency-critical applications in physics experiments.
  task_types:
  - Compression
  ai_capability_measured:
  - Reconstruction quality
  - compression efficiency
  metrics:
  - MSE
  - Compression ratio
  models:
  - Autoencoder
  - Quantized autoencoder
  ml_motif:
  - Real-time, Image/CV
  type: Benchmark
  ml_task: Unsupervised Learning
  notes: |-
    Based on synthetic but realistic physics sensor data
  contact:
    name: Ben Hawks, Nhan Tran
    email: unkown
  cite:
  - |-
    @misc{duarte2022fastmlsciencebenchmarksaccelerating,
      archiveprefix = {arXiv},
      author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Reddi, Vijay Janapa},
      eprint        = {2207.07958},
      primaryclass  = {cs.LG},
      title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
      url           = {https://arxiv.org/abs/2207.07958},
      year          = {2022}
    }
  dataset:
  - name: Custom synthetic irregular sensor dataset
    url: see GitHub repo
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1Q_kENN-Lxod5_BmqUZuqC7yT0tG1KObU9mjS1AV3zK0
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 8.0
      reason: |-
        Classification is clearly defined for real-time inference on simulated LHC jets. Input features (HLFs) are documented, though exact latency or resource constraints are not numerically specified.
    dataset:
      rating: 9.0
      reason: |-
        Two datasets (OpenML and Zenodo) are public, well-formatted, and documented; FAIR principles are followed, though richer metadata would raise confidence to a 10.
    metrics:
      rating: 9.0
      reason: |-
        AUC and Accuracy are standard, quantitative, and well-aligned with goals of jet tagging and inference efficiency.
    reference_solution:
      rating: 8.0
      reason: |-
        Float and quantized Keras/QKeras models are provided with results. Reproducibility is good, though full automation and documentation could be improved.
    documentation:
      rating: 8.0
      reason: |-
        GitHub contains baseline code, data loaders, and references, but setup for deployment (e.g., FPGA pipeline) requires familiarity with the tooling.
- date: '2024-05-01'
  last_updated: 2024-05
  expired: unkown
  valid: 'yes'
  name: Beam Control
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control
  domain: Accelerators and Magnets
  focus: Reinforcement learning control of accelerator beam position
  keywords:
  - RL
  - beam stabilization
  - control systems
  - simulation
  description: "Beam Control explores real-time reinforcement learning strategies for maintaining \nstable\
    \ beam trajectories in particle accelerators. The benchmark is based on the \nBOOSTR environment for\
    \ accelerator simulation."
  task_types:
  - Control
  ai_capability_measured:
  - Policy performance in simulated accelerator control
  metrics:
  - Stability
  - Control loss
  models:
  - DDPG
  - PPO (planned)
  ml_motif:
  - Real-time, RL
  type: Benchmark
  ml_task: Reinforcement Learning
  notes: |-
    Environment defined, baseline RL implementation is in progress
  contact:
    name: Ben Hawks, Nhan Tran
    email: unkown
  cite:
  - |-
    @misc{duarte2022fastmlsciencebenchmarksaccelerating,
      archiveprefix = {arXiv},
      author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},
      eprint        = {2207.07958},
      primaryclass  = {cs.LG},
      title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},
      url           = {https://arxiv.org/abs/2207.07958},
      year          = {2022}
    }
  - |-
    @misc{kafkes2021boostrdatasetacceleratorcontrol,
      archiveprefix = {arXiv},
      author        = {Diana Kafkes and Jason St. John},
      eprint        = {2101.08359},
      primaryclass  = {physics.acc-ph},
      title         = {BOOSTR: A Dataset for Accelerator Control Systems},
      url           = {https://arxiv.org/abs/2101.08359},
      year          = {2021}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1dqOsPNlp7oLix6uDsqXi-j9xHq50DGf5wnQi-Jms2DQ
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: in progress
    benchmark_ready: in progress
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 9.0
      reason: |-
        Task is well defined (real-time compression of sparse, irregular sensor data using autoencoders); latency constraints are implied but not fully quantified.
    dataset:
      rating: 8.0
      reason: |-
        Dataset is custom and synthetic but described well; FAIR-compliance is partial (reusable and accessible, but not externally versioned with rich metadata).
    metrics:
      rating: 9.0
      reason: |-
        Uses standard quantitative metrics (MSE, compression ratio) clearly aligned with compression and reconstruction goals.
    reference_solution:
      rating: 7.0
      reason: |-
        Baseline (autoencoder and quantized variant) is provided, but training/inference pipeline is minimally documented and needs user setup.
    documentation:
      rating: 8.0
      reason: |-
        GitHub repo contains core components, but more structured setup instructions and pretrained weights would improve usability.
- date: '2024-07-08'
  last_updated: 2024-07
  expired: unkown
  valid: 'yes'
  name: Ultrafast jet classification at the HL-LHC
  url: https://arxiv.org/pdf/2402.01876
  domain: Particle Physics
  focus: FPGA-optimized real-time jet origin classification at the HL-LHC
  keywords:
  - jet classification
  - FPGA
  - quantization-aware training
  - Deep Sets
  - Interaction Networks
  description: |-
    Demonstrates three ML models (MLP, Deep Sets, Interaction Networks) optimized for FPGA deployment with O(100 ns) inference using quantized models and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260. :contentReference[oaicite:1]{index=1}
  task_types:
  - Classification
  ai_capability_measured:
  - Real-time inference under FPGA constraints
  metrics:
  - Accuracy
  - Latency
  - Resource utilization
  models:
  - MLP
  - Deep Sets
  - Interaction Network
  ml_motif:
  - Real-time
  type: Model
  ml_task: Supervised Learning
  notes: |-
    Uses quantization-aware training; hardware synthesis evaluated via hls4ml
  contact:
    name: Patrick Odagiu
    email: unkown
  cite: 
  - |-
    @misc{odagiu2024ultrafastjetclassificationfpgas,
      archiveprefix = {arXiv},
      author        = {Patrick Odagiu and Zhiqiang Que and Javier Duarte and Johannes Haller and Gregor Kasieczka and Artur Lobanov and Vladimir Loncar and Wayne Luk and Jennifer Ngadiuba and Maurizio Pierini and Philipp Rincke and Arpita Seksaria and Sioni Summers and Andre Sznajder and Alexander Tapper and Thea K. Aarrestad},
      doi           = {https://doi.org/10.1088/2632-2153/ad5f10},
      eprint        = {2402.01876},
      primaryclass  = {hep-ex},
      title         = {Ultrafast jet classification on FPGAs for the HL-LHC},
      url           = {https://arxiv.org/abs/2402.01876},
      year          = {2024}
    }
  dataset:
  - name: Zenodo DOI:10.5281/zenodo.3602260
    url: constituent-level jets
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1Hk2zHauNv6BcRH4ZY5RH6v_oKDfeKzyjhoYyP0Xw4h4
  - name: ChatGPT LLM
    url: https://docs.google.com/document/d/1gDf1CIYtfmfZ9urv1jCRZMYz_3WwEETkugUC65OZBdw
  fair:
    reproducible: true
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 8.0
      reason: |-
        Task is clear (RL control of beam stability), with BOOSTR-based simulator; control objectives are well motivated, but system constraints and reward structure are still under refinement.
    dataset:
      rating: 7.0
      reason: |-
        BOOSTR dataset exists and is cited, but integration into the benchmark is in early stages; metadata and FAIR structure are limited.
    metrics:
      rating: 7.0
      reason: |-
        Stability and control loss are mentioned, but metrics are not yet formalized with clear definitions or baselines.
    reference_solution:
      rating: 5.5
      reason: |-
        DDPG baseline mentioned; PPO planned; implementation is still in progress with no reproducible results available yet.
    documentation:
      rating: 6.0
      reason: |-
        GitHub has a defined structure but is incomplete; setup and execution instructions for training/evaluation are not fully established.
- date: '2024-10-15'
  last_updated: 2024-10
  expired: unkown
  valid: 'yes'
  name: Quench detection
  url: |-
    https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf
  domain: Accelerators and Magnets
  focus: Real-time detection of superconducting magnet quenches using ML
  keywords:
  - quench detection
  - autoencoder
  - anomaly detection
  - real-time
  description: |-
    Exploration of real-time quench detection using unsupervised and RL approaches, combining multi-modal sensor data (BPM, power supply, acoustic), operating on kHz-MHz streams with anomaly detection and frequency-domain features. :contentReference[oaicite:2]{index=2}
  task_types:
  - Anomaly detection
  - Quench localization
  ai_capability_measured:
  - Real-time anomaly detection with multi-modal sensors
  metrics:
  - ROC-AUC
  - Detection latency
  models:
  - Autoencoder
  - RL agents (in development)
  ml_motif:
  - Real-time, RL
  type: Benchmark
  ml_task: Reinforcement + Unsupervised Learning
  notes: |-
    Precursor detection in progress; multi-modal and dynamic weighting methods
  contact:
    name: Maira Khan
    email: unkown
  cite: []
  dataset:
  - name: BPM and power supply data from BNL
    url: HDF5 preprocessed
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1O7NGfSIKpXqFM1D_y0DWRueYHGm5Sqj0MaWNZzMzb6w
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: in progress
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 10.0
      reason: |-
        Real-time jet origin classification under FPGA constraints is clearly defined, with explicit latency targets (~100 ns) and I/O formats.
    dataset:
      rating: 9.0
      reason: |-
        Data available on Zenodo with DOI, includes constituent-level jets; accessible and well-documented, though not deeply versioned with full FAIR metadata.
    metrics:
      rating: 10.0
      reason: |-
        Accuracy, latency, and hardware resource usage (LUTs, DSPs) are rigorously measured and aligned with real-time goals.
    reference_solution:
      rating: 9.0
      reason: |-
        Includes models (MLP, Deep Sets, Interaction Networks) with quantization-aware training and synthesis results via hls4ml; reproducible but tightly coupled with specific toolchains.
    documentation:
      rating: 8.0
      reason: |-
        Paper and code (via hls4ml) are sufficient, but a centralized, standalone repo for reproducing all models would enhance accessibility.
- date: '2024-10-15'
  last_updated: 2024-10
  expired: unkown
  valid: 'yes'
  name: DUNE
  url: |-
    https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf
  domain: Particle Physics
  focus: Real-time ML for DUNE DAQ time-series data
  keywords:
  - DUNE
  - time-series
  - real-time
  - trigger
  description: |-
    Applying real-time ML methods to time-series data from DUNE detectors, exploring trigger-level anomaly detection and event selection with low latency constraints.
  task_types:
  - Trigger selection
  - Time-series anomaly detection
  ai_capability_measured:
  - Low-latency event detection
  metrics:
  - Detection efficiency
  - Latency
  models:
  - CNN
  - LSTM (planned)
  ml_motif:
  - Real-time, Time-series
  type: Benchmark (in progress)
  ml_task: Supervised Learning
  notes: |-
    Prototype models demonstrated on SONIC platform
  contact:
    name: Andrew J. Morgan
    email: unkown
  cite: 
  - |-
    @misc{abud2021deep,
      title={Deep Underground Neutrino Experiment (DUNE) Near Detector Conceptual Design Report}, 
      author={A. Abed Abud and B. Abi and R. Acciarri and M. A. Acero and G. Adamov and D. Adams and M. Adinolfi and A. Aduszkiewicz and Z. Ahmad and J. Ahmed and T. Alion and S. Alonso Monsalve and M. Alrashed and C. Alt and A. Alton and P. Amedo and J. Anderson and C. Andreopoulos and M. P. Andrews and F. Andrianala and S. Andringa and N. Anfimov and A. Ankowski and M. Antonova and S. Antusch and A. Aranda-Fernandez and A. Ariga and L. O. Arnold and M. A. Arroyave and J. Asaadi and A. Aurisano and V. Aushev and D. Autiero and M. Ayala-Torres and F. Azfar and H. Back and J. J. Back and C. Backhouse and P. Baesso and I. Bagaturia and L. Bagby and S. Balasubramanian and P. Baldi and B. Baller and B. Bambah and F. Barao and G. Barenboim and G. J. Barker and W. Barkhouse and C. Barnes and G. Barr and J. Barranco Monarca and N. Barros and J. L. Barrow and A. Basharina-Freshville and A. Bashyal and V. Basque and E. Belchior and J. B. R. Battat and F. Battisti and F. Bay and J. L. Bazo Alba and J. F. Beacom and E. Bechetoille and B. Behera and L. Bellantoni and G. Bellettini and V. Bellini and O. Beltramello and D. Belver and N. Benekos and F. Bento Neves and S. Berkman and P. Bernardini and R. M. Berner and H. Berns and S. Bertolucci and M. Betancourt and A. Betancur Rodríguez and M. Bhattacharjee and S. Bhuller and B. Bhuyan and S. Biagi and J. Bian and M. Biassoni and K. Biery and B. Bilki and M. Bishai and A. Bitadze and A. Blake and F. D. M. Blaszczyk and G. C. Blazey and E. Blucher and J. Boissevain and S. Bolognesi and T. Bolton and L. Bomben and M. Bonesini and M. Bongrand and F. Bonini and A. Booth and C. Booth and S. Bordoni and A. Borkum and T. Boschi and N. Bostan and P. Bour and C. Bourgeois and S. B. Boyd and D. Boyden and J. Bracinik and D. Braga and D. Brailsford and A. Brandt and J. Bremer and C. Brew and E. Brianne and S. J. Brice and C. Brizzolari and C. Bromberg and G. Brooijmans and J. Brooke and A. Bross and G. Brunetti and M. Brunetti and N. Buchanan and H. Budd and D. Caiulo and P. Calafiura and J. Calcutt and M. Calin and S. Calvez and E. Calvo and A. Caminata and M. Campanelli and K. Cankocak and D. Caratelli and G. Carini and B. Carlus and P. Carniti and I. Caro Terrazas and H. Carranza and T. Carroll and J. F. Castaño Forero and A. Castillo and C. Castromonte and E. Catano-Mur and C. Cattadori and F. Cavalier and F. Cavanna and S. Centro and G. Cerati and A. Cervelli and A. Cervera Villanueva and M. Chalifour and A. Chappell and E. Chardonnet and N. Charitonidis and A. Chatterjee and S. Chattopadhyay and H. Chen and M. Chen and Y. Chen and Z. Chen and D. Cherdack and C. Chi and S. Childress and A. Chiriacescu and G. Chisnall and K. Cho and S. Choate and D. Chokheli and S. Choubey and A. Christensen and D. Christian and G. Christodoulou and A. Chukanov and E. Church and P. Clarke and T. E. Coan and A. G. Cocco and J. A. B. Coelho and E. Conley and R. Conley and J. M. Conrad and M. Convery and S. Copello and L. Corwin and L. Cremaldi and L. Cremonesi and J. I. Crespo-Anadón and E. Cristaldo and R. Cross and A. Cudd and C. Cuesta and Y. Cui and D. Cussans and M. Dabrowski and O. Dalager and H. da Motta and L. Da Silva Peres and C. David and Q. David and G. S. Davies and S. Davini and J. Dawson and K. De and R. M. De Almeida and P. Debbins and I. De Bonis and M. P. Decowski and A. de Gouvêa and P. C. De Holanda and I. L. De Icaza Astiz and A. Deisting and P. De Jong and A. Delbart and D. Delepine and M. Delgado and A. Dell'Acqua and P. De Lurgio and J. R. T. de Mello Neto and D. M. DeMuth and S. Dennis and C. Densham and G. W. Deptuch and A. De Roeck and V. De Romeri and G. De Souza and R. Dharmapalan and F. Diaz and J. S. Díaz and S. Di Domizio and L. Di Giulio and P. Ding and L. Di Noto and C. Distefano and R. Diurba and M. Diwan and Z. Djurcic and N. Dokania and S. Dolan and M. J. Dolinski and L. Domine and D. Douglas and D. Douillet and G. Drake and F. Drielsma and D. Duchesneau and K. Duffy and P. Dunne and T. Durkin and H. Duyang and O. Dvornikov and D. A. Dwyer and A. S. Dyshkant and M. Eads and A. Earle and D. Edmunds and J. Eisch and L. Emberger and S. Emery and A. Ereditato and C. O. Escobar and G. Eurin and J. J. Evans and E. Ewart and A. C. Ezeribe and K. Fahey and A. Falcone and C. Farnese and Y. Farzan and J. Felix and M. Fernandes Carneiro da Silva and E. Fernandez-Martinez and P. Fernandez Menendez and F. Ferraro and L. Fields and F. Filthaut and A. Fiorentini and R. S. Fitzpatrick and W. Flanagan and B. Fleming and R. Flight and D. V. Forero and J. Fowler and W. Fox and J. Franc and K. Francis and D. Franco and J. Freeman and J. Freestone and J. Fried and A. Friedland and S. Fuess and I. Furic and A. P. Furmanski and A. Gago and H. Gallagher and A. Gallas and A. Gallego-Ros and N. Gallice and V. Galymov and E. Gamberini and T. Gamble and R. Gandhi and R. Gandrajula and F. Gao and S. Gao and D. Garcia-Gamez and M. Á García-Peris and S. Gardiner and D. Gastler and G. Ge and B. Gelli and A. Gendotti and S. Gent and Z. Ghorbani-Moghaddam and D. Gibin and I. Gil-Botella and S. Gilligan and C. Girerd and A. K. Giri and D. Gnani and O. Gogota and M. Gold and S. Gollapinni and K. Gollwitzer and R. A. Gomes and L. V. Gomez Bermeo and L. S. Gomez Fajardo and F. Gonnella and J. A. Gonzalez-Cuevas and D. Gonzalez-Diaz and M. Gonzalez-Lopez and M. C. Goodman and O. Goodwin and S. Goswami and C. Gotti and E. Goudzovski and C. Grace and M. Graham and R. Gran and E. Granados and P. Granger and A. Grant and C. Grant and D. Gratieri and P. Green and L. Greenler and J. Greer and W. C. Griffith and M. Groh and J. Grudzinski and K. Grzelak and W. Gu and V. Guarino and R. Guenette and E. Guerard and A. Guglielmi and B. Guo and K. K. Guthikonda and R. Gutierrez and P. Guzowski and M. M. Guzzo and S. Gwon and A. Habig and H. Hadavand and R. Haenni and A. Hahn and J. Haiston and P. Hamacher-Baumann and T. Hamernik and P. Hamilton and J. Han and D. A. Harris and J. Hartnell and J. Harton and T. Hasegawa and C. Hasnip and R. Hatcher and K. W. Hatfield and A. Hatzikoutelis and C. Hayes and E. Hazen and A. Heavey and K. M. Heeger and J. Heise and K. Hennessy and S. Henry and M. A. Hernandez Morquecho and K. Herner and L. Hertel and V Hewes and A. Higuera and T. Hill and S. J. Hillier and A. Himmel and J. Hoff and C. Hohl and A. Holin and E. Hoppe and G. A. Horton-Smith and M. Hostert and A. Hourlier and B. Howard and R. Howell and J. Huang and J. Huang and J. Hugon and G. Iles and N. Ilic and A. M. Iliescu and R. Illingworth and A. Ioannisian and L. Isenhower and R. Itay and A. Izmaylov and S. Jackson and V. Jain and E. James and B. Jargowsky and F. Jediny and D. Jena and Y. S. Jeong and C. Jesús-Valls and X. Ji and L. Jiang and S. Jiménez and A. Jipa and R. Johnson and B. Jones and S. B. Jones and M. Judah and C. K. Jung and T. Junk and Y. Jwa and M. Kabirnezhad and A. Kaboth and I. Kadenko and I. Kakorin and F. Kamiya and N. Kaneshige and G. Karagiorgi and G. Karaman and A. Karcher and M. Karolak and Y. Karyotakis and S. Kasai and S. P. Kasetti and L. Kashur and N. Kazaryan and E. Kearns and P. Keener and K. J. Kelly and E. Kemp and O. Kemularia and W. Ketchum and S. H. Kettell and M. Khabibullin and A. Khotjantsev and A. Khvedelidze and D. Kim and B. King and B. Kirby and M. Kirby and J. Klein and K. Koehler and L. W. Koerner and S. Kohn and P. P. Koller and L. Kolupaeva and M. Kordosky and T. Kosc and U. Kose and V. A. Kostelecký and K. Kothekar and F. Krennrich and I. Kreslo and Y. Kudenko and V. A. Kudryavtsev and S. Kulagin and J. Kumar and P. Kumar and P. Kunze and N. Kurita and C. Kuruppu and V. Kus and T. Kutter and A. Lambert and B. Land and K. Lande and C. E. Lane and K. Lang and T. Langford and J. Larkin and P. Lasorak and D. Last and C. Lastoria and A. Laundrie and A. Lawrence and I. Lazanu and R. LaZur and T. Le and S. Leardini and J. Learned and P. LeBrun and T. LeCompte and G. Lehmann Miotto and R. Lehnert and M. A. Leigui de Oliveira and M. Leitner and L. Li and S. W. Li and T. Li and Y. Li and H. Liao and C. S. Lin and Q. Lin and S. Lin and A. Lister and B. R. Littlejohn and J. Liu and S. Lockwitz and T. Loew and M. Lokajicek and I. Lomidze and K. Long and K. Loo and D. Lorca and T. Lord and J. M. LoSecco and W. C. Louis and X. -G. Lu and K. B. Luk and X. Luo and N. Lurkin and T. Lux and V. P. Luzio and D. MacFarlane and A. A. Machado and P. Machado and C. T. Macias and J. R. Macier and A. Maddalena and A. Madera and P. Madigan and S. Magill and K. Mahn and A. Maio and A. Major and J. A. Maloney and G. Mandrioli and R. C. Mandujano and J. Maneira and L. Manenti and S. Manly and A. Mann and K. Manolopoulos and M. Manrique Plata and V. N. Manyam and L. Manzanillas and M. Marchan and A. Marchionni and W. Marciano and D. Marfatia and C. Mariani and J. Maricic and R. Marie and F. Marinho and A. D. Marino and D. Marsden and M. Marshak and C. M. Marshall and J. Marshall and J. Marteau and J. Martin-Albo and N. Martinez and D. A. Martinez Caicedo and S. Martynenko and K. Mason and A. Mastbaum and M. Masud and S. Matsuno and J. Matthews and C. Mauger and N. Mauri and K. Mavrokoridis and I. Mawby and R. Mazza and A. Mazzacane and E. Mazzucato and T. McAskill and E. McCluskey and N. McConkey and K. S. McFarland and C. McGrew and A. McNab and A. Mefodiev and P. Mehta and P. Melas and O. Mena and S. Menary and H. Mendez and D. P. Méndez and A. Menegolli and G. Meng and M. D. Messier and W. Metcalf and T. Mettler and M. Mewes and H. Meyer and T. Miao and G. Michna and T. Miedema and J. Migenda and V. Mikola and R. Milincic and W. Miller and J. Mills and C. Milne and O. Mineev and O. G. Miranda and S. Miryala and C. S. Mishra and S. R. Mishra and A. Mislivec and D. Mladenov and I. Mocioiu and K. Moffat and N. Moggi and R. Mohanta and T. A. Mohayai and N. Mokhov and J. Molina and L. Molina Bueno and A. Montanari and C. Montanari and D. Montanari and L. M. Montano Zetina and J. Moon and M. Mooney and A. F. Moor and D. Moreno and C. Morris and C. Mossey and E. Motuk and C. A. Moura and J. Mousseau and W. Mu and L. Mualem and J. Mueller and M. Muether and S. Mufson and F. Muheim and A. Muir and M. Mulhearn and D. Munford and H. Muramatsu and S. Murphy and J. Musser and J. Nachtman and S. Nagu and M. Nalbandyan and R. Nandakumar and D. Naples and S. Narita and D. Navas-Nicolás and A. Navrer-Agasson and N. Nayak and M. Nebot-Guinot and K. Negishi and J. K. Nelson and J. Nesbit and M. Nessi and D. Newbold and M. Newcomer and D. Newhart and H. Newton and R. Nichol and F. Nicolas-Arnaldos and E. Niner and K. Nishimura and A. Norman and A. Norrick and R. Northrop and P. Novella and J. A. Nowak and M. Oberling and J. P. Ochoa-Ricoux and A. Olivares Del Campo and A. Olivier and A. Olshevskiy and Y. Onel and Y. Onishchuk and J. Ott and L. Pagani and S. Pakvasa and G. Palacio and O. Palamara and S. Palestini and J. M. Paley and M. Pallavicini and C. Palomares and J. L. Palomino-Gallo and E. Pantic and V. Paolone and V. Papadimitriou and R. Papaleo and A. Papanestis and S. Paramesvaran and S. Parke and Z. Parsa and M. Parvu and S. Pascoli and L. Pasqualini and J. Pasternak and J. Pater and C. Patrick and L. Patrizii and R. B. Patterson and S. J. Patton and T. Patzak and A. Paudel and B. Paulos and L. Paulucci and Z. Pavlovic and G. Pawloski and D. Payne and V. Pec and S. J. M. Peeters and E. Pennacchio and A. Penzo and O. L. G. Peres and J. Perry and D. Pershey and G. Pessina and G. Petrillo and C. Petta and R. Petti and F. Piastra and L. Pickering and F. Pietropaolo and R. Plunkett and R. Poling and X. Pons and N. Poonthottathil and S. Pordes and J. Porter and M. Potekhin and R. Potenza and B. V. K. S. Potukuchi and J. Pozimski and M. Pozzato and S. Prakash and T. Prakash and S. Prince and D. Pugnere and X. Qian and M. C. Queiroga Bazetto and J. L. Raaf and V. Radeka and J. Rademacker and B. Radics and A. Rafique and E. Raguzin and M. Rai and M. Rajaoalisoa and I. Rakhno and A. Rakotonandrasana and L. Rakotondravohitra and Y. A. Ramachers and R. Rameika and M. A. Ramirez Delgado and B. Ramson and A. Rappoldi and G. Raselli and P. Ratoff and S. Raut and R. F. Razakamiandra and J. S. Real and B. Rebel and M. Reggiani-Guzzo and T. Rehak and J. Reichenbacher and S. D. Reitzner and H. Rejeb Sfar and A. Renshaw and S. Rescia and F. Resnati and A. Reynolds and C. Riccio and G. Riccobene and L. C. J. Rice and J. Ricol and A. Rigamonti and Y. Rigaut and D. Rivera and L. Rochester and M. Roda and P. Rodrigues and M. J. Rodriguez Alonso and E. Rodriguez Bonilla and J. Rodriguez Rondon and S. Rosauro-Alcaraz and M. Rosenberg and P. Rosier and B. Roskovec and M. Rossella and J. Rout and P. Roy and S. Roy and A. Rubbia and C. Rubbia and F. C. Rubio and B. Russell and D. Ruterbories and R. Saakyan and S. Sacerdoti and T. Safford and R. Sahay and N. Sahu and P. Sala and N. Samios and O. Samoylov and M. C. Sanchez and D. A. Sanders and D. Sankey and S. Santana and M. Santos-Maldonado and N. Saoulidou and P. Sapienza and C. Sarasty and I. Sarcevic and G. Savage and V. Savinov and A. Scaramelli and A. Scarff and A. Scarpelli and T. Schaffer and H. Schellman and P. Schlabach and D. Schmitz and K. Scholberg and A. Schukraft and E. Segreto and J. Sensenig and I. Seong and A. Sergi and D. Sgalaberna and M. H. Shaevitz and S. Shafaq and M. Shamma and R. Sharankova and H. R. Sharma and R. Sharma and R. Kumar and T. Shaw and C. Shepherd-Themistocleous and S. Shin and D. Shooltz and R. Shrock and L. Simard and F. Simon and N. Simos and J. Sinclair and G. Sinev and J. Singh and J. Singh and V. Singh and R. Sipos and F. W. Sippach and G. Sirri and A. Sitraka and K. Siyeon and K. Skarpaas VIII and A. Smith and E. Smith and P. Smith and J. Smolik and M. Smy and E. L. Snider and P. Snopok and M. Soares Nunes and H. Sobel and M. Soderberg and C. J. Solano Salinas and S. Söldner-Rembold and N. Solomey and V. Solovov and W. E. Sondheim and M. Sorel and J. Soto-Oton and A. Sousa and K. Soustruznik and F. Spagliardi and M. Spanu and J. Spitz and N. J. C. Spooner and K. Spurgeon and R. Staley and M. Stancari and L. Stanco and R. Stanley and R. Stein and H. M. Steiner and J. Stewart and B. Stillwell and J. Stock and F. Stocker and T. Stokes and M. Strait and T. Strauss and S. Striganov and A. Stuart and J. G. Suarez and H. Sullivan and D. Summers and A. Surdo and V. Susic and L. Suter and C. M. Sutera and R. Svoboda and B. Szczerbinska and A. M. Szelc and R. Talaga and H. A. Tanaka and B. Tapia Oregui and A. Tapper and S. Tariq and E. Tatar and R. Tayloe and A. M. Teklu and M. Tenti and K. Terao and C. A. Ternes and F. Terranova and G. Testera and A. Thea and J. L. Thompson and C. Thorn and S. C. Timm and J. Todd and A. Tonazzo and D. Torbunov and M. Torti and M. Tortola and F. Tortorici and D. Totani and M. Toups and C. Touramanis and J. Trevor and S. Trilov and W. H. Trzaska and Y. T. Tsai and Z. Tsamalaidze and K. V. Tsang and N. Tsverava and S. Tufanli and C. Tull and E. Tyley and M. Tzanov and M. A. Uchida and J. Urheim and T. Usher and S. Uzunyan and M. R. Vagins and P. Vahle and G. A. Valdiviesso and E. Valencia and Z. Vallari and J. W. F. Valle and S. Vallecorsa and R. Van Berg and R. G. Van de Water and F. Varanini and D. Vargas and G. Varner and J. Vasel and S. Vasina and G. Vasseur and N. Vaughan and K. Vaziri and S. Ventura and A. Verdugo and S. Vergani and M. A. Vermeulen and M. Verzocchi and M. Vicenzi and H. Vieira de Souza and C. Vignoli and C. Vilela and B. Viren and T. Vrba and T. Wachala and A. V. Waldron and M. Wallbank and H. Wang and J. Wang and M. H. L. S. Wang and Y. Wang and Y. Wang and K. Warburton and D. Warner and M. Wascko and D. Waters and A. Watson and P. Weatherly and A. Weber and M. Weber and H. Wei and A. Weinstein and D. Wenman and M. Wetstein and A. White and L. H. Whitehead and D. Whittington and M. J. Wilking and C. Wilkinson and Z. Williams and F. Wilson and R. J. Wilson and J. Wolcott and T. Wongjirad and A. Wood and K. Wood and E. Worcester and M. Worcester and C. Wret and W. Wu and W. Wu and Y. Xiao and E. Yandel and G. Yang and K. Yang and S. Yang and T. Yang and A. Yankelevich and N. Yershov and K. Yonehara and T. Young and B. Yu and H. Yu and J. Yu and W. Yuan and R. Zaki and J. Zalesak and L. Zambelli and B. Zamorano and A. Zani and L. Zazueta and G. Zeit and G. P. Zeller and J. Zennamo and K. Zeug and C. Zhang and M. Zhao and E. Zhivun and G. Zhu and P. Zilberman and E. D. Zimmerman and M. Zito and S. Zucchelli and J. Zuklin and V. Zutshi and R. Zwaska},
      year={2021},
      eprint={2103.13910},
      archivePrefix={arXiv},
      primaryClass={physics.ins-det},
      url={https://arxiv.org/abs/2103.13910}, 
    }
  dataset:
  - name: DUNE SONIC data
    url: via internal FNAL systems
  results:
  - name: Gemini LLM Deep Research
    url: https://docs.google.com/document/d/1_xI6kpeb3zSCMY_rzKV9s-MCMi7kHAdsLLV0eHxG9kM
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: in progress
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 8.0
      reason: |-
        Task (quench detection via anomaly detection) is clearly described; multi-modal sensors, streaming rates, and objective are provided, but constraints (latency thresholds) are qualitative.
    dataset:
      rating: 7.0
      reason: |-
        Custom dataset using real data from BNL; HDF5 formatted and structured, but access may be internal or limited, and not versioned for public FAIR use.
    metrics:
      rating: 8.0
      reason: |-
        ROC-AUC and detection latency are defined; relevant and quantitative but not yet paired with benchmark baselines.
    reference_solution:
      rating: 6.0
      reason: |-
        Autoencoder prototype exists; RL methods are in development; no fully reproducible pipeline is available yet.
    documentation:
      rating: 7.0
      reason: |-
        Slides and GDocs outline results; implementation is in progress with limited setup/code release.
- date: '2025-01-08'
  last_updated: 2025-01
  expired: unkown
  valid: 'yes'
  name: Intelligent experiments through real-time AI
  url: https://arxiv.org/pdf/2501.04845
  domain: Instrumentation and Detectors; Nuclear Physics; Particle Physics
  focus: Real-time FPGA-based triggering and detector control for sPHENIX and future EIC
  keywords:
  - FPGA
  - Graph Neural Network
  - hls4ml
  - real-time inference
  - detector control
  description: |-
    Resaerch and Development demonstrator for real-time processing of high-rate tracking data from the sPHENIX detector (RHIC) and future EIC systems. Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events (heavy flavor, DIS electrons) within 10 micros latency. Demonstrated improved accuracy and latency on Alveo/FELIX platforms.
  task_types:
  - Trigger classification
  - Detector control
  - Real-time inference
  ai_capability_measured:
  - Low-latency GNN inference on FPGA
  metrics:
  - Accuracy (charm and beauty detection)
  - Latency (micros)
  - Resource utilization (LUT/FF/BRAM/DSP)
  models:
  - Bipartite Graph Network with Set Transformers (BGN-ST)
  - GarNet (edge-classifier)
  ml_motif:
  - Real-time
  type: Model
  ml_task: Supervised Learning
  notes: |-
    Achieved ~97.4% accuracy for beauty decay triggers; sub-10 micros latency on Alveo U280; hit-based FPGA design via hls4ml and FlowGNN.
  contact:
    name: Jakub Kvapil (lanl.gov)
    email: unkown
  cite:
  - |-
    @misc{kvapil2025intelligentexperimentsrealtimeai,
      archiveprefix={arXiv},
      author={J. Kvapil and G. Borca-Tasciuc and H. Bossi and K. Chen and Y. Chen and Y. Corrales Morales and H. Da Costa and C. Da Silva and C. Dean and J. Durham and S. Fu and C. Hao and P. Harris and O. Hen and H. Jheng and Y. Lee and P. Li and X. Li and Y. Lin and M. X. Liu and V. Loncar and J. P. Mitrevski and A. Olvera and M. L. Purschke and J. S. Renck and G. Roland and J. Schambach and Z. Shi and N. Tran and N. Wuerfel and B. Xu and D. Yu and H. Zhang},
      eprint={2501.04845},
      primaryclass={physics.ins-det},
      title={Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors},
      url={https://arxiv.org/abs/2501.04845},
      year={2025}
    }
  dataset:
  - name: Internal simulated tracking data
    url: sPHENIX and EIC DIS-electron tagger
  results:
  - name: Gemini LLM Deep Research
    url: ''
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: true
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 8.0
      reason: |-
        Task (trigger-level anomaly detection) is clearly defined for low-latency streaming input, but the problem framing lacks complete architectural/system specs.
    dataset:
      rating: 6.0
      reason: |-
        Internal DUNE SONIC data; not publicly released and no formal FAIR support; replicability is institutionally gated.
    metrics:
      rating: 7.0
      reason: |-
        Metrics include detection efficiency and latency, which are relevant, but only lightly supported by baselines or formal eval scripts.
    reference_solution:
      rating: 5.0
      reason: |-
        One CNN prototype demonstrated; LSTM planned. No public implementation or ready-to-run example yet.
    documentation:
      rating: 6.0
      reason: |-
        Slides and some internal documentation exist, but no full pipeline or public GitHub repo yet.
- date: '2025-01-09'
  last_updated: 2025-01
  expired: unkown
  valid: 'yes'
  name: Neural Architecture Codesign for Fast Physics Applications
  url: https://arxiv.org/abs/2501.05515
  domain: Physics; Materials Science; Particle Physics
  focus: |-
    Automated neural architecture search and hardware-efficient model codesign for fast physics applications
  keywords:
  - neural architecture search
  - FPGA deployment
  - quantization
  - pruning
  - hls4ml
  description: |-
    Introduces a two-stage neural architecture codesign (NAC) pipeline combining global and local search,
    quantization-aware training, and pruning to design efficient models for fast Bragg peak finding and
    jet classification, synthesized for FPGA deployment with hls4ml. Achieves >30x reduction in BOPs
    and sub-100 ns inference latency on FPGA.
  task_types:
  - Classification
  - Peak finding
  ai_capability_measured:
  - Hardware-aware model optimization; low-latency inference
  metrics:
  - Accuracy
  - Latency
  - Resource utilization
  models:
  - NAC-based BraggNN
  - NAC-optimized Deep Sets (jet)
  ml_motif:
  - Real-time, Image/CV
  type: Framework
  ml_task: Supervised Learning
  notes: |-
    Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced.
  contact:
    name: Jason Weitz (UCSD), Nhan Tran (FNAL)
    email: unkown
  cite:
  - |-
    @misc{weitz2025neuralarchitecturecodesignfast,
      archiveprefix={arXiv},
      author={Jason Weitz and Dmitri Demler and Luke McDermott and Nhan Tran and Javier Duarte},
      eprint={2501.05515},
      primaryclass={cs.LG},
      title={Neural Architecture Codesign for Fast Physics Applications},
      url={https://arxiv.org/abs/2501.05515},
      year={2025}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1X6RvGHaF1rZGYSorZSEEAxlwGMYau9RQHVOn82vWv2I/edit?usp=sharing
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: Yes (nac-opt, hls4ml)
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 10.0
      reason: |-
        Task is clearly defined (triggering on rare events with sub-10 micros latency); architecture, constraints, and system context (FPGA, Alveo) are well detailed.
    dataset:
      rating: 7.0
      reason: |-
        Simulated tracking data from sPHENIX and EIC; internally structured but not yet released in a public FAIR-compliant format.
    metrics:
      rating: 10.0
      reason: |-
        Accuracy, latency, and hardware resource utilization (LUTs, DSPs) are clearly defined and used in evaluation.
    reference_solution:
      rating: 9.0
      reason: |-
        Graph-based models (BGN-ST, GarNet) are implemented and tested on real hardware; reproducibility possible with hls4ml but full scripts not bundled.
    documentation:
      rating: 8.0
      reason: |-
        Paper is detailed and tool usage (FlowGNN, hls4ml) is described, but repo release and dataset access remain in progress.
- date: '2024-06-24'
  last_updated: 2024-06
  expired: unkown
  valid: 'yes'
  name: Smart Pixels for LHC
  url: https://arxiv.org/abs/2406.14860
  domain: Particle Physics; Instrumentation and Detectors
  focus: On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors
  keywords:
  - smart pixel
  - on-sensor inference
  - data reduction
  - trigger
  description: |-
    Presents a 256x256-pixel ROIC in 28 nm CMOS with embedded 2-layer NN for cluster filtering
    at 25 ns, achieving 54-75% data reduction while maintaining noise and latency constraints. Prototype
    consumes ~300 microW/pixel and operates in combinatorial digital logic.
  task_types:
  - Image Classification
  - Data filtering
  ai_capability_measured:
  - On-chip
  - low-power inference; data reduction
  metrics:
  - Data rejection rate
  - Power per pixel
  models:
  - 2-layer pixel NN
  ml_motif:
  - Real-time, Image/CV
  type: Benchmark
  ml_task: Image Classification
  notes: |-
    Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades.
  contact:
    name: Lindsey Gray; Jennet Dickinson
    email: unkown
  cite:
  - |-
    @misc{parpillon2024smartpixelsinpixelai,
      archiveprefix = {arXiv},
      author        = {Benjamin Parpillon and Chinar Syal and Jieun Yoo and Jennet Dickinson and Morris Swartz and Giuseppe Di Guglielmo and Alice Bean and Douglas Berry and Manuel Blanco Valentin and Karri DiPetrillo and Anthony Badea and Lindsey Gray and Petar Maksimovic and Corrinne Mills and Mark S. Neubauer and Gauri Pradhan and Nhan Tran and Dahai Wen and Farah Fahim},
      eprint        = {2406.14860},
      primaryclass  = {physics.ins-det},
      title         = {Smart Pixels: In-pixel AI for on-sensor data filtering},
      url           = {https://arxiv.org/abs/2406.14860},
      year          = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1Fevo7IGGAFC8pHrGGGA4t9V-nUwZkDezncAKDHN4v0E/edit?usp=sharing
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: true
    benchmark_ready: Yes (Zenodo:7331128)
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 9.0
      reason: |-
        Task (automated neural architecture search for real-time physics) is well formulated with clear latency, model compression, and deployment goals.
    dataset:
      rating: 6.0
      reason: |-
        Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant, though mentioned in the paper.
    metrics:
      rating: 10.0
      reason: |-
        BOP reduction, latency, and accuracy are all quantitatively evaluated.
    reference_solution:
      rating: 8.0
      reason: |-
        NAC-generated models for Bragg peak and jet classification are described, but pipeline requires integration of several tools and is not fully packaged.
    documentation:
      rating: 7.0
      reason: |-
        NAC pipeline, hls4ml usage, and results are discussed; code (e.g., nac-opt) referenced, but replication requires stitching together toolchain and data.
- date: '2023-10-03'
  last_updated: 2023-10
  expired: unkown
  valid: 'yes'
  name: HEDM (BraggNN)
  url: https://arxiv.org/abs/2008.08198
  domain: Material Science
  focus: Fast Bragg peak analysis using deep learning in diffraction microscopy
  keywords:
  - BraggNN
  - diffraction
  - peak finding
  - HEDM
  description: |-
    Uses BraggNN, a deep neural network, for rapid Bragg peak localization in 
    high-energy diffraction microscopy, achieving about 13x speedup compared 
    to Voigt-based methods while maintaining sub-pixel accuracy.
  task_types:
  - Peak detection
  ai_capability_measured:
  - High-throughput peak localization
  metrics:
  - Localization accuracy
  - Inference time
  models:
  - BraggNN
  ml_motif:
  - Real-time, Image/CV
  type: Framework
  ml_task: Peak finding
  notes: |-
    Enables real-time HEDM workflows; basis for NAC case study.
  contact:
    name: Jason Weitz (UCSD)
    email: unkown
  cite:
  - |-
    @misc{liu2021braggnnfastxraybragg,
      archiveprefix = {arXiv},
      author        = {Zhengchun Liu and Hemant Sharma and Jun-Sang Park and Peter Kenesei and Antonino Miceli and Jonathan Almer and Rajkumar Kettimuthu and Ian Foster},
      eprint        = {2008.08198},
      primaryclass  = {eess.IV},
      title         = {BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning},
      url           = {https://arxiv.org/abs/2008.08198},
      year          = {2021}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1wdUwyMyOi00QzQmkI8VBfwseTVXndxPAurwGsuvoQmQ/edit?usp=sharing
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 10.0
      reason: |-
        Fully specified: describes task (data filtering/classification, system design (on-sensor inference), 
        latency (25 ns), and power constraints.
    dataset:
      rating: 8.0
      reason: |-
        In-pixel charge cluster data used, but dataset release info is minimal; FAIR metadata/versioning limited.
    metrics:
      rating: 9.0
      reason: |-
        Data rejection rate and power per pixel are clearly defined and directly tied to hardware goals.
    reference_solution:
      rating: 9.0
      reason: |-
        2-layer NN implementation is evaluated in hardware; reproducible via hls4ml flow with results in paper.
    documentation:
      rating: 8.0
      reason: |-
        Paper is clear; Zenodo asset is referenced, but additional GitHub or setup repo would improve reproducibility.
- date: '2023-12-03'
  last_updated: 2023-12
  expired: unkown
  valid: 'yes'
  name: 4D-STEM
  url: "https://openreview.net/pdf?id=7yt3N0o0W9"
  domain: Material Science
  focus: Real-time ML for scanning transmission electron microscopy
  keywords:
  - 4D-STEM
  - electron microscopy
  - real-time
  - image processing
  description: |-
    Proposes ML methods for real-time analysis of 4D scanning transmission electron microscopy
    datasets; framework details in progress.
  task_types:
  - Image Classification
  - Streamed data inference
  ai_capability_measured:
  - Real-time large-scale microscopy inference
  metrics:
  - Classification accuracy
  - Throughput
  models:
  - CNN models (prototype)
  ml_motif:
  - Real-time, Image/CV
  type: Model
  ml_task: Image Classification
  notes: |-
    In-progress; model design under development.
  contact:
    name: unkown
    email: unkown
  cite:
  - |-
    @inproceedings{qin2023extremely,
      title={Extremely Noisy 4D-TEM Strain Mapping Using Cycle Consistent Spatial Transforming Autoencoders},
      author={Shuyu Qin and Joshua Agar and Nhan Tran},
      booktitle={AI for Accelerated Materials Design - NeurIPS 2023 Workshop},
      year={2023},
      url={https://openreview.net/forum?id=7yt3N0o0W9}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1RhoGej2LmTOb0ZF3mPzhPqV2aCct805dF40LARh_YZE/edit?usp=sharing
  - name: ChatGPT LLM
    url: ''
  fair:
    reproducible: in progress
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 9.0
      reason: |-
        Peak localization task is well-defined for diffraction images; input/output described clearly, but no system constraints.
    dataset:
      rating: 8.0
      reason: |-
        Simulated diffraction images provided; reusable and downloadable, but not externally versioned or FAIR-structured.
    metrics:
      rating: 9.0
      reason: |-
        Inference speed and localization accuracy are standard and quantitatively reported.
    reference_solution:
      rating: 8.0
      reason: |-
        BraggNN model and training pipeline exist, but need stitching from separate repositories.
    documentation:
      rating: 8.0
      reason: |-
        Paper and codebase are available and usable, though not fully turnkey.
- date: '2023-12-05'
  last_updated: 2023-12
  expired: unkown
  valid: 'yes'
  name: In-Situ High-Speed Computer Vision
  url: https://arxiv.org/abs/2312.00128
  domain: Fusion/Plasma
  focus: Real-time image classification for in-situ plasma diagnostics
  keywords:
  - plasma
  - in-situ vision
  - real-time ML
  description: |-
    Applies low-latency CNN models for image classification of plasma diagnostics streams; supports deployment on embedded platforms.
  task_types:
  - Image Classification
  ai_capability_measured:
  - Real-time diagnostic inference
  metrics:
  - Accuracy
  - FPS
  models:
  - CNN
  ml_motif:
  - Real-time, Image/CV
  type: Model
  ml_task: Image Classification
  notes: |-
    Embedded/deployment details in progress.
  contact:
    name: unkown
    email: unkown
  cite:
  - |-
    @misc{wei2024lowlatencyopticalbasedmode,
      archiveprefix = {arXiv},
      author        = {Yumou Wei and Ryan F. Forelli and Chris Hansen and Jeffrey P. Levesque and Nhan Tran and Joshua C. Agar and Giuseppe Di Guglielmo and Michael E. Mauel and Gerald A. Navratil},
      doi           = {https://doi.org/10.1063/5.0190354},
      eprint        = {2312.00128},
      primaryclass  = {physics.plasm-ph},
      title         = {Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak},
      url           = {https://arxiv.org/abs/2312.00128},
      year          = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1OcPX1eQpCcQpwZ19oOoUdzY3gcIxLCHA5R_JrCPVt2A/edit?usp=sharing
  - name: ChatGPT LLM
    url: |-
      https://docs.google.com/document/d/1EqkRHuQs1yQqMvZs_L6p9JAy2vKX5OCTubzttFBuRoQ/edit?usp=sharing
  fair:
    reproducible: in progress
    benchmark_ready: false
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 7.0
      reason: |-
        General task defined (real-time microscopy inference), but no standardized I/O format, latency constraint, or complete problem framing yet.
    dataset:
      rating: 0.0
      reason: |-
        Dataset not provided or described in any formal way.
    metrics:
      rating: 6.0
      reason: |-
        Mentions throughput and accuracy, but metrics are not formally defined or benchmarked.
    reference_solution:
      rating: 2.0
      reason: |-
        Prototype CNNs described; no baseline or implementation released.
    documentation:
      rating: 5.0
      reason: |-
        OpenReview paper and Gemini doc give some insight, but no working code, environment, or example.
- date: '2020-01-01'
  last_updated: 2020-01
  expired: unkown
  valid: 'yes'
  name: BenchCouncil AIBench
  url: https://www.benchcouncil.org/AIBench/
  domain: General
  focus: End-to-end AI benchmarking across micro, component, and application levels
  keywords:
  - benchmarking
  - AI systems
  - application-level evaluation
  description: |-
    AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component, application) across hardware systems-covering image generation, object detection, translation, recommendation, video prediction, etc.
  task_types:
  - Training
  - Inference
  - End-to-end AI workloads
  ai_capability_measured:
  - System-level AI workload performance
  metrics:
  - Throughput
  - Latency
  - Accuracy
  models:
  - ResNet
  - BERT
  - GANs
  - Recommendation systems
  ml_motif:
  - General
  type: Benchmark
  ml_task: NA
  notes: |-
    Covers scenario-distilling, micro, component, and end-to-end benchmarks.
  contact:
    name: Wanling Gao (BenchCouncil)
    email: unkown
  cite:
  - |-
    @misc{gao2019aibenchindustrystandardinternet,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Fei Tang and Lei Wang and Jianfeng Zhan and Chunxin Lan and Chunjie Luo and Yunyou Huang and Chen Zheng and Jiahui Dai and Zheng Cao and Daoyi Zheng and Haoning Tang and Kunlin Zhan and Biao Wang and Defei Kong and Tong Wu and Minghe Yu and Chongkang Tan and Huan Li and Xinhui Tian and Yatao Li and Junchao Shao and Zhenyu Wang and Xiaoyu Wang and Hainan Ye},
      eprint        = {1908.08998},
      primaryclass  = {cs.CV},
      title         = {AIBench: An Industry Standard Internet Service AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1908.08998},
      year          = {2019}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1scxhARd4vzEaWpVfwKPF_nTSxv4DirlQqcGlSG0yzJc/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 8.0
      reason: |-
        Task (plasma diagnostic classification) and real-time deployment described; system specs (FPS targets) implied but not fully quantified.
    dataset:
      rating: 6.0
      reason: |-
        Dataset is sensor stream-based but not shared or FAIR-documented.
    metrics:
      rating: 8.0
      reason: |-
        FPS and classification accuracy reported and relevant.
    reference_solution:
      rating: 7.0
      reason: |-
        CNN model described and evaluated, but public implementation and benchmarks are not available yet.
    documentation:
      rating: 6.0
      reason: |-
        Paper and Gemini doc exist, but full setup instructions and tools are still in progress.
- date: '2020-01-01'
  last_updated: 2020-01
  expired: unkown
  valid: 'yes'
  name: BenchCouncil BigDataBench
  url: https://www.benchcouncil.org/BigDataBench/
  domain: General
  focus: |-
    Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads
  keywords:
  - big data
  - AI benchmarking
  - data analytics
  description: |-
    BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI.
  task_types:
  - Data preprocessing
  - Inference
  - End-to-end data pipelines
  ai_capability_measured:
  - Data processing and AI model inference performance at scale
  metrics:
  - Data throughput
  - Latency
  - Accuracy
  models:
  - CNN
  - LSTM
  - SVM
  - XGBoost
  ml_motif:
  - General
  type: Benchmark
  ml_task: NA
  notes: |-
    Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
  contact:
    name: Jianfeng Zhan (BenchCouncil)
    email: unkown
  cite:
  - |-
    @misc{gao2018bigdatabenchscalableunifiedbig,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Jianfeng Zhan and Lei Wang and Chunjie Luo and Daoyi Zheng and Xu Wen and Rui Ren and Chen Zheng and Xiwen He and Hainan Ye and Haoning Tang and Zheng Cao and Shujie Zhang and Jiahui Dai},
      eprint        = {1802.08254},
      primaryclass  = {cs.DC},
      title         = {BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1802.08254},
      year          = {2018}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1FlvWeGm_J5QabOL7J0RWN3udzl0QFDs7wafptXx8sRU/edit?usp=sharing
  - name: ChatGPT LLM
    url: |-
      https://docs.google.com/document/d/1VFRxhR2G5A83S8PqKBrP99LLVgcCGvX2WW4vTtwxmQ4/edit?usp=sharing
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 9.0
      reason: |-
        Evaluates AI at multiple levels (micro to end-to-end); tasks and workloads are clearly defined, though specific I/O formats and constraints vary.
    dataset:
      rating: 9.0
      reason: |-
        Realistic datasets across diverse domains; FAIR structure for many components, but individual datasets may not all be versioned or richly annotated.
    metrics:
      rating: 9.0
      reason: |-
        Latency, throughput, and accuracy clearly defined for end-to-end tasks; consistent across models and setups.
    reference_solution:
      rating: 8.0
      reason: |-
        Reference implementations for several tasks exist, but setup across all tasks is complex and not fully streamlined.
    documentation:
      rating: 8.0
      reason: |-
        Central documentation exists, with detailed component breakdowns; environment setup across platforms (e.g., hardware variations) can require manual adjustment.
- date: '2021-10-20'
  last_updated: 2021-10
  expired: unkown
  valid: 'yes'
  name: MLPerf HPC
  url: https://github.com/mlcommons/hpc
  domain: Cosmology, Climate, Protein Structure, Catalysis
  focus: Scientific ML training and inference on HPC systems
  keywords:
  - HPC
  - training
  - inference
  - scientific ML
  description: |-
    MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations.
  task_types:
  - Training
  - Inference
  ai_capability_measured:
  - Scaling efficiency
  - training time
  - model accuracy on HPC
  metrics:
  - Training time
  - Accuracy
  - GPU utilization
  models:
  - CosmoFlow
  - DeepCAM
  - OpenCatalyst
  ml_motif:
  - HPC/inference, HPC/training
  type: Framework
  ml_task: NA
  notes: |-
    Shared framework with MLCommons Science; reference implementations included.
  contact:
    name: Steven Farrell (MLCommons)
    email: unkown
  cite:
  - |-
    @misc{farrell2021mlperfhpcholisticbenchmark,
      archiveprefix = {arXiv},
      author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendonça and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},
      eprint        = {2110.11466},
      primaryclass  = {cs.LG},
      title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},
      url           = {https://arxiv.org/abs/2110.11466},
      year          = {2021}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: See MLCommons Science entry below
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 9.0
      reason: |-
        Focused on structured/unstructured data pipelines; clearly defined tasks spanning analytics to AI; some scenarios lack hardware constraint modeling.
    dataset:
      rating: 9.0
      reason: |-
        Built from 13 real-world sources; structured for realistic big data scenarios; partially FAIR-compliant with documented data motifs.
    metrics:
      rating: 9.0
      reason: |-
        Covers data throughput, latency, and accuracy; quantitative and benchmark-ready.
    reference_solution:
      rating: 8.0
      reason: |-
        Many pipeline and model examples provided using Hadoop/Spark/Flink; setup effort varies by task and platform.
    documentation:
      rating: 8.0
      reason: |-
        Strong documentation with examples and task specifications; centralized support exists, but task-specific tuning may require domain expertise.
- date: '2023-06-01'
  last_updated: 2023-06
  expired: unkown
  valid: 'yes'
  name: MLCommons Science
  url: https://github.com/mlcommons/science
  domain: Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD
  focus: |-
    AI benchmarks for scientific applications including time-series, imaging, and simulation
  keywords:
  - science AI
  - benchmark
  - MLCommons
  - HPC
  description: |-
    MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.
  task_types:
  - Time-series analysis
  - Image classification
  - Simulation surrogate modeling
  ai_capability_measured:
  - Inference accuracy
  - simulation speed-up
  - generalization
  metrics:
  - MAE
  - Accuracy
  - Speedup vs simulation
  models:
  - CNN
  - GNN
  - Transformer
  ml_motif:
  - Time-series, Image/CV, HPC/inference
  type: Framework
  ml_task: NA
  notes: |-
    Joint national-lab effort under Apache-2.0 license.
  contact:
    name: MLCommons Science Working Group
    email: unkown
  cite:
  - |-
    @InProceedings{10.1007/978-3-031-23220-6_4,
      author="Thiyagalingam, Jeyan
      and von Laszewski, Gregor
      and Yin, Junqi
      and Emani, Murali
      and Papay, Juri
      and Barrett, Gregg
      and Luszczek, Piotr
      and Tsaris, Aristeidis
      and Kirkpatrick, Christine
      and Wang, Feiyi
      and Gibbs, Tom
      and Vishwanath, Venkatram
      and Shankar, Mallikarjun
      and Fox, Geoffrey
      and Hey, Tony",
      editor="Anzt, Hartwig
      and Bienz, Amanda
      and Luszczek, Piotr
      and Baboulin, Marc",
      title="AI Benchmarking for Science: Efforts from the MLCommons Science Working Group",
      booktitle="High Performance Computing. ISC High Performance 2022 International Workshops",
      year="2022",
      publisher="Springer International Publishing",
      address="Cham",
      pages="47--64",
      abstract="With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.",
      isbn="978-3-031-23220-6"
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed. 
    specification:
      rating: 10.0
      reason: |-
        Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly defined with HPC system-level constraints and targets.
    dataset:
      rating: 9.0
      reason: |-
        Public scientific datasets (e.g., cosmology, weather); used consistently, though FAIR-compliance of individual datasets varies slightly.
    metrics:
      rating: 10.0
      reason: |-
        Training time, GPU utilization, and accuracy are all directly measured and benchmarked across HPC systems.
    reference_solution:
      rating: 9.0
      reason: |-
        Reference implementations available and actively maintained; HPC setup may require domain-specific environment.
    documentation:
      rating: 9.0
      reason: |-
        GitHub repo and papers provide detailed instructions; reproducibility supported across multiple institutions.
- date: '2021-07-05'
  last_updated: 2021-07
  expired: unkown
  valid: 'yes'
  name: LHC New Physics Dataset
  url: https://arxiv.org/pdf/2107.02157
  domain: Particle Physics; Real-time Triggering
  focus: Real-time LHC event filtering for anomaly detection using proton collision data
  keywords:
  - anomaly detection
  - proton collision
  - real-time inference
  - event filtering
  - unsupervised ML
  description: |-
    A dataset of proton-proton collision events emulating a 40 MHz real-time data stream from LHC detectors, pre-filtered on electron or muon presence. Designed for unsupervised new-physics detection algorithms under latency/bandwidth constraints.
  task_types:
  - Anomaly detection
  - Event classification
  ai_capability_measured:
  - Unsupervised signal detection under latency and bandwidth constraints
  metrics:
  - ROC-AUC
  - Detection efficiency
  models:
  - Autoencoder
  - Variational autoencoder
  - Isolation forest
  ml_motif:
  - Multiple
  type: Framework
  ml_task: NA
  notes: |-
    Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box.
  contact:
    name: Ema Puljak (ema.puljak@cern.ch)
    email: unkown
  cite:
  - |-
    @misc{https://doi.org/10.5281/zenodo.5046389,
      author    = {Aarrestad, Thea and Govorkova, Ekaterina and Ngadiuba, Jennifer and Puljak, Ema and Pierini, Maurizio and Wozniak, Kinga Anna},
      copyright = {Creative Commons Attribution 4.0 International},
      doi       = {10.5281/ZENODO.5046389},
      publisher = {Zenodo},
      title     = {Unsupervised New Physics detection at 40 MHz: Training Dataset},
      url       = {https://zenodo.org/record/5046389},
      year      = {2021}
    }
  dataset:
  - name: 'Zenodo stores: background + 3 black-box signal sets'
    url: 1M events each
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1BnX67GfTQxHbDuUsH-MuHIl1uKxCIjrXHSoxvIaB72g/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analysed. 
    specification:
      rating: 7.0
      reason: |-
        The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but lacks a formal task specification or constraints.
    dataset:
      rating: 8.0
      reason: |-
        Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo.
    metrics:
      rating: 7.0
      reason: |-
        Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script.
    reference_solution:
      rating: 5.0
      reason: |-
        Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers.
    documentation:
      rating: 6.0
      reason: |-
        Publicly available papers and datasets with descriptions, but no unified README or training setup.
- date: '2023-07-17'
  last_updated: 2023-07
  expired: unkown
  valid: 'yes'
  name: MLCommons Medical AI
  url: https://github.com/mlcommons/medical
  domain: Healthcare; Medical AI
  focus: |-
    Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data
  keywords:
  - medical AI
  - federated evaluation
  - privacy-preserving
  - fairness
  - healthcare benchmarks
  description: |-
    The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE)
    to accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical
    models on diverse datasets, improving generalizability and equity while keeping data onsite :contentReference[oaicite:1]{index=1}.
  task_types:
  - Federated evaluation
  - Model validation
  ai_capability_measured:
  - Clinical accuracy
  - fairness
  - generalizability
  - privacy compliance
  metrics:
  - ROC AUC
  - Accuracy
  - Fairness metrics
  models:
  - MedPerf-validated CNNs
  - GaNDLF workflows
  ml_motif:
  - Multiple
  type: Platform
  ml_task: NA
  notes: |-
    Open-source platform under Apache-2.0; used across 20+ institutions and hospitals :contentReference[oaicite:2]{index=2}.
  contact:
    name: Alex Karargyris (MLCommons Medical AI)
    email: unkown
  cite:
  - |-
    @article{karargyris2023federated,
      author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J. and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and Narayana Moorthy, Prakash and Chowdhury, Alexander and Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and Kanter, David and Xenochristou, Maria and Beutel, Daniel J. and Chung, Verena and Bergquist, Timothy and Eddy, James and Abid, Abubakar and Tunstall, Lewis and Sanseviero, Omar and Dimitriadis, Dimitrios and Qian, Yiming and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and Bala, Srini and Bittorf, Victor and Puchala, Sreekar Reddy and Ricciuti, Biagio and Samineni, Soujanya and Sengupta, Eshna and Chaudhari, Akshay and Coleman, Cody and Desinghu, Bala and Diamos, Gregory and Dutta, Debo and Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and Kashyap, Satyananda and Lane, Nicholas and Mallick, Indranil and Mascagni, Pietro and Mehta, Virendra and Moraes, Cassiano Ferro and Natarajan, Vivek and Nikolov, Nikola and Padoy, Nicolas and Pekhimenko, Gennady and Reddi, Vijay Janapa and Reina, G. Anthony and Ribalta, Pablo and Singh, Abhishek and Thiagarajan, Jayaraman J. and Albrecht, Jacob and Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and Shah, Prashant and Xu, Daguang and Yadav, Poonam and Talby, David and Awad, Mark M. and Howard, Jeremy P. and Rosenthal, Michael and Marchionni, Luigi and Loda, Massimo and Johnson, Jason M. and Bakas, Spyridon and Mattson, Peter and FeTS Consortium and BraTS-2020 Consortium and AI4SafeChole Consortium},
      month = jul,
      doi = {10.1038/s42256-023-00652-2},
      journal = {Nature Machine Intelligence},
      number = {7},
      pages = {799--810},
      title = {Federated benchmarking of medical artificial intelligence with MedPerf},
      url = {https://doi.org/10.1038/s42256-023-00652-2},
      volume = {5},
      year = {2023},
    }
  dataset:
  - name: Multi-institutional clinical datasets
    url: radiology
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Diverse scientific tasks (earthquake, CFD, microscopy) with detailed problem statements and goals; system constraints not uniformly applied.
    dataset:
      rating: 9.0
      reason: |-
        Domain-specific datasets (e.g., microscopy, climate); mostly public and structured, but FAIR annotations are not always explicit.
    metrics:
      rating: 9.0
      reason: |-
        Task-specific metrics (MAE, speedup, accuracy) are clear and reproducible.
    reference_solution:
      rating: 9.0
      reason: |-
        Reference models (CNN, GNN, Transformer) provided with training/evaluation pipelines.
    documentation:
      rating: 9.0
      reason: |-
        Well-documented, open-sourced, and maintained with examples; strong community support and reproducibility focus.
- date: '2024-10-28'
  last_updated: 2024-10
  expired: unkown
  valid: 'yes'
  name: CaloChallenge 2022
  url: http://arxiv.org/abs/2410.21611
  domain: LHC Calorimeter; Particle Physics
  focus: Fast generative-model-based calorimeter shower simulation evaluation
  keywords:
  - calorimeter simulation
  - generative models
  - surrogate modeling
  - LHC
  - fast simulation
  description: |-
    The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative-model submissions (VAEs, GANs, Flows, Diffusion)
    on four calorimeter shower datasets; benchmarking shower quality, generation speed, and model complexity :contentReference[oaicite:3]{index=3}.
  task_types:
  - Surrogate modeling
  ai_capability_measured:
  - Simulation fidelity
  - speed
  - efficiency
  metrics:
  - Histogram similarity
  - Classifier AUC
  - Generation latency
  models:
  - VAE variants
  - GAN variants
  - Normalizing flows
  - Diffusion models
  ml_motif:
  - Surrogate
  type: Dataset
  ml_task: Surrogate Modeling
  notes: |-
    The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset sizes.
  contact:
    name: Claudius Krause (CaloChallenge Lead)
    email: unkown
  cite:
  - |-
    @misc{krause2024calochallenge2022communitychallenge,
      archiveprefix = {arXiv},
      author        = {Claudius Krause and Michele Faucci Giannelli and Gregor Kasieczka and Benjamin Nachman and Dalila Salamani and David Shih and Anna Zaborowska and Oz Amram and Kerstin Borras and Matthew R. Buckley and Erik Buhmann and Thorsten Buss and Renato Paulo Da Costa Cardoso and Anthony L. Caterini and Nadezda Chernyavskaya and Federico A. G. Corchia and Jesse C. Cresswell and Sascha Diefenbacher and Etienne Dreyer and Vijay Ekambaram and Engin Eren and Florian Ernst and Luigi Favaro and Matteo Franchini and Frank Gaede and Eilam Gross and Shih-Chieh Hsu and Kristina Jaruskova and Benno Käch and Jayant Kalagnanam and Raghav Kansal and Taewoo Kim and Dmitrii Kobylianskii and Anatolii Korol and William Korcari and Dirk Krücker and Katja Krüger and Marco Letizia and Shu Li and Qibin Liu and Xiulong Liu and Gabriel Loaiza-Ganem and Thandikire Madula and Peter McKeown and Isabell-A. Melzer-Pellmann and Vinicius Mikuni and Nam Nguyen and Ayodele Ore and Sofia Palacios Schweitzer and Ian Pang and Kevin Pedro and Tilman Plehn and Witold Pokorski and Huilin Qu and Piyush Raikwar and John A. Raine and Humberto Reyes-Gonzalez and Lorenzo Rinaldi and Brendan Leigh Ross and Moritz A. W. Scham and Simon Schnake and Chase Shimmin and Eli Shlizerman and Nathalie Soybelman and Mudhakar Srivatsa and Kalliopi Tsolaki and Sofia Vallecorsa and Kyongmin Yeo and Rui Zhang},
      eprint        = {2410.21611},
      primaryclass  = {physics.ins-det},
      title         = {CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation},
      url           = {https://arxiv.org/abs/2410.21611},
      year          = {2024}
    }
  dataset:
  - name: Four LHC calorimeter shower datasets
    url: various voxel resolutions
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1JBH3WTDp2jpSt_utc1p5Dv3-MBX4xY-NVzzfXCd9xhA/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Task is clearly defined: real-time anomaly detection from high-rate LHC collisions. Latency and bandwidth constraints are mentioned, though not numerically enforced.
    dataset:
      rating: 9.0
      reason: |-
        Publicly available via Zenodo, with structured signal/background splits, and rich metadata; nearly fully FAIR.
    metrics:
      rating: 9.0
      reason: |-
        ROC-AUC and detection efficiency are clearly defined and appropriate for unsupervised anomaly detection.
    reference_solution:
      rating: 8.0
      reason: |-
        Several baseline methods (autoencoder, VAE, isolation forest) are evaluated; runnable versions available via community repos but not tightly bundled.
    documentation:
      rating: 8.0
      reason: |-
        Paper and data documentation are clear, and the dataset is widely reused. Setup requires some manual effort to reproduce full pipelines.
- date: ongoing
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: Papers With Code (SOTA Platform)
  url: https://paperswithcode.com/sota
  domain: General ML; All domains
  focus: |-
    Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers
  keywords:
  - leaderboard
  - benchmarking
  - reproducibility
  - open-source
  description: |-
    Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research:
    12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.
  task_types:
  - Multiple (Classification, Detection, NLP, etc.)
  ai_capability_measured:
  - Model performance across tasks (accuracy
  - F1
  - BLEU
  - etc.)
  metrics:
  - Task-specific (Accuracy, F1, BLEU, etc.)
  models:
  - All published models with code
  ml_motif:
  - Multiple
  type: Platform
  ml_task: Multiple
  notes: |-
    Community-driven open platform; automatic data extraction and versioning.
  contact:
    name: Papers With Code Team
    email: unkown
  cite:
  - |-
    @InProceedings{pmlr-v37-blum15,
      title =    {The Ladder: A Reliable Leaderboard for Machine Learning Competitions},
      author =   {Blum, Avrim and Hardt, Moritz},
      booktitle =        {Proceedings of the 32nd International Conference on Machine Learning},
      pages =    {1006--1014},
      year =     {2015},
      editor =   {Bach, Francis and Blei, David},
      volume =   {37},
      series =   {Proceedings of Machine Learning Research},
      address =          {Lille, France},
      month =    jul,
      publisher =    {PMLR},
      pdf =      {http://proceedings.mlr.press/v37/blum15.pdf},
      url =      {https://proceedings.mlr.press/v37/blum15.html},
      abstract =         {The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1pbn_K20t6Kr0zxdUSAL68ChruOu6B_x5sZbp-dcrG_g/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Evaluation setting (federated clinical benchmarking) is well-defined; I/O interfaces vary slightly by task but are standardized in MedPerf platform.
    dataset:
      rating: 8.0
      reason: |-
        Uses distributed, real-world clinical datasets across institutions; FAIR compliance varies across hospitals and data hosts.
    metrics:
      rating: 9.0
      reason: |-
        ROC AUC, accuracy, and fairness metrics are explicitly defined and task-dependent; consistently tracked across institutions.
    reference_solution:
      rating: 8.0
      reason: |-
        Validated CNNs and GaNDLF pipelines are used and shared via the MedPerf tool, but some implementations are abstracted behind the platform.
    documentation:
      rating: 9.0
      reason: |-
        Excellent documentation across MedPerf, GaNDLF, and COFE; reproducibility handled via containerized flows and task templates.
- date: '2022-01-01'
  last_updated: 2025-03
  expired: unkown
  valid: 'yes'
  name: Codabench
  url: https://www.codabench.org/
  domain: General ML; Multiple
  focus: Open-source platform for organizing reproducible AI benchmarks and competitions
  keywords:
  - benchmark platform
  - code submission
  - competitions
  - meta-benchmark
  description: |-
    Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks
    and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues :contentReference[oaicite:1]{index=1}.
  task_types:
  - Multiple
  ai_capability_measured:
  - Model reproducibility
  - performance across datasets
  metrics:
  - Submission count
  - Leaderboard ranking
  - Task-specific metrics
  models:
  - Arbitrary code submissions
  ml_motif:
  - Multiple
  type: Platform
  ml_task: Multiple
  notes: |-
    Hosts 51 public competitions, ~26 k users, 177 k submissions :contentReference[oaicite:2]{index=2}
  contact:
    name: Isabelle Guyon (Université Paris-Saclay)
    email: unkown
  cite:
  - |-
    @article{xu-2022,
      author    = {Xu, Zhen and Escalera, Sergio and Pavão, Adrien and Richard, Magali and Tu, Wei-Wei and Yao, Quanming and Zhao, Huan and Guyon, Isabelle},
      doi       = {10.1016/j.patter.2022.100543},
      issn      = {2666-3899},
      journal   = {Patterns},
      month     = jul,
      number    = {7},
      pages     = {100543},
      publisher = {Elsevier BV},
      title     = {Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},
      url       = {http://dx.doi.org/10.1016/j.patter.2022.100543},
      volume    = {3},
      year      = {2022}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1sIwNDCs01s790DApVt5leiG8UaBDFVINA4bOixZ1CUw/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 10.0
      reason: |-
        Simulation task (generative calorimeter showers) is clearly stated with multiple datasets, fidelity requirements, and performance constraints.
    dataset:
      rating: 9.5
      reason: |-
        Public datasets available in multiple sizes and formats; well-documented; not versioned
    metrics:
      rating: 10.0
      reason: |-
        Histogram similarity, classifier AUC, and generation latency are clearly defined and benchmarked across all submissions.
    reference_solution:
      rating: 9.0
      reason: |-
        31 model implementations submitted; some made public and reproducible, though others remain undocumented or private.
    documentation:
      rating: 9.0
      reason: |-
        Paper, leaderboard, and Gemini doc are comprehensive; unified repo or launchable baseline kit would push this to a 10.
- date: '2021-09-27'
  last_updated: 2023-07
  expired: unkown
  valid: 'yes'
  name: Sabath (SBI-FAIR)
  url: https://sbi-fair.github.io/docs/software/sabath/
  domain: Systems; Metadata
  focus: FAIR metadata framework for ML-driven surrogate workflows in HPC systems
  keywords:
  - meta-benchmark
  - metadata
  - HPC
  - surrogate modeling
  description: |-
    Sabath is a metadata framework from the SBI-FAIR group (UTK, Argonne, Virginia) facilitating
    FAIR-compliant benchmarking and surrogate execution logging across HPC systems :contentReference[oaicite:3]{index=3}.
  task_types:
  - Systems benchmarking
  ai_capability_measured:
  - Metadata tracking
  - reproducible HPC workflows
  metrics:
  - Metadata completeness
  - FAIR compliance
  models:
  - N/A
  ml_motif:
  - Systems
  type: Platform
  ml_task: NA
  notes: |-
    Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc. :contentReference[oaicite:4]{index=4}
  contact:
    name: Piotr Luszczek (luszczek@utk.edu)
    email: unkown
  cite:
  - |-
    @techreport{luszczek2021sabath,
      title={SABATH: FAIR Metadata Technology for Surrogate Benchmarks},
      author={Luszczek, Piotr},
      year={2021},
      institution={University of Tennessee},
      url={https://github.com/icl-utk-edu/slip/tree/sabath}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: (none)
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: N/A
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle physics datasets.
    dataset:
      rating: 8.0
      reason: |-
        Data is well-structured for SBI and publicly available with clear licensing.
    metrics:
      rating: 8.0
      reason: |-
        Includes likelihood and posterior accuracy; metrics well-matched to SBI.
    reference_solution:
      rating: 7.0
      reason: |-
        Baseline SBI models are implemented and reproducible.
    documentation:
      rating: 6.0
      reason: |-
        GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs.
- date: '2022-10-13'
  last_updated: 2025-05
  expired: unkown
  valid: 'yes'
  name: PDEBench
  url: https://github.com/pdebench/PDEBench
  domain: CFD; Weather Modeling
  focus: Benchmark suite for ML-based surrogates solving time-dependent PDEs
  keywords:
  - PDEs
  - CFD
  - scientific ML
  - surrogate modeling
  - NeurIPS
  description: |-
    PDEBench offers forward/inverse PDE tasks with large ready-to-use datasets and baselines (FNO, U-Net, PINN), packaged via a unified API. It won the SimTech Best Paper Award 2023 :contentReference[oaicite:5]{index=5}.
  task_types:
  - Supervised Learning
  ai_capability_measured:
  - Time-dependent PDE modeling; physical accuracy
  metrics:
  - RMSE
  - boundary RMSE
  - Fourier RMSE
  models:
  - FNO
  - U-Net
  - PINN
  - Gradient-Based inverse methods
  ml_motif:
  - Multiple
  type: Framework
  ml_task: Supervised Learning
  notes: |-
    Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email :contentReference[oaicite:6]{index=6}
  contact:
    name: Makoto Takamoto (makoto.takamoto@neclab.eu)
    email: unkown
  cite:
  - |-
    @misc{takamoto2024pdebenchextensivebenchmarkscientific,
      archiveprefix = {arXiv},
      author        = {Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pflüger and Mathias Niepert},
      eprint        = {2210.07182},
      primaryclass  = {cs.LG},
      title         = {PDEBENCH: An Extensive Benchmark for Scientific Machine Learning},
      url           = {https://arxiv.org/abs/2210.07182},
      year          = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1MvXdFub0PxUDtB49wqli6mmSCdLErv2nLdOJUtMylOo/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Clearly defined PDE-solving tasks with well-specified constraints and solution formats.
    dataset:
      rating: 9.0
      reason: |-
        Includes synthetic and real-world PDE datasets with detailed format descriptions.
    metrics:
      rating: 8.0
      reason: |-
        Uses L2 error and other norms relevant to PDE solutions.
    reference_solution:
      rating: 7.0
      reason: |-
        Includes baseline solvers and trained models across multiple PDE tasks.
    documentation:
      rating: 8.0
      reason: |-
        Well-organized GitHub with examples, dataset loading scripts, and training configs.
- date: '2024-12-03'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: The Well
  url: https://polymathic-ai.org/the_well/
  domain: biological systems, fluid dynamics, acoustic scattering, astrophysical MHD
  focus: Foundation model + surrogate dataset spanning 16 physical simulation domains
  keywords:
  - surrogate modeling
  - foundation model
  - physics simulations
  - spatiotemporal dynamics
  description: |-
    A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains-from biology to astrophysical magnetohydrodynamic simulations-with unified API and metadata. Ideal for training surrogate and foundation models on scientific data. :contentReference[oaicite:1]{index=1}
  task_types:
  - Supervised Learning
  ai_capability_measured:
  - Surrogate modeling
  - physics-based prediction
  metrics:
  - Dataset size
  - Domain breadth
  models:
  - FNO baselines
  - U-Net baselines
  ml_motif:
  - Foundation model, Surrogate
  type: Dataset
  ml_task: Supervised Learning
  notes: |-
    Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. :contentReference[oaicite:2]{index=2}
  contact:
    name: Wes Brewer
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_4f9a5acd,
      author = {Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina J. and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesley and Dalziel, Stuart B. and Fielding, Drummond B. and Fortunato, Daniel and Goldberg, Jared A. and Hirashima, Keiya and Jiang, Yan-Fei and Kerswell, Rich R. and Maddu, Suryanarayana and Miller, Jonah and Mukhopadhyay, Payel and Nixon, Stefan S. and Shen, Jeff and Watteaux, Romain and Blancard, Bruno R\'{e}galdo-Saint and Rozet, Fran\c{c}ois and Parker, Liam H. and Cranmer, Miles and Ho, Shirley},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {44989--45037},
      publisher = {Curran Associates, Inc.},
      title = {The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset:
  - name: 16 simulation datasets
    url: HDF5) via PyPI/GitHub
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1axQvD_aY9O71A2nxWaoFvsfE8HvZ4TmwDoZ4cQQwW58/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 7.0
      reason: |-
        Explores LLM understanding of mental health scenarios; framing is creative but loosely defined.
    dataset:
      rating: 6.0
      reason: |-
        Dataset is described in concept but not released; privacy limits public access though synthetic proxies are referenced.
    metrics:
      rating: 7.0
      reason: |-
        Uses manual annotation and quality scores, but lacks standardized automatic metrics.
    reference_solution:
      rating: 6.0
      reason: |-
        Provides few-shot prompt examples and human rating calibration details.
    documentation:
      rating: 5.0
      reason: |-
        Paper gives use cases, but code and data are not yet public.
- date: '2024-10-31'
  last_updated: 2024-11
  expired: unkown
  valid: 'yes'
  name: LLM-Inference-Bench
  url: https://github.com/argonne-lcf/LLM-Inference-Bench
  domain: LLM; HPC/inference
  focus: Hardware performance benchmarking of LLMs on AI accelerators
  keywords:
  - LLM
  - inference benchmarking
  - GPU
  - accelerator
  - throughput
  description: |-
    A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics. :contentReference[oaicite:3]{index=3}
  task_types:
  - Inference Benchmarking
  ai_capability_measured:
  - Inference throughput
  - latency
  - hardware utilization
  metrics:
  - Token throughput (tok/s)
  - Latency
  - Framework-hardware mix performance
  models:
  - LLaMA-2-7B
  - LLaMA-2-70B
  - Mistral-7B
  - Qwen-7B
  ml_motif:
  - HPC/inference
  type: Dataset
  ml_task: Inference Benchmarking
  notes: |-
    Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators. :contentReference[oaicite:4]{index=4}
  contact:
    name: Krishna Teja Chitty-Venkata (Argonne LCF)
    email: unkown
  cite:
  - |-
    @INPROCEEDINGS{10820566,
      author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},
      booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
      title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, 
      year={2024},
      volume={},
      number={},
      pages={1362-1379},
      keywords={Performance evaluation;Power demand;Computational modeling;Large language models;Scalability;High performance computing;AI accelerators;Benchmark testing;Propulsion;Throughput;Large Language Models;AI Accelerators;Inference Performance Evaluation;Benchmarking},
      doi={10.1109/SCW63240.2024.00178}}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1I3UvByGn4KaruQC1pi6XcfoAOzt4iiA61S0nR9ovC94/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        PDE tasks (forward/inverse) and I/O structures are clearly specified with detailed PDE context and constraints.
    dataset:
      rating: 10.0
      reason: |-
        Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant.
    metrics:
      rating: 9.0
      reason: |-
        Uses RMSE variants and Fourier-based errors.
    reference_solution:
      rating: 10.0
      reason: |-
        Baselines (FNO, U-Net, PINN) implemented and ready-to-run; strong community adoption.
    documentation:
      rating: 9.0
      reason: |-
        Clean GitHub with usage, dataset links, and tutorial notebooks.
- date: '2023-12-12'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: SGLang Framework
  url: https://github.com/sgl-project/sglang/tree/main/benchmark
  domain: LLM Vision
  focus: Fast serving framework for LLMs and vision-language models
  keywords:
  - LLM serving
  - vision-language
  - RadixAttention
  - performance
  - JSON decoding
  description: |-
    A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching, quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives. :contentReference[oaicite:5]{index=5}
  task_types:
  - Model serving framework
  ai_capability_measured:
  - Serving throughput
  - JSON/task-specific latency
  metrics:
  - Tokens/sec
  - Time-to-first-token
  - Throughput gain vs baseline
  models:
  - LLaVA
  - DeepSeek
  - Llama
  ml_motif:
  - LLM Vision
  type: Framework
  ml_task: Model serving
  notes: |-
    Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025. :contentReference[oaicite:6]{index=6}
  contact:
    name: SGLang Team
    email: unkown
  cite:
  - |-
    @misc{zheng2024sglangefficientexecutionstructured,
      archiveprefix = {arXiv},
      author        = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      eprint        = {2312.07104},
      primaryclass  = {cs.AI},
      title         = {SGLang: Efficient Execution of Structured Language Model Programs},
      url           = {https://arxiv.org/abs/2312.07104},
      year          = {2024}
    } 
  dataset:
  - name: Benchmark configs
    url: dummy or real
  results:
  - name: Gemini LLM Deep Research
    url: (none)
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Clearly framed around surrogate learning across 16 domains, but not all tasks are formally posed or constrained in a unified benchmark protocol. Paper mentions performance on NVIDIA H100.
    dataset:
      rating: 9.0
      reason: |-
        FAIR-compliant physics simulation dataset, structured in HDF5 with unified metadata.
    metrics:
      rating: 7.0
      reason: |-
        Metrics like dataset size and domain coverage are listed, but standardized quantitative model evaluation metrics (e.g., RMSE, MAE) are not enforced.
    reference_solution:
      rating: 9.0
      reason: |-
        FNO and U-Net baselines available; full benchmarking implementations pending NeurIPS paper code release.
    documentation:
      rating: 10.0
      reason: |-
        Site and GitHub offer a unified API, metadata standards, and dataset loading tools; NeurIPS paper adds detailed design context.
- date: '2023-09-12'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: vLLM Inference and Serving Engine
  url: https://github.com/vllm-project/vllm/tree/main/benchmarks
  domain: LLM; HPC/inference
  focus: High-throughput, memory-efficient inference and serving engine for LLMs
  keywords:
  - LLM inference
  - PagedAttention
  - CUDA graph
  - streaming API
  - quantization
  description: "vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large\
    \ language models, \nfeaturing PagedAttention, continuous batching, and support for quantized and\
    \ pipelined model execution. \nBenchmarks compare it to TensorRT-LLM, SGLang, and others. :contentReference[oaicite:1]{index=1}"
  task_types:
  - Inference Benchmarking
  ai_capability_measured:
  - Throughput
  - latency
  - memory efficiency
  metrics:
  - Tokens/sec
  - Time to First Token (TTFT)
  - Memory footprint
  models:
  - LLaMA
  - Mixtral
  - FlashAttention-based models
  ml_motif:
  - HPC/inference
  type: Framework
  ml_task: Inference
  notes: |-
    Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers :contentReference[oaicite:2]{index=2}
  contact:
    name: Woosuk Kwon (vLLM Team)
    email: unkown
  cite:
  - |-
    @inproceedings{10.1145/3600006.3613165,
      author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
      title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
      year = {2023},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3600006.3613165},
      doi = {10.1145/3600006.3613165},
      abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
      booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
      pages = {611-626},
      numpages = {16},
      location = {Koblenz, Germany},
      series = {SOSP '23}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1nPZyRZEZHciGXYNJShR9FCJbre7ImJLKf2MG6g4-3gQ/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Benchmarks hardware performance of LLM inference across multiple platforms with well-defined input/output and platform constraints.
    dataset:
      rating: 7.0
      reason: |-
        Uses structured log files and configs instead of conventional datasets; suitable for inference benchmarking.
    metrics:
      rating: 9.0
      reason: |-
        Clear throughput, latency, and utilization metrics; platform comparison dashboard enhances evaluation.
    reference_solution:
      rating: 8.0
      reason: |-
        Includes reproducible scripts and example runs; models like LLaMA and Mistral are referenced with platform-specific configs.
    documentation:
      rating: 8.0
      reason: |-
        GitHub contains clear instructions, platform details, and framework comparisons.
- date: '2022-06-22'
  last_updated: 2025-01
  expired: unkown
  valid: 'yes'
  name: vLLM Performance Dashboard
  url: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/
  domain: LLM; HPC/inference
  focus: Interactive dashboard showing inference performance of vLLM
  keywords:
  - Dashboard
  - Throughput visualization
  - Latency analysis
  - Metric tracking
  description: |-
    A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and hardware configurations.
  task_types:
  - Performance visualization
  ai_capability_measured:
  - Throughput
  - latency
  - hardware utilization
  metrics:
  - Tokens/sec
  - TTFT
  - Memory usage
  models:
  - LLaMA-2
  - Mistral
  - Qwen
  ml_motif:
  - HPC/inference
  type: Framework
  ml_task: Visualization
  notes: |-
    Built using ObservableHQ; integrates live data from vLLM benchmarks.
  contact:
    name: Simon Mo
    email: unkown
  cite:
  - |-
    @misc{mo2024vllm_dashboard,
      title={vLLM Performance Dashboard},
      author={Mo, Simon},
      year={2024},
      url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: (none)
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Framed as a model-serving tool rather than a benchmark, but includes benchmark configurations and real model tasks.
    dataset:
      rating: 6.0
      reason: |-
        Mostly uses dummy configs or external model endpoints for evaluation; not designed around a formal dataset.
    metrics:
      rating: 8.0
      reason: |-
        Well-defined serving metrics: tokens/sec, time-to-first-token, and gain over baselines.
    reference_solution:
      rating: 9.0
      reason: |-
        Core framework includes full reproducible serving benchmarks and code; multiple deployment case studies.
    documentation:
      rating: 9.0
      reason: |-
        High-quality usage guides, examples, and performance tuning docs.
- date: '2022-04-01'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: Nixtla NeuralForecast
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series forecasting; General ML
  focus: High-performance neural forecasting library with >30 models
  keywords:
  - time-series
  - neural forecasting
  - NBEATS, NHITS, TFT
  - probabilistic forecasting
  - usability
  description: |-
    NeuralForecast offers scalable, user-friendly implementations of over 30 neural forecasting models (NBEATS, NHITS, TFT, DeepAR, etc.),
    emphasizing quality, usability, interpretability, and performance.
  task_types:
  - Time-series forecasting
  ai_capability_measured:
  - Forecast accuracy
  - interpretability
  - speed
  metrics:
  - RMSE
  - MAPE
  - CRPS
  models:
  - NBEATS
  - NHITS
  - TFT
  - DeepAR
  ml_motif:
  - Time-series
  type: Platform
  ml_task: Forecasting
  notes: |-
    AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. First official NHITS implementation. contentReference oaicite:4 ndex=4
  contact:
    name: Kin G. Olivares (Nixtla)
    email: unkown
  cite:
  - |-
    @misc{olivares2022library_neuralforecast,
      author={Kin G. Olivares and Cristian Challú and Federico Garza and Max Mergenthaler Canseco and Artur Dubrawski},
      title = {NeuralForecast: User friendly state-of-the-art neural forecasting models.},
      year={2022},
      howpublished={PyCon Salt Lake City, Utah, US 2022},
      url={https://github.com/Nixtla/neuralforecast}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1VzhaUubIm-SHK7cfKWyoi8GtykpCuOH2qPM-k_g8bKU/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Targets high-throughput LLM inference via PagedAttention and memory-optimized serving; benchmarks cover many configs.
    dataset:
      rating: 7.0
      reason: |-
        Focuses on model configs and streaming input/output pipelines rather than classical datasets.
    metrics:
      rating: 9.0
      reason: |-
        Strong token/sec, memory usage, and TTFT metrics; comparative plots and logs included.
    reference_solution:
      rating: 9.0
      reason: |-
        Benchmarks reproducible via script with support for multiple models and hardware types.
    documentation:
      rating: 9.0
      reason: |-
        Excellent GitHub docs, CLI/API usage, and deployment walkthroughs.
- date: '2023-06-01'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: Nixtla Neural Forecast NHITS
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Official NHITS implementation for long-horizon time series forecasting
  keywords:
  - NHITS
  - long-horizon forecasting
  - neural interpolation
  - time-series
  description: |-
    NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art model that
    improved accuracy by ~25% and reduced compute by 50x compared to Transformer baselines,
    using hierarchical interpolation and multi-rate sampling :contentReference[oaicite:1]{index=1}.
  task_types:
  - Time-series forecasting
  ai_capability_measured:
  - Accuracy
  - compute efficiency for long series
  metrics:
  - RMSE
  - MAPE
  models:
  - NHITS
  ml_motif:
  - Time-series
  type: Platform
  ml_task: Forecasting
  notes: |-
    Official implementation in NeuralForecast, included since its AAAI 2023 release.
  contact:
    name: Kin G. Olivares (Nixtla)
    email: unkown
  cite:
  - |-
   @inproceedings{challu2023nhits,
    title={Nhits: Neural hierarchical interpolation for time series forecasting},
    author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
    booktitle={Proceedings of the AAAI conference on artificial intelligence},
    volume={37},
    number={6},
    pages={6989--6997},
    year={2023}
    }
  dataset:
  - name: Standard forecast datasets
    url: M4
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/15Hm5ekGu99aQWsdtiUIwX6JMoaoFpRbIhDylrWqSoHY/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 7.0
      reason: |-
        Primarily a visualization frontend; underlying benchmark definitions come from vLLM project.
    dataset:
      rating: 6.0
      reason: |-
        No traditional dataset; displays live or logged benchmark metrics.
    metrics:
      rating: 9.0
      reason: |-
        Live throughput, memory, latency, and TTFT displayed interactively; highly informative for performance analysis.
    reference_solution:
      rating: 7.0
      reason: |-
        Dashboard built on vLLM benchmarks but not itself a complete experiment package.
    documentation:
      rating: 8.0
      reason: |-
        Observable notebooks are intuitive; customization instructions are minimal but UI is self-explanatory.
- date: '2023-10-03'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: Nixtla Neural Forecast TimeLLM
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Reprogramming LLMs for time series forecasting
  keywords:
  - Time-LLM
  - language model
  - time-series
  - reprogramming
  description: |-
    Time-LLM uses reprogramming layers to adapt frozen LLMs for time series forecasting, treating
    forecasting as a language task :contentReference[oaicite:2]{index=2}.
  task_types:
  - Time-series forecasting
  ai_capability_measured:
  - Model reuse via LLM
  - few-shot forecasting
  metrics:
  - RMSE
  - MAPE
  models:
  - Time-LLM
  ml_motif:
  - Time-series
  type: Platform
  ml_task: Forecasting
  notes: |-
    Fully open-source; transforms forecasting using LLM text reconstruction.
  contact:
    name: Ming Jin (Nixtla)
    email: unkown
  cite:
  - |-
    @misc{jin2024timellmtimeseriesforecasting,
      title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models}, 
      author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},
      year={2024},
      eprint={2310.01728},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01728}, 
    }
  dataset:
  - name: Standard forecast datasets
    url: M4
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1xXGzRt-qhUFTvnBGQi2IbcoBdYyo-ZrAn3IOkswd3fw/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 7.0
      reason: |-
        Describes forecasting with LLMs, but less formal on input/output or task framing.
    dataset:
      rating: 6.0
      reason: |-
        Uses open time series datasets, but lacks a consolidated data release or splits.
    metrics:
      rating: 7.0
      reason: |-
        Reports metrics like MASE and SMAPE, standard in forecasting.
    reference_solution:
      rating: 6.0
      reason: |-
        Provides TimeLLM with open source, but no other baselines included.
    documentation:
      rating: 6.0
      reason: |-
        GitHub readme with installation and example usage; lacks API or extensive tutorials.
- date: '2023-10-05'
  last_updated: 2025-06
  expired: unkown
  valid: 'yes'
  name: Nixtla Neural Forecast TimeGPT
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Time-series foundation model "TimeGPT" for forecasting and anomaly detection
  keywords:
  - TimeGPT
  - foundation model
  - time-series
  - generative model
  description: |-
    TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for
    zero-shot forecasting and anomaly detection via API :contentReference[oaicite:3]{index=3}.
  task_types:
  - Time-series forecasting
  - Anomaly detection
  ai_capability_measured:
  - Zero-shot forecasting
  - anomaly detection
  metrics:
  - RMSE
  - Anomaly detection metrics
  models:
  - TimeGPT
  ml_motif:
  - Time-series
  type: Platform
  ml_task: Forecasting
  notes: |-
    Offered via Nixtla API and Azure Studio; enterprise-grade support available.
  contact:
    name: Azul Garza (Nixtla)
    email: unkown
  cite:
  - |-
    @misc{garza2024timegpt1,
      archiveprefix = {arXiv},
      author        = {Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},
      eprint        = {2310.03589},
      primaryclass  = {cs.LG},
      title         = {TimeGPT-1},
      url           = {https://arxiv.org/abs/2310.03589},
      year          = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: |-
      https://docs.google.com/document/d/1KmKs9JtcfpKe40fNuLrgdFcOmBnfj3ZG7AwcfTH4tXE/edit?usp=sharing
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 7.0
      reason: |-
        Describes forecasting with LLMs, but less formal on input/output or task framing.
    dataset:
      rating: 6.0
      reason: |-
        Uses open time series datasets, but lacks a consolidated data release or splits.
    metrics:
      rating: 7.0
      reason: |-
        Reports metrics like MASE and SMAPE, standard in forecasting.
    reference_solution:
      rating: 6.0
      reason: |-
        Provides TimeLLM with open source, but no other baselines included.
    documentation:
      rating: 6.0
      reason: |-
        GitHub readme with installation and example usage; lacks API or extensive tutorials.
- date: '2025-03-03'
  last_updated: 2025-03
  expired: unkown
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Gravitational Waves)
  url: https://www.codabench.org/competitions/2626/
  domain: Astrophysics; Time-series
  focus: Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets
  keywords:
  - anomaly detection
  - gravitational waves
  - astrophysics
  - time-series
  description: |-
    A benchmark for detecting anomalous transient gravitational-wave signals, including "unknown-unknowns," using preprocessed LIGO time-series at 4096 Hz. Competitors submit inference models on Codabench for continuous 50 ms segments from dual interferometers. :contentReference[oaicite:1]{index=1}
  task_types:
  - Anomaly detection
  ai_capability_measured:
  - Novel event detection in physical signals
  metrics:
  - ROC-AUC
  - Precision/Recall
  models:
  - Deep latent CNNs
  - Autoencoders
  ml_motif:
  - Time-series
  type: Dataset
  ml_task: Anomaly detection
  notes: |-
    NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench. :contentReference[oaicite:2]{index=2}
  contact:
    name: HDR A3D3 Team
    email: unkown
  cite:
  - |-
    @misc{campolongo2025buildingmachinelearningchallenges,
      archiveprefix = {arXiv},
      author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      eprint        = {2503.02112},
      primaryclass  = {cs.LG},
      title         = {Building Machine Learning Challenges for Anomaly Detection in Science},
      url           = {https://arxiv.org/abs/2503.02112},
      year          = {2025}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: https://www.codabench.org/competitions/2626/
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Novel approach treating forecasting as text generation is explained; framing is less conventional.
    dataset:
      rating: 9.0
      reason: |-
        Compatible with standard forecasting datasets (e.g., M4, electricity).
    metrics:
      rating: 8.0
      reason: |-
        RMSE and MAPE are included, but less emphasis on interpretability or time-series domain constraints.
    reference_solution:
      rating: 9.0
      reason: |-
        Open-source with reprogramming layers, LLM interface scripts provided.
    documentation:
      rating: 8.0
      reason: |-
        Model and architecture overview present, though usability guide is slightly lighter than others.
- date: '2025-03-03'
  last_updated: 2025-03
  expired: unkown
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Butterfly)
  url: https://www.codabench.org/competitions/3764/
  domain: Genomics; Image/CV
  focus: |-
    Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset
  keywords:
  - anomaly detection
  - computer vision
  - genomics
  - butterfly hybrids
  description: |-
    Image-based challenge for detecting butterfly hybrids in microscopy-driven species data. Participants evaluate models on Codabench using image segmentation/classification. :contentReference[oaicite:3]{index=3}
  task_types:
  - Anomaly detection
  ai_capability_measured:
  - Hybrid detection in biological systems
  metrics:
  - Classification accuracy
  - F1 score
  models:
  - CNN-based detectors
  ml_motif:
  - Image/CV
  type: Dataset
  ml_task: Anomaly detection
  notes: |-
    Hybrid detection benchmarks hosted on Codabench. :contentReference[oaicite:4]{index=4}
  contact:
    name: Imageomics/HDR Team
    email: unkown
  cite:
  - |-
    @misc{campolongo2025buildingmachinelearningchallenges,
      archiveprefix = {arXiv},
      author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      eprint        = {2503.02112},
      primaryclass  = {cs.LG},
      title         = {Building Machine Learning Challenges for Anomaly Detection in Science},
      url           = {https://arxiv.org/abs/2503.02112},
      year          = {2025}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: https://www.codabench.org/competitions/3764/
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Task of detecting rare anomalies in butterfly physics is well-described with physics motivation.
    dataset:
      rating: 7.0
      reason: |-
        Real detector data with injected anomalies is available, but requires NDA for full access.
    metrics:
      rating: 7.0
      reason: |-
        Uses ROC, F1, and anomaly precision, standard in challenge evaluations.
    reference_solution:
      rating: 4.0
      reason: |-
        Partial baselines described, but no codebase or reproducible runs.
    documentation:
      rating: 6.0
      reason: |-
        Challenge site includes overview and metrics, but limited in walkthrough or examples.
- date: '2025-03-03'
  last_updated: 2025-03
  expired: unkown
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Sea Level Rise)
  url: https://www.codabench.org/competitions/3223/
  domain: Climate Science; Time-series, Image/CV
  focus: |-
    Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery
  keywords:
  - anomaly detection
  - climate science
  - sea-level rise
  - time-series
  - remote sensing
  description: |-
    A challenge combining North Atlantic sea-level time-series and satellite imagery to detect flooding anomalies. Models submitted via Codabench. :contentReference[oaicite:5]{index=5}
  task_types:
  - Anomaly detection
  ai_capability_measured:
  - Detection of environmental anomalies
  metrics:
  - ROC-AUC
  - Precision/Recall
  models:
  - CNNs, RNNs, Transformers
  ml_motif:
  - Time-series, Image/CV
  type: Dataset
  ml_task: Anomaly detection
  notes: |-
    Sponsored by NSF HDR; integrates sensor and satellite data. :contentReference[oaicite:6]{index=6}
  contact:
    name: HDR A3D3 Team
    email: unkown
  cite:
  - |-
    @misc{campolongo2025buildingmachinelearningchallenges,
      archiveprefix = {arXiv},
      author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Saúl Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},
      eprint        = {2503.02112},
      primaryclass  = {cs.LG},
      title         = {Building Machine Learning Challenges for Anomaly Detection in Science},
      url           = {https://arxiv.org/abs/2503.02112},
      year          = {2025}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: https://www.codabench.org/competitions/3223/
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        TBD
    specification:
      rating: 9.0
      reason: |-
        Clear anomaly detection objective framed for physical signal discovery (LIGO/Virgo).
    dataset:
      rating: 10.0
      reason: |-
        Preprocessed waveform data from dual interferometers, public and well-structured.
    metrics:
      rating: 9.0
      reason: |-
        ROC-AUC, Precision/Recall, and confusion-based metrics are standardized.
    reference_solution:
      rating: 1.0
      reason: |-
        No starter model or baseline code linked
    documentation:
      rating: 9.0
      reason: |-
        Codabench page, GitHub starter kit, and related papers provide strong guidance.
- date: '2025-01-24'
  last_updated: 2025-02
  expired: unkown
  valid: 'yes'
  name: Single Qubit Readout on QICK System
  url: https://github.com/fastmachinelearning/ml-quantum-readout
  domain: Quantum Computing
  focus: Real-time single-qubit state classification using FPGA firmware
  keywords:
  - qubit readout
  - hls4ml
  - FPGA
  - QICK
  description: |-
    Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination. :contentReference[oaicite:0]{index=0}
  task_types:
  - Classification
  ai_capability_measured:
  - Single-shot fidelity
  - inference latency
  metrics:
  - Accuracy
  - Latency
  models:
  - hls4ml quantized NN
  ml_motif:
  - Real-time
  type: Benchmark
  ml_task: Supervised Learning
  notes: |-
    Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization. :contentReference[oaicite:1]{index=1}
  contact:
    name: Javier Campos, Giuseppe Di Guglielmo
    email: unkown
  cite:
  - |-
    @misc{diguglielmo2025endtoendworkflowmachinelearningbased,
      archiveprefix = {arXiv},
      author        = {Giuseppe Di Guglielmo and Botao Du and Javier Campos and Alexandra Boltasseva and Akash V. Dixit and Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and Ruichao Ma and Gabriel N. Perdue and Nhan Tran and Omer Yesilyurt and Daniel Bowring},
      eprint        = {2501.14663},
      primaryclass  = {quant-ph},
      title         = {End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml},
      url           = {https://arxiv.org/abs/2501.14663},
      year          = {2025}
    }
  dataset:
  - name: 'Zenodo: ml-quantum-readout dataset'
    url: zenodo.org/records/14427490
  results:
  - name: Gemini LLM Deep Research
    url: (none)
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Task clearly framed around detecting hybrid species via images, but exact labeling methods and hybrid definitions may need elaboration.
    dataset:
      rating: 8.0
      reason: |-
        Dataset hosted on Codabench; appears structured but details on image sourcing and labeling pipeline are limited.
    metrics:
      rating: 9.0
      reason: |-
        Classification accuracy and F1 are standard and appropriate.
    reference_solution:
      rating: 1.0
      reason: |-
        No starter model or baseline code linked
    documentation:
      rating: 7.5
      reason: |-
        Codabench task page describes dataset and evaluation method but lacks full API/docs.
- date: '2023-11-20'
  last_updated: 2023-11
  expired: unkown
  valid: 'yes'
  name: 'GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark'
  url: https://arxiv.org/abs/2311.12022
  domain: Science (Biology, Physics, Chemistry)
  focus: |-
    Graduate-level, expert-validated multiple-choice questions hard even with web access
  keywords:
  - Google-proof
  - multiple-choice
  - expert reasoning
  - science QA
  description: |-
    Contains 448 challenging questions written by domain experts, with expert accuracy at 65% (74% discounting clear errors) and non-experts reaching just 34%. GPT-4 baseline scores ~39%-designed for scalable oversight evaluation. :contentReference[oaicite:2]{index=2}
  task_types:
  - Multiple choice
  ai_capability_measured:
  - Scientific reasoning
  - knowledge probing
  metrics:
  - Accuracy
  models:
  - GPT-4 baseline
  ml_motif:
  - Multiple choice
  type: Benchmark
  ml_task: Multiple choice
  notes: |-
    Google-proof, supports oversight research.
  contact:
    name: David Rein (NYU)
    email: unkown
  cite:
  - |-
    @misc{rein2023gpqagraduatelevelgoogleproofqa,
      archiveprefix = {arXiv},
      author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      eprint        = {2311.12022},
      primaryclass  = {cs.AI},
      title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
      url           = {https://arxiv.org/abs/2311.12022},
      year          = {2023}
    }
  dataset:
  - name: GPQA dataset
    url: zip/HuggingFace
  results:
  - name: Gemini LLM Deep Research
    url: (none)
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Clear dual-modality task (image + time-series); environmental focus is well described.
    dataset:
      rating: 9.0
      reason: |-
        Time-series and satellite imagery data provided; sensor info and collection intervals are explained.
    metrics:
      rating: 9.0
      reason: |-
        ROC-AUC, Precision/Recall are appropriate and robust.
    reference_solution:
      rating: 1.0
      reason: |-
        No starter model or baseline code linked
    documentation:
      rating: 6.5
      reason: |-
        Moderate Codabench documentation with climate context; lacks pipeline-level walkthrough.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: SeafloorAI
  url: "https://neurips.cc/virtual/2024/poster/97432"
  domain: Marine Science; Vision-Language
  focus: |-
    Large-scale vision-language dataset for seafloor mapping and geological classification
  keywords:
  - sonar imagery
  - vision-language
  - seafloor mapping
  - segmentation
  - QA
  description: |-
    A first-of-its-kind dataset covering 17,300 sq.km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus ~7M QA pairs-designed for both vision and language-based ML models in marine science :contentReference[oaicite:1]{index=1}.
  task_types:
  - Image segmentation
  - Vision-language QA
  ai_capability_measured:
  - Geospatial understanding
  - multimodal reasoning
  metrics:
  - Segmentation pixel accuracy
  - QA accuracy
  models:
  - SegFormer
  - ViLT-style multimodal models
  ml_motif:
  - Vision-Language
  type: Dataset
  ml_task: Segmentation, QA
  notes: |-
    Data processing code publicly available, covering five geological layers; curated with marine scientists :contentReference[oaicite:2]{index=2}.
  contact:
    name: Kien X. Nguyen
    email: unkown
  cite:
  - |-
    @misc{nguyen2024seafloor,
      archiveprefix = {arXiv},
      author = {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},
      eprint = {2411.00172},
      primaryclass = {cs.CV},
      title = {SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},
      url = {https://arxiv.org/abs/2411.00172},
      year=2024
    }
  dataset:
  - name: Sonar imagery + annotations
    url: ~15 TB
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Real-time qubit classification task clearly defined in quantum instrumentation context.
    dataset:
      rating: 9.0
      reason: |-
        Dataset available on Zenodo with signal traces; compact and reproducible.
    metrics:
      rating: 9.0
      reason: |-
        Accuracy and latency are well defined and crucial in this setting.
    reference_solution:
      rating: 9.0
      reason: |-
        GitHub repo has reproducible code and HLS firmware targeting FPGA.
    documentation:
      rating: 8.0
      reason: |-
        Good setup instructions, but no interactive visualization or starter notebook.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: SuperCon3D
  url: https://neurips.cc/virtual/2024/poster/97553
  domain: Materials Science; Superconductivity
  focus: |-
    Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures
  keywords:
  - superconductivity
  - crystal structures
  - equivariant GNN
  - generative models
  description: |-
    SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates :contentReference[oaicite:3]{index=3}.
  task_types:
  - Regression (Tc prediction)
  - Generative modeling
  ai_capability_measured:
  - Structure-to-property prediction
  - structure generation
  metrics:
  - MAE (Tc)
  - Validity of generated structures
  models:
  - SODNet
  - DiffCSP-SC
  ml_motif:
  - Materials Modeling
  type: Dataset + Models
  ml_task: Regression, Generation
  notes: |-
    Demonstrates advantage of combining ordered and disordered structural data in model design :contentReference[oaicite:4]{index=4}.
  contact:
    name: Zhong Zuo
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_c4e3b55e,
      author = {Chen, Pin and Peng, Luoxuan and Jiao, Rui and Mo, Qing and Wang, Zhen and Huang, Wenbing and Liu, Yang and Lu, Yutong},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {108902--108928},
      publisher = {Curran Associates, Inc.},
      title = {Learning Superconductivity from Ordered and Disordered Material Structures},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 10.0
      reason: |-
        Multimodal task (segmentation + natural language QA pairs);.
    dataset:
      rating: 10.0
      reason: |-
        sonar imagery + masks + descriptions, georeferenced and labeled with QA
    metrics:
      rating: 9.0
      reason: |-
        Pixel accuracy and QA metrics clearly defined; tasks split by modality.
    reference_solution:
      rating: 8.0
      reason: |-
        Baseline models (SegFormer, ViLT) are cited, partial configs likely available.
    documentation:
      rating: 8.5
      reason: |-
        Paper + GitHub metadata and processing details are comprehensive, though full dataset is not yet available.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: GeSS
  url: https://neurips.cc/virtual/2024/poster/97816
  domain: Scientific ML; Geometric Deep Learning
  focus: |-
    Benchmark suite evaluating geometric deep learning models under real-world distribution shifts
  keywords:
  - geometric deep learning
  - distribution shift
  - OOD robustness
  - scientific applications
  description: |-
    GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access :contentReference[oaicite:5]{index=5}.
  task_types:
  - Classification
  - Regression
  ai_capability_measured:
  - OOD performance in scientific settings
  metrics:
  - Accuracy
  - RMSE
  - OOD robustness delta
  models:
  - GCN
  - EGNN
  - DimeNet++
  ml_motif:
  - Geometric DL
  type: Benchmark
  ml_task: Classification, Regression
  notes: |-
    Includes no-OOD, unlabeled-OOD, and few-label scenarios :contentReference[oaicite:6]{index=6}.
  contact:
    name: Deyu Zou
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_a8063075,
      author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {92499--92528},
      publisher = {Curran Associates, Inc.},
      title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Well-defined problem (Tc prediction, generation) with strong scientific motivation (high-Tc materials), but no formal hardware constraints.
    dataset:
      rating: 9.0
      reason: |-
        Includes curated 3D crystal structures and Tc data; readily downloadable and used in paper models.
    metrics:
      rating: 9.0
      reason: |-
        MAE and structural validity used, well-established in materials modeling.
    reference_solution:
      rating: 8.0
      reason: |-
        Provides two reference models (SODNet, DiffCSP-SC) with results. Code likely available post-conference.
    documentation:
      rating: 8.0
      reason: |-
        Paper and poster explain design choices well; software availability confirms reproducibility but limited external documentation.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: Vocal Call Locator (VCL)
  url: https://neurips.cc/virtual/2024/poster/97470
  domain: Neuroscience; Bioacoustics
  focus: |-
    Benchmarking sound-source localization of rodent vocalizations from multi-channel audio
  keywords:
  - source localization
  - bioacoustics
  - time-series
  - SSL
  description: |-
    The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms in bioacoustics :contentReference[oaicite:1]{index=1}.
  task_types:
  - Sound source localization
  ai_capability_measured:
  - Source localization accuracy in bioacoustic settings
  metrics:
  - Localization error (cm)
  - Recall/Precision
  models:
  - CNN-based SSL models
  ml_motif:
  - Real-time
  type: Dataset
  ml_task: Anomaly detection / localization
  notes: |-
    Dataset spans real, simulated, and mixed audio; supports benchmarking across data types :contentReference[oaicite:2]{index=2}.
  contact:
    name: Ralph Peterson
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_c00d37d6,
      author = {Peterson, Ralph E and Tanelus, Aramis and Ick, Christopher and Mimica, Bartul and Francis, Niegil and Ivan, Violet J and Choudhri, Aman and Falkner, Annegret L and Murthy, Mala and Schneider, David M and Sanes, Dan H and Williams, Alex H},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {106370--106382},
      publisher = {Curran Associates, Inc.},
      title = {Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Clear benchmark scenarios across GDL tasks under multiple real-world shift settings; OOD settings precisely categorized.
    dataset:
      rating: 8.0
      reason: |-
        Scientific graph datasets provided in multiple shift regimes; standardized splits across domains. Exact format of data not specified.
    metrics:
      rating: 9.0
      reason: |-
        Includes base metrics (accuracy, RMSE) plus OOD delta robustness for evaluation under shifts.
    reference_solution:
      rating: 9.0
      reason: |-
        Multiple baselines (11 algorithms x 3 backbones) evaluated; setup supports reproducible comparison.
    documentation:
      rating: 2.0
      reason: |-
        Paper, poster, and source code provide thorough access to methodology and implementation. Setup instructions and accompanying code not present.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: MassSpecGym
  url: https://neurips.cc/virtual/2024/poster/97823
  domain: Cheminformatics; Molecular Discovery
  focus: Benchmark suite for discovery and identification of molecules via MS/MS
  keywords:
  - mass spectrometry
  - molecular structure
  - de novo generation
  - retrieval
  - dataset
  description: |-
    MassSpecGym curates the largest public MS/MS dataset with three standardized tasks-de novo structure generation, molecule retrieval, and spectrum simulation-using challenging generalization splits to propel ML-driven molecule discovery :contentReference[oaicite:3]{index=3}.
  task_types:
  - De novo generation
  - Retrieval
  - Simulation
  ai_capability_measured:
  - Molecular identification and generation from spectral data
  metrics:
  - Structure accuracy
  - Retrieval precision
  - Simulation MSE
  models:
  - Graph-based generative models
  - Retrieval baselines
  ml_motif:
  - Benchmark
  type: Dataset + Benchmark
  ml_task: Generation, retrieval, simulation
  notes: |-
    Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks :contentReference[oaicite:4]{index=4}.
  contact:
    name: Roman Bushuiev
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_c6c31413,
      author = {Bushuiev, Roman and Bushuiev, Anton and de Jonge, Niek F. and Young, Adamo and Kretschmer, Fleming and Samusevich, Raman and Heirman, Janne and Wang, Fei and Zhang, Luke and D\"{u}hrkop, Kai and Ludwig, Marcus and Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and Schmid, Robin and Greiner, Russell and Wang, Bo and Wishart, David S. and Liu, Li-Ping and Rousu, Juho and Bittremieux, Wout and Rost, Hannes and Mak, Tytus D. and Hassoun, Soha and Huber, Florian and van der Hooft, Justin J.J. and Stravs, Michael A. and B\"{o}cker, Sebastian and Sivic, Josef and Pluskal, Tom\'{a}\v{s}},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {110010--110027},
      publisher = {Curran Associates, Inc.},
      title = {MassSpecGym: A benchmark for the discovery and identification of molecules},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Focused on sound source localization for rodent vocalizations in lab settings; well-scoped.
    dataset:
      rating: 9.5
      reason: |-
        767000 annotated audio segments across diverse conditions. Minor deduction for no train/test/valid split.
    metrics:
      rating: 9.5
      reason: |-
        Localization error, precision/recall used
    reference_solution:
      rating: 7.0
      reason: |-
        CNN-based baselines referenced but unclear whether pretrained models or training code are available.
    documentation:
      rating: 2.0
      reason: |-
        Poster and paper outline benchmark intent and setup; repo expected but not confirmed in dataset card.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: Urban Data Layer (UDL)
  url: https://neurips.cc/virtual/2024/poster/97837
  domain: Urban Computing; Data Engineering
  focus: Unified data pipeline for multi-modal urban science research
  keywords:
  - data pipeline
  - urban science
  - multi-modal
  - benchmark
  description: |-
    UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks :contentReference[oaicite:5]{index=5}.
  task_types:
  - Prediction
  - Classification
  ai_capability_measured:
  - Multi-modal urban inference
  - standardization
  metrics:
  - Task-specific accuracy or RMSE
  models:
  - Baseline regression/classification pipelines
  ml_motif:
  - Data engineering
  type: Framework
  ml_task: Prediction, classification
  notes: |-
    Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models :contentReference[oaicite:6]{index=6}.
  contact:
    name: Yiheng Wang
    email: unkown
  cite:
  - |-
    @inproceedings{neurips2024_0db7f135,
      author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {7296--7310},
      publisher = {Curran Associates, Inc.},
      title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        Three tasks (de novo generation, retrieval, simulation) are clearly defined for MS/MS molecule discovery.
    dataset:
      rating: 10.0
      reason: |-
        Over 1 million spectra with structure annotations; dataset is open-source and well-documented.
    metrics:
      rating: 9.0
      reason: |-
        Task-appropriate metrics (structure accuracy, precision, MSE) are specified and used consistently.
    reference_solution:
      rating: 8.0
      reason: |-
        Baseline models are available (graph-based and retrieval), though not exhaustive.
    documentation:
      rating: 9.0
      reason: |-
        GitHub repo and poster provide code and reproducibility guidance.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: Delta Squared-DFT
  url: https://neurips.cc/virtual/2024/poster/97788
  domain: Computational Chemistry; Materials Science
  focus: |-
    Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies
  keywords:
  - density functional theory
  - Delta Squared-ML correction
  - reaction energetics
  - quantum chemistry
  description: |-
    Introduces the Delta Squared-ML paradigm-using ML corrections to DFT to predict reaction energies with accuracy comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and organometallic transformations.
  task_types:
  - Regression
  ai_capability_measured:
  - High-accuracy energy prediction
  - DFT correction
  metrics:
  - Mean Absolute Error (eV)
  - Energy ranking accuracy
  models:
  - Delta Squared-ML correction networks
  - Kernel ridge regression
  ml_motif:
  - Scientific ML
  type: Dataset + Benchmark
  ml_task: Regression
  notes: |-
    Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility.
  contact:
    name: Wei Liu
    email: unkown
  cite:
  - |-
    @misc{khrabrov2024nabla2dftuniversalquantumchemistry,
      title={$\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials}, 
      author={Kuzma Khrabrov and Anton Ber and Artem Tsypin and Konstantin Ushenin and Egor Rumiantsev and Alexander Telepov and Dmitry Protasov and Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and Sergey Nikolenko and Elena Tutubalina and Artur Kadurin},
      year={2024},
      eprint={2406.14347},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      url={https://arxiv.org/abs/2406.14347}, 
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 8.0
      reason: |-
        Clear goals around unifying urban data formats and tasks (e.g., air quality prediction), though some specifics could be more formal.
    dataset:
      rating: 9.0
      reason: |-
        Multi-modal data is standardized and accessible; GitHub repo available.
    metrics:
      rating: 8.0
      reason: |-
        Uses common task metrics like accuracy/RMSE, though varies by task.
    reference_solution:
      rating: 7.0
      reason: |-
        Baseline regression/classification models included.
    documentation:
      rating: 8.0
      reason: |-
        Source code supports pipeline reuse, but formal evaluation splits may vary.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: LLMs for Crop Science
  url: https://neurips.cc/virtual/2024/poster/97570
  domain: Agricultural Science; NLP
  focus: |-
    Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts
  keywords:
  - crop science
  - prompt engineering
  - domain adaptation
  - question answering
  description: |-
    Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs covering crop traits, growth stages, and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and retrieval-augmented prompts.
  task_types:
  - Question Answering
  - Inference
  ai_capability_measured:
  - Scientific knowledge
  - crop reasoning
  metrics:
  - Accuracy
  - F1 score
  models:
  - GPT-4
  - LLaMA-2-13B
  - T5-XXL
  ml_motif:
  - NLP
  type: Dataset
  ml_task: QA, inference
  notes: |-
    Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation.
  contact:
    name: Deepak Patel
    email: unkown
  cite:
  - |-
    @misc{shen2024exploringuserretrievalintegration,
      title={Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation}, 
      author={Tingjia Shen and Hao Wang and Jiaqing Zhang and Sirui Zhao and Liangyue Li and Zulong Chen and Defu Lian and Enhong Chen},
      year={2024},
      eprint={2406.03085},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03085}, 
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 9.0
      reason: |-
        The task of ML correction to DFT energy predictions is well-specified.
    dataset:
      rating: 9.0
      reason: |-
        10 public reaction datasets with DFT and CC references; well-documented.
    metrics:
      rating: 8.0
      reason: |-
        Uses MAE and ranking accuracy, suitable for this task.
    reference_solution:
      rating: 8.0
      reason: |-
        Includes both Delta^2 and KRR baselines.
    documentation:
      rating: 9.0
      reason: |-
        Public benchmarks and clear reproducibility via datasets and model code.
- date: '2024-12-13'
  last_updated: 2024-12
  expired: unkown
  valid: 'yes'
  name: SPIQA (LLM)
  url: https://neurips.cc/virtual/2024/poster/97575
  domain: Multimodal Scientific QA; Computer Vision
  focus: |-
    Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance)
  keywords:
  - multimodal QA
  - scientific figures
  - image+text
  - chain-of-thought prompting
  description: |-
    A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions. Highlights performance differences between chain-of-thought and end-to-end adapter models.
  task_types:
  - Multimodal QA
  ai_capability_measured:
  - Visual reasoning
  - scientific figure understanding
  metrics:
  - Accuracy
  - F1 score
  models:
  - LLaVA
  - MiniGPT-4
  - Owl-LLM adapter variants
  ml_motif:
  - Multimodal QA
  type: Benchmark
  ml_task: Multimodal QA
  notes: |-
    Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs.
  contact:
    name: Xiaoyan Zhong
    email: unkown
  cite:
  - |-
    @misc{pramanick2025spiqadatasetmultimodalquestion,
      title={SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers}, 
      author={Shraman Pramanick and Rama Chellappa and Subhashini Venugopalan},
      year={2025},
      eprint={2407.09413},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09413}, 
    }
  dataset: []
  results:
  - name: Gemini LLM Deep Research
    url: unkown
  - name: ChatGPT LLM
    url: unkown
  fair:
    reproducible: 'Yes'
    benchmark_ready: 'Yes'
  ratings:
    software:
      rating: 0
      reason: |-
        Not analyzed.
    specification:
      rating: 6.0
      reason: |-
        Task of QA over scientific figures is interesting but not fully formalized in input/output terms.
    dataset:
      rating: 6.0
      reason: |-
        Uses SPIQA dataset with ~10 adapters; figures and questions are included, but not fully open.
    metrics:
      rating: 7.0
      reason: |-
        Reports accuracy and F1; fair but no visual reasoning-specific metric.
    reference_solution:
      rating: 6.0
      reason: |-
        10 LLM adapter baselines; results included.
    documentation:
      rating: 5.0
      reason: |-
        Poster paper and limited documentation; no reproducibility instructions.
