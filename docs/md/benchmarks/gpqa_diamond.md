# GPQA Diamond


**Date**: 2023-11-20


**Name**: GPQA Diamond


**Domain**: Science


**Focus**: Graduate-level scientific reasoning


**Keywords**: Google-proof, graduate-level, science QA, chemistry, physics


**Task Types**: Multiple choice, Multi-step QA


**Metrics**: Accuracy


**Models**: o1, DeepSeek-R1


**Citation**:


- David Rein, Betty Li Hou, and Asa Cooper Stickland. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022.

  - bibtex: |

      @misc{rein2023gpqagraduatelevelgoogleproofqa,

        title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},

        author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},

        year={2023},

        url={https://arxiv.org/abs/2311.12022}

      }



**Ratings:**


Specification:


  - **Rating:** 6.5


  - **Reason:** Good description of how the problems are received, but little specification on how the models are tested 


Dataset:


  - **Rating:** 8.5


  - **Reason:** Easily able to access dataset. No labels or train/test/valid split 


Metrics:


  - **Rating:** 10


  - **Reason:** Each question has a correct answer 


Reference Solution:


  - **Rating:** 7.5


  - **Reason:** Common models such as GPT-3.5 were compared. Reproducibility of results unknown 


Documentation:


  - **Rating:** 1


  - **Reason:** No reference solution, platform for reproduction, or procedure for replication 


**Radar Plot:**
 ![Gpqa Diamond radar plot](../../tex/images/gpqa_diamond_radar.png)