- - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Jet Classification
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time classification of particle jets using HL-LHC simulation features
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - classification
    - real-time ML
    - jet tagging
    - QKeras
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time inference, model compression performance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - AUC
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Keras DNN
    - QKeras quantized DNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes both float and quantized models using QKeras
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{hawks2022fastml,\n  title={Fast Machine Learning for Science: Benchmarks and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n  url={https://arxiv.org/abs/2207.07958}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'OpenML: hls4ml_lhc_jets_hlf (https://www.openml.org/d/42468), JetClass (https://zenodo.org/record/6619768)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.
    - dataset_rating: 9.0
    - dataset_reason: Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.
    - metrics_rating: 9.0
    - metrics_reason: Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not bundled as an official starter kit.
    - documentation_rating: 7.0
    - documentation_reason: Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually connecting models to dataset.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Irregular Sensor Data Compression
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time compression of sparse sensor data with autoencoders
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - compression
    - autoencoder
    - sparse data
    - irregular sampling
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Compression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Reconstruction quality, compression efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MSE
    - Compression ratio
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - Quantized autoencoder
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Unsupervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Based on synthetic but realistic physics sensor data
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{hawks2022fastml2,\n  title={Fast Machine Learning for Science: Benchmarks and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n  url={https://arxiv.org/abs/2207.07958}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Custom synthetic irregular sensor dataset (see GitHub repo)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Classification is clearly defined for real-time inference on simulated LHC jets. Input features (HLFs) are documented, though exact latency or resource constraints are not numerically specified.
    - dataset_rating: 9.0
    - dataset_reason: Two datasets (OpenML and Zenodo) are public, well-formatted, and documented; FAIR principles are followed, though richer metadata would raise confidence to a 10.
    - metrics_rating: 9.0
    - metrics_reason: AUC and Accuracy are standard, quantitative, and well-aligned with goals of jet tagging and inference efficiency.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Float and quantized Keras/QKeras models are provided with results. Reproducibility is good, though full automation and documentation could be improved.
    - documentation_rating: 8.0
    - documentation_reason: GitHub contains baseline code, data loaders, and references, but setup for deployment (e.g., FPGA pipeline) requires familiarity with the tooling.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-05-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Beam Control
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Accelerators and Magnets
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Reinforcement learning control of accelerator beam position
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - RL
    - beam stabilization
    - control systems
    - simulation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Control
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Policy performance in simulated accelerator control
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Stability
    - Control loss
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - DDPG
    - PPO (planned)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, RL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Reinforcement Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Environment defined, baseline RL implementation is in progress
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{hawks2022fastml3,\n  title={Fast Machine Learning for Science: Benchmarks and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n  url={https://arxiv.org/abs/2207.07958}\n}"
    - "@article{wang2021booster,\n  title={BOOSTR: A Dataset for Accelerator Control Systems},\n  author={Wang, Qizhi and others},\n  year={2021},\n  url={https://arxiv.org/abs/2101.08359}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'BOOSTR: https://arxiv.org/pdf/2101.08359'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Task is well defined (real-time compression of sparse, irregular sensor data using autoencoders); latency constraints are implied but not fully quantified.
    - dataset_rating: 8.0
    - dataset_reason: Dataset is custom and synthetic but described well; FAIR-compliance is partial (reusable and accessible, but not externally versioned with rich metadata).
    - metrics_rating: 9.0
    - metrics_reason: Uses standard quantitative metrics (MSE, compression ratio) clearly aligned with compression and reconstruction goals.
    - reference_solution_rating: 7.0
    - reference_solution_reason: Baseline (autoencoder and quantized variant) is provided, but training/inference pipeline is minimally documented and needs user setup.
    - documentation_rating: 8.0
    - documentation_reason: GitHub repo contains core components, but more structured setup instructions and pretrained weights would improve usability.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-07-08'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Ultrafast jet classification at the HL-LHC
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2402.01876
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: FPGA-optimized real-time jet origin classification at the HL-LHC
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - jet classification
    - FPGA
    - quantization-aware training
    - Deep Sets
    - Interaction Networks
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time inference under FPGA constraints
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    - Resource utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - MLP
    - Deep Sets
    - Interaction Network
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '3'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Uses quantization-aware training; hardware synthesis evaluated via hls4ml
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - TODO
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Zenodo DOI:10.5281/zenodo.3602260 (constituent-level jets)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task is clear (RL control of beam stability), with BOOSTR-based simulator; control objectives are well motivated, but system constraints and reward structure are still under refinement.
    - dataset_rating: 7.0
    - dataset_reason: BOOSTR dataset exists and is cited, but integration into the benchmark is in early stages; metadata and FAIR structure are limited.
    - metrics_rating: 7.0
    - metrics_reason: Stability and control loss are mentioned, but metrics are not yet formalized with clear definitions or baselines.
    - reference_solution_rating: 5.5
    - reference_solution_reason: DDPG baseline mentioned; PPO planned; implementation is still in progress with no reproducible results available yet.
    - documentation_rating: 6.0
    - documentation_reason: GitHub has a defined structure but is incomplete; setup and execution instructions for training/evaluation are not fully established.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-10-15'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Quench detection
    description: The name of the benchmark.
    condition: required
  - url: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Accelerators and Magnets
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time detection of superconducting magnet quenches using ML
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - quench detection
    - autoencoder
    - anomaly detection
    - real-time
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    - Quench localization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time anomaly detection with multi-modal sensors
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC‑AUC
    - Detection latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - RL agents (in development)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, RL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Reinforcement + Unsupervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 1 (autoencoder)
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Precursor detection in progress; multi-modal and dynamic weighting methods
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - TODO
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: BPM and power supply data from BNL (HDF5 preprocessed, ~67k BPM + 32k PS windows)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: Real-time jet origin classification under FPGA constraints is clearly defined, with explicit latency targets (~100 ns) and I/O formats.
    - dataset_rating: 9.0
    - dataset_reason: Data available on Zenodo with DOI, includes constituent-level jets; accessible and well-documented, though not deeply versioned with full FAIR metadata.
    - metrics_rating: 10.0
    - metrics_reason: Accuracy, latency, and hardware resource usage (LUTs, DSPs) are rigorously measured and aligned with real-time goals.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Includes models (MLP, Deep Sets, Interaction Networks) with quantization-aware training and synthesis results via hls4ml; reproducible but tightly coupled with specific toolchains.
    - documentation_rating: 8.0
    - documentation_reason: Paper and code (via hls4ml) are sufficient, but a centralized, standalone repo for reproducing all models would enhance accessibility.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-10-15'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: DUNE
    description: The name of the benchmark.
    condition: required
  - url: https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time ML for DUNE DAQ time-series data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - DUNE
    - time-series
    - real-time
    - trigger
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Trigger selection
    - Time-series anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Low-latency event detection
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Detection efficiency
    - Latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - LSTM (planned)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark (in progress)
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Prototype models demonstrated on SONIC platform
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - TODO
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: DUNE SONIC data (via internal FNAL systems)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task (quench detection via anomaly detection) is clearly described; multi-modal sensors, streaming rates, and objective are provided, but constraints (latency thresholds) are qualitative.
    - dataset_rating: 7.0
    - dataset_reason: Custom dataset using real data from BNL; HDF5 formatted and structured, but access may be internal or limited, and not versioned for public FAIR use.
    - metrics_rating: 8.0
    - metrics_reason: ROC-AUC and detection latency are defined; relevant and quantitative but not yet paired with benchmark baselines.
    - reference_solution_rating: 6.0
    - reference_solution_reason: Autoencoder prototype exists; RL methods are in development; no fully reproducible pipeline is available yet.
    - documentation_rating: 7.0
    - documentation_reason: Slides and GDocs outline results; implementation is in progress with limited setup/code release.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-01-08'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Intelligent experiments through real-time AI
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2501.04845
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Instrumentation and Detectors; Nuclear Physics; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time FPGA-based triggering and detector control for sPHENIX and future EIC
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - FPGA
    - Graph Neural Network
    - hls4ml
    - real-time inference
    - detector control
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Trigger classification
    - Detector control
    - Real-time inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Low-latency GNN inference on FPGA
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy (charm and beauty detection)
    - Latency (µs)
    - Resource utilization (LUT/FF/BRAM/DSP)
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Bipartite Graph Network with Set Transformers (BGN-ST)
    - GarNet (edge-classifier)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Achieved ~97.4% accuracy for beauty decay triggers; sub-10 µs latency on Alveo U280; hit-based FPGA design via hls4ml and FlowGNN.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{kvapil2025intelligent,\n  title={Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors},\n  author={Kvapil, Jakub and Borca-Tasciuc, Giorgian and ... Tran, Nhan and others},\n  year={2025},\n  url={https://arxiv.org/abs/2501.04845}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Internal simulated tracking data (sPHENIX and EIC DIS-electron tagger)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task (trigger-level anomaly detection) is clearly defined for low-latency streaming input, but the problem framing lacks complete architectural/system specs.
    - dataset_rating: 6.0
    - dataset_reason: Internal DUNE SONIC data; not publicly released and no formal FAIR support; replicability is institutionally gated.
    - metrics_rating: 7.0
    - metrics_reason: Metrics include detection efficiency and latency, which are relevant, but only lightly supported by baselines or formal eval scripts.
    - reference_solution_rating: 5.0
    - reference_solution_reason: One CNN prototype demonstrated; LSTM planned. No public implementation or ready-to-run example yet.
    - documentation_rating: 6.0
    - documentation_reason: Slides and some internal documentation exist, but no full pipeline or public GitHub repo yet.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-01-09'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Neural Architecture Codesign for Fast Physics Applications
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2501.05515
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Physics; Materials Science; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Automated neural architecture search and hardware-efficient model codesign for fast physics applications
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - neural architecture search
    - FPGA deployment
    - quantization
    - pruning
    - hls4ml
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    - Peak finding
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Hardware-aware model optimization; low-latency inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    - Resource utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NAC-based BraggNN
    - NAC-optimized Deep Sets (jet)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 2 (BraggNN, Jet DS)
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{weitz2025nacph,\n  title={Neural Architecture Codesign for Fast Physics Applications},\n  author={Weitz, Jason and Demler, Dmitri and McDermott, Luke and Tran, Nhan and Duarte, Javier},\n  year={2025},\n  url={https://arxiv.org/abs/2501.05515}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Internal Bragg microscopy and HEP jet datasets
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: Task is clearly defined (triggering on rare events with sub-10 µs latency); architecture, constraints, and system context (FPGA, Alveo) are well detailed.
    - dataset_rating: 7.0
    - dataset_reason: Simulated tracking data from sPHENIX and EIC; internally structured but not yet released in a public FAIR-compliant format.
    - metrics_rating: 10.0
    - metrics_reason: Accuracy, latency, and hardware resource utilization (LUTs, DSPs) are clearly defined and used in evaluation.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Graph-based models (BGN-ST, GarNet) are implemented and tested on real hardware; reproducibility possible with hls4ml but full scripts not bundled.
    - documentation_rating: 8.0
    - documentation_reason: Paper is detailed and tool usage (FlowGNN, hls4ml) is described, but repo release and dataset access remain in progress.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-06-24'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Smart Pixels for LHC
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2406.14860
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics; Instrumentation and Detectors
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - smart pixel
    - on-sensor inference
    - data reduction
    - trigger
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    - Data filtering
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: On-chip, low-power inference; data reduction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Data rejection rate
    - Power per pixel
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - 2-layer pixel NN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{parpillon2024smartpixels,\n  title={Smart Pixels: In-pixel AI for on-sensor data filtering},\n  author={Parpillon, Benjamin and ... and Tran, Nhan},\n  year={2024},\n  url={https://arxiv.org/abs/2406.14860}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: In-pixel charge cluster data
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Task (automated neural architecture search for real-time physics) is well formulated with clear latency, model compression, and deployment goals.
    - dataset_rating: 6.0
    - dataset_reason: Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant, though mentioned in the paper.
    - metrics_rating: 10.0
    - metrics_reason: BOP reduction, latency, and accuracy are all quantitatively evaluated.
    - reference_solution_rating: 8.0
    - reference_solution_reason: NAC-generated models for Bragg peak and jet classification are described, but pipeline requires integration of several tools and is not fully packaged.
    - documentation_rating: 7.0
    - documentation_reason: NAC pipeline, hls4ml usage, and results are discussed; code (e.g., nac-opt) referenced, but replication requires stitching together toolchain and data.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-10-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HEDM (BraggNN)
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2008.08198
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Material Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast Bragg peak analysis using deep learning in diffraction microscopy
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - BraggNN
    - diffraction
    - peak finding
    - HEDM
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Peak detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: High-throughput peak localization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Localization accuracy
    - Inference time
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - BraggNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Peak finding
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Enables real-time HEDM workflows; basis for NAC case study.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{xiao2020braggnn,\n  title={BraggNN: Fast X-ray Bragg peak analysis using deep learning},\n  author={Xiao, Yu and ...},\n  year={2020},\n  url={https://arxiv.org/abs/2008.08198}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Simulated HEDM diffraction images
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: 'Fully specified: describes task (data filtering/classification), system design (on-sensor inference), latency (25 ns), and power constraints.'
    - dataset_rating: 8.0
    - dataset_reason: In-pixel charge cluster data used, but dataset release info is minimal; FAIR metadata/versioning limited.
    - metrics_rating: 9.0
    - metrics_reason: Data rejection rate and power per pixel are clearly defined and directly tied to hardware goals.
    - reference_solution_rating: 9.0
    - reference_solution_reason: 2-layer NN implementation is evaluated in hardware; reproducible via hls4ml flow with results in paper.
    - documentation_rating: 8.0
    - documentation_reason: Paper is clear; Zenodo asset is referenced, but additional GitHub or setup repo would improve reproducibility.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-12-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: 4D‑STEM
    description: The name of the benchmark.
    condition: required
  - url: https://openreview.net/pdf?id=7yt3N0o0W9
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Material Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time ML for scanning transmission electron microscopy
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - 4D-STEM
    - electron microscopy
    - real-time
    - image processing
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    - Streamed data inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time large-scale microscopy inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Classification accuracy
    - Throughput
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN models (prototype)
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '0'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: In-progress; model design under development.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{anonymous2023_4dstem,\n  title={4D-STEM: Real-Time ML for Electron Microscopy},\n  author={Anonymous},\n  year={2023},\n  url={https://openreview.net/pdf?id=7yt3N0o0W9}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: —
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Peak localization task is well-defined for diffraction images; input/output described clearly, but no system constraints.
    - dataset_rating: 8.0
    - dataset_reason: Simulated diffraction images provided; reusable and downloadable, but not externally versioned or FAIR-structured.
    - metrics_rating: 9.0
    - metrics_reason: Inference speed and localization accuracy are standard and quantitatively reported.
    - reference_solution_rating: 8.0
    - reference_solution_reason: BraggNN model and training pipeline exist, but need stitching from separate repositories.
    - documentation_rating: 8.0
    - documentation_reason: Paper and codebase are available and usable, though not fully turnkey.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-12-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: In-Situ High-Speed Computer Vision
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2312.00128
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Fusion/Plasma
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time image classification for in-situ plasma diagnostics
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - plasma
    - in-situ vision
    - real-time ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Real-time diagnostic inference
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - FPS
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Model
    description: The type of this benchmark.
    condition: required
  - ml_task: Image Classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '1'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Embedded/deployment details in progress.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{smith2023insitu,\n  title={In-Situ High-Speed Computer Vision for Plasma Diagnostics},\n  author={Smith, John and Doe, Jane},\n  year={2023},\n  url={https://arxiv.org/abs/2312.00128}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: In-situ sensor imagery streams
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: General task defined (real-time microscopy inference), but no standardized I/O format, latency constraint, or complete problem framing yet.
    - dataset_rating: 0.0
    - dataset_reason: Dataset not provided or described in any formal way.
    - metrics_rating: 6.0
    - metrics_reason: Mentions throughput and accuracy, but metrics are not formally defined or benchmarked.
    - reference_solution_rating: 2.0
    - reference_solution_reason: Prototype CNNs described; no baseline or implementation released.
    - documentation_rating: 5.0
    - documentation_reason: OpenReview paper and Gemini doc give some insight, but no working code, environment, or example.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2020-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2020-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: BenchCouncil AIBench
    description: The name of the benchmark.
    condition: required
  - url: https://www.benchcouncil.org/AIBench/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: End-to-end AI benchmarking across micro, component, and application levels
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - benchmarking
    - AI systems
    - application-level evaluation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Training
    - Inference
    - End-to-end AI workloads
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: System-level AI workload performance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Throughput
    - Latency
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - ResNet
    - BERT
    - GANs
    - Recommendation systems
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: General
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '4'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Covers scenario-distilling, micro, component, and end-to-end benchmarks.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{gao2020aibench,\n  title={AIBench: An Industry Standard Internet Service AI Benchmark Suite},\n  author={Gao, Wanling and Zhan, Jianfeng and others},\n  year={2020},\n  url={https://arxiv.org/abs/1908.08998}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task (plasma diagnostic classification) and real-time deployment described; system specs (FPS targets) implied but not fully quantified.
    - dataset_rating: 6.0
    - dataset_reason: Dataset is sensor stream-based but not shared or FAIR-documented.
    - metrics_rating: 8.0
    - metrics_reason: FPS and classification accuracy reported and relevant.
    - reference_solution_rating: 7.0
    - reference_solution_reason: CNN model described and evaluated, but public implementation and benchmarks are not available yet.
    - documentation_rating: 6.0
    - documentation_reason: Paper and Gemini doc exist, but full setup instructions and tools are still in progress.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2020-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2020-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: BenchCouncil BigDataBench
    description: The name of the benchmark.
    condition: required
  - url: https://www.benchcouncil.org/BigDataBench/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - big data
    - AI benchmarking
    - data analytics
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Data preprocessing
    - Inference
    - End-to-end data pipelines
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Data processing and AI model inference performance at scale
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Data throughput
    - Latency
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - LSTM
    - SVM
    - XGBoost
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: General
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{gao2018bigdatabench,\n  title={BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite},\n  author={Gao, Wanling and Zhan, Jianfeng and others},\n  year={2018},\n  url={https://arxiv.org/abs/1802.08254}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Evaluates AI at multiple levels (micro to end-to-end); tasks and workloads are clearly defined, though specific I/O formats and constraints vary.
    - dataset_rating: 9.0
    - dataset_reason: Realistic datasets across diverse domains; FAIR structure for many components, but individual datasets may not all be versioned or richly annotated.
    - metrics_rating: 9.0
    - metrics_reason: Latency, throughput, and accuracy clearly defined for end-to-end tasks; consistent across models and setups.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Reference implementations for several tasks exist, but setup across all tasks is complex and not fully streamlined.
    - documentation_rating: 8.0
    - documentation_reason: Central documentation exists, with detailed component breakdowns; environment setup across platforms (e.g., hardware variations) can require manual adjustment.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2021-10-20'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2021-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLPerf HPC
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/hpc
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Cosmology, Climate, Protein Structure, Catalysis
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Scientific ML training and inference on HPC systems
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - HPC
    - training
    - inference
    - scientific ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Training
    - Inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scaling efficiency, training time, model accuracy on HPC
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Training time
    - Accuracy
    - GPU utilization
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CosmoFlow
    - DeepCAM
    - OpenCatalyst
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference, HPC/training
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '4'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Shared framework with MLCommons Science; reference implementations included.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{farrell2021mlperf,\n  title={MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  author={Farrell, Steven and Emani, Murali and others},\n  year={2021},\n  url={https://arxiv.org/abs/2110.11466}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Focused on structured/unstructured data pipelines; clearly defined tasks spanning analytics to AI; some scenarios lack hardware constraint modeling.
    - dataset_rating: 9.0
    - dataset_reason: Built from 13 real-world sources; structured for realistic big data scenarios; partially FAIR-compliant with documented data motifs.
    - metrics_rating: 9.0
    - metrics_reason: Covers data throughput, latency, and accuracy; quantitative and benchmark-ready.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Many pipeline and model examples provided using Hadoop/Spark/Flink; setup effort varies by task and platform.
    - documentation_rating: 8.0
    - documentation_reason: Strong documentation with examples and task specifications; centralized support exists, but task-specific tuning may require domain expertise.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-06-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLCommons Science
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/science
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: AI benchmarks for scientific applications including time-series, imaging, and simulation
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - science AI
    - benchmark
    - MLCommons
    - HPC
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series analysis
    - Image classification
    - Simulation surrogate modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Inference accuracy, simulation speed-up, generalization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MAE
    - Accuracy
    - Speedup vs simulation
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN
    - GNN
    - Transformer
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series, Image/CV, HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: TODO
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Joint national-lab effort under Apache‑2.0 license.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@misc{mlcommons_science2023,\n  title={MLCommons Science Working Group Benchmarks},\n  author={MLCommons Science Working Group},\n  year={2023},\n  url={https://github.com/mlcommons/science}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: TODO
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly defined with HPC system-level constraints and targets.
    - dataset_rating: 9.0
    - dataset_reason: Public scientific datasets (e.g., cosmology, weather); used consistently, though FAIR-compliance of individual datasets varies slightly.
    - metrics_rating: 10.0
    - metrics_reason: Training time, GPU utilization, and accuracy are all directly measured and benchmarked across HPC systems.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Reference implementations available and actively maintained; HPC setup may require domain-specific environment.
    - documentation_rating: 9.0
    - documentation_reason: GitHub repo and papers provide detailed instructions; reproducibility supported across multiple institutions.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2021-07-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2021-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LHC New Physics Dataset
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/pdf/2107.02157
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Particle Physics; Real-time Triggering
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time LHC event filtering for anomaly detection using proton collision data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - proton collision
    - real-time inference
    - event filtering
    - unsupervised ML
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    - Event classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Unsupervised signal detection under latency and bandwidth constraints
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC-AUC
    - Detection efficiency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Autoencoder
    - Variational autoencoder
    - Isolation forest
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '3'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{govorkova2022lhcnewphysics,\n  title={LHC physics dataset for unsupervised New Physics detection at 40 MHz},\n  author={Govorkova, Ekaterina and Puljak, Ema and Pierini, Maurizio and others},\n  journal={Scientific Data},\n  year={2022},\n  doi={10.6084/m9.figshare.5046389},\n  url={https://doi.org/10.5281/zenodo.5046389}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'Zenodo stores: background + 3 black-box signal sets (1M events each)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but lacks a formal task specification or constraints.
    - dataset_rating: 8.0
    - dataset_reason: Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo.
    - metrics_rating: 7.0
    - metrics_reason: Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script.
    - reference_solution_rating: 5.0
    - reference_solution_reason: Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers.
    - documentation_rating: 6.0
    - documentation_reason: Publicly available papers and datasets with descriptions, but no unified README or training setup.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-07-17'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MLCommons Medical AI
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/mlcommons/medical
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Healthcare; Medical AI
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - medical AI
    - federated evaluation
    - privacy-preserving
    - fairness
    - healthcare benchmarks
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Federated evaluation
    - Model validation
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Clinical accuracy, fairness, generalizability, privacy compliance
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC AUC
    - Accuracy
    - Fairness metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - MedPerf-validated CNNs
    - GaNDLF workflows
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Open-source platform under Apache‑2.0; used across 20+ institutions and hospitals :contentReference[oaicite:2]{index=2}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{karargyris2023federated,\n  title={Federated benchmarking of medical artificial intelligence with MedPerf},\n  author={Karargyris, Alex and Sheller, Micah J and others},\n  journal={Nature Machine Intelligence},\n  year={2023},\n  url={https://www.nature.com/articles/s42256-023-00652-2}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-institutional clinical datasets (radiology, EHR)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Diverse scientific tasks (earthquake, CFD, microscopy) with detailed problem statements and goals; system constraints not uniformly applied.
    - dataset_rating: 9.0
    - dataset_reason: Domain-specific datasets (e.g., microscopy, climate); mostly public and structured, but FAIR annotations are not always explicit.
    - metrics_rating: 9.0
    - metrics_reason: Task-specific metrics (MAE, speedup, accuracy) are clear and reproducible.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Reference models (CNN, GNN, Transformer) provided with training/evaluation pipelines.
    - documentation_rating: 9.0
    - documentation_reason: Well-documented, open-sourced, and maintained with examples; strong community support and reproducibility focus.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-10-28'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-10
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: CaloChallenge 2022
    description: The name of the benchmark.
    condition: required
  - url: http://arxiv.org/abs/2410.21611
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LHC Calorimeter; Particle Physics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast generative-model-based calorimeter shower simulation evaluation
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - calorimeter simulation
    - generative models
    - surrogate modeling
    - LHC
    - fast simulation
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Surrogate modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Simulation fidelity, speed, efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Histogram similarity
    - Classifier AUC
    - Generation latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - VAE variants
    - GAN variants
    - Normalizing flows
    - Diffusion models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Surrogate
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Surrogate Modeling
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '31'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset sizes.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{krause2024calochallenge,\n  title={CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation},\n  author={Krause, Claudius and Nachman, Benjamin and others},\n  year={2024},\n  url={https://arxiv.org/abs/2410.21611}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Four LHC calorimeter shower datasets (various voxel resolutions)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: 'Task is clearly defined: real-time anomaly detection from high-rate LHC collisions. Latency and bandwidth constraints are mentioned, though not numerically enforced.'
    - dataset_rating: 9.0
    - dataset_reason: Publicly available via Zenodo, with structured signal/background splits, and rich metadata; nearly fully FAIR.
    - metrics_rating: 9.0
    - metrics_reason: ROC-AUC and detection efficiency are clearly defined and appropriate for unsupervised anomaly detection.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Several baseline methods (autoencoder, VAE, isolation forest) are evaluated; runnable versions available via community repos but not tightly bundled.
    - documentation_rating: 8.0
    - documentation_reason: Paper and data documentation are clear, and the dataset is widely reused. Setup requires some manual effort to reproduce full pipelines.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: ongoing
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Papers With Code (SOTA Platform)
    description: The name of the benchmark.
    condition: required
  - url: https://paperswithcode.com/sota
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General ML; All domains
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - leaderboard
    - benchmarking
    - reproducibility
    - open-source
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple (Classification, Detection, NLP, etc.)
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model performance across tasks (accuracy, F1, BLEU, etc.)
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Task-specific (Accuracy, F1, BLEU, etc.)
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - All published models with code
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '154766'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Community-driven open platform; automatic data extraction and versioning.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@misc{pwc2025,\n  title={Papers With Code: Open machine learning benchmarks and leaderboards},\n  author={Papers With Code},\n  year={2025},\n  url={https://paperswithcode.com}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Curated benchmark-task pairs from literature
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Evaluation setting (federated clinical benchmarking) is well-defined; I/O interfaces vary slightly by task but are standardized in MedPerf platform.
    - dataset_rating: 8.0
    - dataset_reason: Uses distributed, real-world clinical datasets across institutions; FAIR compliance varies across hospitals and data hosts.
    - metrics_rating: 9.0
    - metrics_reason: ROC AUC, accuracy, and fairness metrics are explicitly defined and task-dependent; consistently tracked across institutions.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Validated CNNs and GaNDLF pipelines are used and shared via the MedPerf tool, but some implementations are abstracted behind the platform.
    - documentation_rating: 9.0
    - documentation_reason: Excellent documentation across MedPerf, GaNDLF, and COFE; reproducibility handled via containerized flows and task templates.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2022-01-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Codabench
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: General ML; Multiple
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Open-source platform for organizing reproducible AI benchmarks and competitions
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - benchmark platform
    - code submission
    - competitions
    - meta-benchmark
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model reproducibility, performance across datasets
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Submission count
    - Leaderboard ranking
    - Task-specific metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Arbitrary code submissions
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 98 071
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Hosts 51 public competitions, ~26 k users, 177 k submissions :contentReference[oaicite:2]{index=2}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{xu2021codabench,\n  title={Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},\n  author={Xu, Zhen and Escalera, Sergio and others},\n  journal={Patterns},\n  volume={3},\n  number={7},\n  pages={100543},\n  year={2022},\n  doi={10.1016/j.patter.2022.100543}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: N/A
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: Simulation task (generative calorimeter showers) is clearly stated with multiple datasets, fidelity requirements, and performance constraints.
    - dataset_rating: 9.5
    - dataset_reason: Public datasets available in multiple sizes and formats; well-documented; not versioned
    - metrics_rating: 10.0
    - metrics_reason: Histogram similarity, classifier AUC, and generation latency are clearly defined and benchmarked across all submissions.
    - reference_solution_rating: 9.0
    - reference_solution_reason: 31 model implementations submitted; some made public and reproducible, though others remain undocumented or private.
    - documentation_rating: 9.0
    - documentation_reason: Paper, leaderboard, and Gemini doc are comprehensive; unified repo or launchable baseline kit would push this to a 10.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2021-09-27'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-07
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Sabath (SBI-FAIR)
    description: The name of the benchmark.
    condition: required
  - url: https://sbi-fair.github.io/docs/software/sabath/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Systems; Metadata
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: FAIR metadata framework for ML-driven surrogate workflows in HPC systems
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - meta-benchmark
    - metadata
    - HPC
    - surrogate modeling
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Systems benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Metadata tracking, reproducible HPC workflows
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Metadata completeness
    - FAIR compliance
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - N/A
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Systems
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: NA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: N/A
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc. :contentReference[oaicite:4]{index=4}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@techreport{luszczek2021sabath,\n  title={SABATH: FAIR Metadata Technology for Surrogate Benchmarks},\n  author={Luszczek, Piotr and others},\n  year={2021},\n  institution={University of Tennessee}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: N/A
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle physics datasets.
    - dataset_rating: 8.0
    - dataset_reason: Data is well-structured for SBI and publicly available with clear licensing.
    - metrics_rating: 8.0
    - metrics_reason: Includes likelihood and posterior accuracy; metrics well-matched to SBI.
    - reference_solution_rating: 7.0
    - reference_solution_reason: Baseline SBI models are implemented and reproducible.
    - documentation_rating: 6.0
    - documentation_reason: GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2022-10-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-05
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: PDEBench
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/pdebench/PDEBench
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: CFD; Weather Modeling
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite for ML-based surrogates solving time-dependent PDEs
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - PDEs
    - CFD
    - scientific ML
    - surrogate modeling
    - NeurIPS
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Supervised Learning
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Time-dependent PDE modeling; physical accuracy
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - boundary RMSE
    - Fourier RMSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - FNO
    - U-Net
    - PINN
    - Gradient-Based inverse methods
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 'Yes'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email :contentReference[oaicite:6]{index=6}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{takamoto2022pdebench,\n  author={Takamoto, Makoto and Praditia, Timothy and others},\n  title={PDEBench: An Extensive Benchmark for Scientific Machine Learning},\n  booktitle={NeurIPS Datasets and Benchmarks Track},\n  year={2022},\n  url={https://arxiv.org/abs/2210.07182}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: DaRUS repository via DOI:10.18419/darus‑2986
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Clearly defined PDE-solving tasks with well-specified constraints and solution formats.
    - dataset_rating: 9.0
    - dataset_reason: Includes synthetic and real-world PDE datasets with detailed format descriptions.
    - metrics_rating: 8.0
    - metrics_reason: Uses L2 error and other norms relevant to PDE solutions.
    - reference_solution_rating: 7.0
    - reference_solution_reason: Includes baseline solvers and trained models across multiple PDE tasks.
    - documentation_rating: 8.0
    - documentation_reason: Well-organized GitHub with examples, dataset loading scripts, and training configs.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: The Well
    description: The name of the benchmark.
    condition: required
  - url: https://polymathic-ai.org/the_well/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: biological systems, fluid dynamics, acoustic scattering, astrophysical MHD
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Foundation model + surrogate dataset spanning 16 physical simulation domains
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - surrogate modeling
    - foundation model
    - physics simulations
    - spatiotemporal dynamics
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Supervised Learning
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Surrogate modeling, physics-based prediction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Dataset size
    - Domain breadth
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - FNO baselines
    - U‑Net baselines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Foundation model, Surrogate
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '16'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: 'Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. :contentReference[oaicite:2]{index=2}'
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{ohana2024well,\n  title={The well: a large-scale collection of diverse physics simulations for machine learning},\n  author={Ohana, Ruben and McCabe, Michael and Meyer, Lucas and others},\n  journal={NeurIPS},\n  volume={37},\n  pages={44989--45037},\n  year={2024}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 16 simulation datasets (HDF5) via PyPI/GitHub
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: Explores LLM understanding of mental health scenarios; framing is creative but loosely defined.
    - dataset_rating: 6.0
    - dataset_reason: Dataset is described in concept but not released; privacy limits public access though synthetic proxies are referenced.
    - metrics_rating: 7.0
    - metrics_reason: Uses manual annotation and quality scores, but lacks standardized automatic metrics.
    - reference_solution_rating: 6.0
    - reference_solution_reason: Provides few-shot prompt examples and human rating calibration details.
    - documentation_rating: 5.0
    - documentation_reason: Paper gives use cases, but code and data are not yet public.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-10-31'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-11
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LLM-Inference-Bench
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/argonne-lcf/LLM-Inference-Bench
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Hardware performance benchmarking of LLMs on AI accelerators
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM
    - inference benchmarking
    - GPU
    - accelerator
    - throughput
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Inference Benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Inference throughput, latency, hardware utilization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Token throughput (tok/s)
    - Latency
    - Framework-hardware mix performance
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA-2‑7B
    - LLaMA-2‑70B
    - Mistral‑7B
    - Qwen‑7B
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Inference Benchmarking
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ''
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Licensed under BSD‑3, maintained by Argonne; supports GPUs and accelerators. :contentReference[oaicite:4]{index=4}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{chitty2024llm,\n  title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators},\n  author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and others},\n  journal={arXiv preprint arXiv:2411.00136},\n  year={2024}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Performance logs, model-hardware pairs
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: PDE tasks (forward/inverse) and I/O structures are clearly specified with detailed PDE context and constraints.
    - dataset_rating: 10.0
    - dataset_reason: Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant.
    - metrics_rating: 9.0
    - metrics_reason: Uses RMSE variants and Fourier-based errors.
    - reference_solution_rating: 10.0
    - reference_solution_reason: Baselines (FNO, U-Net, PINN) implemented and ready-to-run; strong community adoption.
    - documentation_rating: 9.0
    - documentation_reason: Clean GitHub with usage, dataset links, and tutorial notebooks.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-12-12'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SGLang Framework
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/sgl-project/sglang/tree/main/benchmark
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM Vision
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Fast serving framework for LLMs and vision-language models
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM serving
    - vision-language
    - RadixAttention
    - performance
    - JSON decoding
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Model serving framework
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Serving throughput, JSON/task-specific latency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - Time-to-first-token
    - Throughput gain vs baseline
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaVA
    - DeepSeek
    - Llama
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: LLM Vision
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Model serving
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ''
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025. :contentReference[oaicite:6]{index=6}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{zheng2023sglang,\n  title={SGLang: Efficient Execution of Structured Language Model Programs},\n  author={Zheng, Lianmin and Yin, Liangsheng and others},\n  year={2023},\n  url={https://arxiv.org/abs/2312.07104}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Benchmark configs (dummy or real)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Clearly framed around surrogate learning across 16 domains, but not all tasks are formally posed or constrained in a unified benchmark protocol. Paper mentions performance on NVIDIA H100.
    - dataset_rating: 9.0
    - dataset_reason: FAIR-compliant physics simulation dataset, structured in HDF5 with unified metadata.
    - metrics_rating: 7.0
    - metrics_reason: Metrics like dataset size and domain coverage are listed, but standardized quantitative model evaluation metrics (e.g., RMSE, MAE) are not enforced.
    - reference_solution_rating: 9.0
    - reference_solution_reason: FNO and U-Net baselines available; full benchmarking implementations pending NeurIPS paper code release.
    - documentation_rating: 10.0
    - documentation_reason: Site and GitHub offer a unified API, metadata standards, and dataset loading tools; NeurIPS paper adds detailed design context.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-09-12'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: vLLM Inference and Serving Engine
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/vllm-project/vllm/tree/main/benchmarks
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: High-throughput, memory-efficient inference and serving engine for LLMs
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - LLM inference
    - PagedAttention
    - CUDA graph
    - streaming API
    - quantization
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Inference Benchmarking
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Throughput, latency, memory efficiency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - Time to First Token (TTFT)
    - Memory footprint
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA
    - Mixtral
    - FlashAttention-based models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Inference
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Incubated by LF AI and Data; achieves up to 24× throughput over HuggingFace Transformers :contentReference[oaicite:2]{index=2}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and others},\n  booktitle={SOSP 2023},\n  year={2023}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Benchmark scripts and model configurations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Benchmarks hardware performance of LLM inference across multiple platforms with well-defined input/output and platform constraints.
    - dataset_rating: 7.0
    - dataset_reason: Uses structured log files and configs instead of conventional datasets; suitable for inference benchmarking.
    - metrics_rating: 9.0
    - metrics_reason: Clear throughput, latency, and utilization metrics; platform comparison dashboard enhances evaluation.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Includes reproducible scripts and example runs; models like LLaMA and Mistral are referenced with platform-specific configs.
    - documentation_rating: 8.0
    - documentation_reason: GitHub contains clear instructions, platform details, and framework comparisons.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2022-06-22'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-01
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: vLLM Performance Dashboard
    description: The name of the benchmark.
    condition: required
  - url: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: LLM; HPC/inference
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Interactive dashboard showing inference performance of vLLM
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Dashboard
    - Throughput visualization
    - Latency analysis
    - Metric tracking
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Performance visualization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Throughput, latency, hardware utilization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Tokens/sec
    - TTFT
    - Memory usage
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaMA-2
    - Mistral
    - Qwen
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: HPC/inference
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Visualization
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Built using ObservableHQ; integrates live data from vLLM benchmarks.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@misc{mo2024vllm_dashboard,\n  title={vLLM Performance Dashboard},\n  author={Mo, Simon},\n  year={2024},\n  url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Dashboard configurations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Framed as a model-serving tool rather than a benchmark, but includes benchmark configurations and real model tasks.
    - dataset_rating: 6.0
    - dataset_reason: Mostly uses dummy configs or external model endpoints for evaluation; not designed around a formal dataset.
    - metrics_rating: 8.0
    - metrics_reason: 'Well-defined serving metrics: tokens/sec, time-to-first-token, and gain over baselines.'
    - reference_solution_rating: 9.0
    - reference_solution_reason: Core framework includes full reproducible serving benchmarks and code; multiple deployment case studies.
    - documentation_rating: 9.0
    - documentation_reason: High-quality usage guides, examples, and performance tuning docs.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2022-04-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla NeuralForecast
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series forecasting; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: High-performance neural forecasting library with >30 models
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - time-series
    - neural forecasting
    - NBEATS, NHITS, TFT
    - probabilistic forecasting
    - usability
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Forecast accuracy, interpretability, speed
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    - CRPS
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NBEATS
    - NHITS
    - TFT
    - DeepAR
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. Fi­rst official NHITS implementation. :contentReference[oaicite:4]{index=4}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@misc{olivares2022library_neuralforecast,\n  author={Olivares, Kin G. and Challú, Cristian and others},\n  title={NeuralForecast: User friendly state‑of‑the‑art neural forecasting models},\n  year={2022},\n  howpublished={{PyCon} US},\n  url={https://github.com/Nixtla/neuralforecast}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: M4, electricity, standard TS benchmarks
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Targets high-throughput LLM inference via PagedAttention and memory-optimized serving; benchmarks cover many configs.
    - dataset_rating: 7.0
    - dataset_reason: Focuses on model configs and streaming input/output pipelines rather than classical datasets.
    - metrics_rating: 9.0
    - metrics_reason: Strong token/sec, memory usage, and TTFT metrics; comparative plots and logs included.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Benchmarks reproducible via script with support for multiple models and hardware types.
    - documentation_rating: 9.0
    - documentation_reason: Excellent GitHub docs, CLI/API usage, and deployment walkthroughs.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-06-01'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast NHITS
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Official NHITS implementation for long-horizon time series forecasting
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - NHITS
    - long-horizon forecasting
    - neural interpolation
    - time-series
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Accuracy, compute efficiency for long series
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - NHITS
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Official implementation in NeuralForecast, included since its AAAI 2023 release.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@inproceedings{challu2023nhits,\n  title={NHITS: Neural Hierarchical Interpolation for Time Series Forecasting},\n  author={Challu, Cristian and Olivares, Kin G. and others},\n  booktitle={AAAI 2023},\n  year={2023}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Standard forecast datasets (M4, etc.)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: Primarily a visualization frontend; underlying benchmark definitions come from vLLM project.
    - dataset_rating: 6.0
    - dataset_reason: No traditional dataset; displays live or logged benchmark metrics.
    - metrics_rating: 9.0
    - metrics_reason: Live throughput, memory, latency, and TTFT displayed interactively; highly informative for performance analysis.
    - reference_solution_rating: 7.0
    - reference_solution_reason: Dashboard built on vLLM benchmarks but not itself a complete experiment package.
    - documentation_rating: 8.0
    - documentation_reason: Observable notebooks are intuitive; customization instructions are minimal but UI is self-explanatory.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-10-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast TimeLLM
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Reprogramming LLMs for time series forecasting
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Time-LLM
    - language model
    - time-series
    - reprogramming
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Model reuse via LLM, few-shot forecasting
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - MAPE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Time‑LLM
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Fully open-source; transforms forecasting using LLM text reconstruction.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{jin2023time,\n  title={Time‑LLM: Time Series Forecasting by Reprogramming Large Language Models},\n  author={Jin, Ming and Wang, Shiyu and others},\n  journal={arXiv preprint arXiv:2310.01728},\n  year={2023}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Standard forecast datasets (M4, etc.)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
    - dataset_rating: 6.0
    - dataset_reason: Uses open time series datasets, but lacks a consolidated data release or splits.
    - metrics_rating: 7.0
    - metrics_reason: Reports metrics like MASE and SMAPE, standard in forecasting.
    - reference_solution_rating: 6.0
    - reference_solution_reason: Provides TimeLLM with open source, but no other baselines included.
    - documentation_rating: 6.0
    - documentation_reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-10-05'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-06
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Nixtla Neural Forecast TimeGPT
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/Nixtla/neuralforecast
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Time-series; General ML
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Time-series foundation model "TimeGPT" for forecasting and anomaly detection
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - TimeGPT
    - foundation model
    - time-series
    - generative model
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Time-series forecasting
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Zero-shot forecasting, anomaly detection
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - RMSE
    - Anomaly detection metrics
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - TimeGPT
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Platform
    description: The type of this benchmark.
    condition: required
  - ml_task: Forecasting
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '26'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Offered via Nixtla API and Azure Studio; enterprise-grade support available.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{garza2023timegpt,\n  title={TimeGPT‑1: A Foundation Model for Time Series},\n  author={Garza, Azul and Challu, Cristian and others},\n  year={2023},\n  url={https://arxiv.org/abs/2310.03589}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Pretrained on 100 B+ time series via Nixtla
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 7.0
    - problem_spec_reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
    - dataset_rating: 6.0
    - dataset_reason: Uses open time series datasets, but lacks a consolidated data release or splits.
    - metrics_rating: 7.0
    - metrics_reason: Reports metrics like MASE and SMAPE, standard in forecasting.
    - reference_solution_rating: 6.0
    - reference_solution_reason: Provides TimeLLM with open source, but no other baselines included.
    - documentation_rating: 6.0
    - documentation_reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge (Gravitational Waves)
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/2626/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Astrophysics; Time-series
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - gravitational waves
    - astrophysics
    - time-series
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Novel event detection in physical signals
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC‑AUC
    - Precision/Recall
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Deep latent CNNs
    - Autoencoders
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench. :contentReference[oaicite:2]{index=2}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{campolongo2025hdranomaly2,\n  title={Building Machine Learning Challenges for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Preprocessed LIGO/Hanford and Livingston waveforms
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Novel approach treating forecasting as text generation is explained; framing is less conventional.
    - dataset_rating: 9.0
    - dataset_reason: Compatible with standard forecasting datasets (e.g., M4, electricity).
    - metrics_rating: 8.0
    - metrics_reason: RMSE and MAPE are included, but less emphasis on interpretability or time-series domain constraints.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Open-source with reprogramming layers, LLM interface scripts provided.
    - documentation_rating: 8.0
    - documentation_reason: Model and architecture overview present, though usability guide is slightly lighter than others.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge (Butterfly)
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/3764/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Genomics; Image/CV
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - computer vision
    - genomics
    - butterfly hybrids
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Hybrid detection in biological systems
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Classification accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN-based detectors
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Hybrid detection benchmarks hosted on Codabench. :contentReference[oaicite:4]{index=4}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{campolongo2025hdranomaly,\n  title={Building Machine Learning Challenges for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Butterfly hybrid image dataset
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task of detecting rare anomalies in butterfly physics is well-described with physics motivation.
    - dataset_rating: 7.0
    - dataset_reason: Real detector data with injected anomalies is available, but requires NDA for full access.
    - metrics_rating: 7.0
    - metrics_reason: Uses ROC, F1, and anomaly precision, standard in challenge evaluations.
    - reference_solution_rating: 4.0
    - reference_solution_reason: Partial baselines described, but no codebase or reproducible runs.
    - documentation_rating: 6.0
    - documentation_reason: Challenge site includes overview and metrics, but limited in walkthrough or examples.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-03-03'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-03
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: HDR ML Anomaly Challenge (Sea Level Rise)
    description: The name of the benchmark.
    condition: required
  - url: https://www.codabench.org/competitions/3223/
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Climate Science; Time-series, Image/CV
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - anomaly detection
    - climate science
    - sea-level rise
    - time-series
    - remote sensing
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Anomaly detection
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Detection of environmental anomalies
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - ROC‑AUC
    - Precision/Recall
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNNs, RNNs, Transformers
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Time-series, Image/CV
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Sponsored by NSF HDR; integrates sensor and satellite data. :contentReference[oaicite:6]{index=6}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{campolongo2025hdranomaly3,\n  title={Building Machine Learning Challenges for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Sea-level time-series and satellite imagery
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Clear anomaly detection objective framed for physical signal discovery (LIGO/Virgo).
    - dataset_rating: 10.0
    - dataset_reason: Preprocessed waveform data from dual interferometers, public and well-structured.
    - metrics_rating: 9.0
    - metrics_reason: ROC-AUC, Precision/Recall, and confusion-based metrics are standardized.
    - reference_solution_rating: 1.0
    - reference_solution_reason: No starter model or baseline code linked
    - documentation_rating: 9.0
    - documentation_reason: Codabench page, GitHub starter kit, and related papers provide strong guidance.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2025-01-24'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2025-02
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Single Qubit Readout on QICK System
    description: The name of the benchmark.
    condition: required
  - url: https://github.com/fastmachinelearning/ml-quantum-readout
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Quantum Computing
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Real-time single-qubit state classification using FPGA firmware
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - qubit readout
    - hls4ml
    - FPGA
    - QICK
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Single-shot fidelity, inference latency
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - Latency
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - hls4ml quantized NN
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Supervised Learning
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '-'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization. :contentReference[oaicite:1]{index=1}
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{diguglielmo2025endtoend,\n  title={End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml},\n  author={Di Guglielmo, Giuseppe and Campos, Javier and others},\n  year={2025},\n  url={https://arxiv.org/abs/2501.14663}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 'Zenodo: ml-quantum-readout dataset (zenodo.org/records/14427490)'
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Task clearly framed around detecting hybrid species via images, but exact labeling methods and hybrid definitions may need elaboration.
    - dataset_rating: 8.0
    - dataset_reason: Dataset hosted on Codabench; appears structured but details on image sourcing and labeling pipeline are limited.
    - metrics_rating: 9.0
    - metrics_reason: Classification accuracy and F1 are standard and appropriate.
    - reference_solution_rating: 1.0
    - reference_solution_reason: No starter model or baseline code linked
    - documentation_rating: 7.5
    - documentation_reason: Codabench task page describes dataset and evaluation method but lacks full API/docs.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2023-11-20'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2023-11
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: 'GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark'
    description: The name of the benchmark.
    condition: required
  - url: https://arxiv.org/abs/2311.12022
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Science (Biology, Physics, Chemistry)
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Graduate-level, expert-validated multiple-choice questions hard even with web access
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - Google-proof
    - multiple-choice
    - expert reasoning
    - science QA
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multiple choice
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scientific reasoning, knowledge probing
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GPT‑4 baseline
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multiple choice
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Multiple choice
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 448 questions
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: “Google-proof”; supports oversight research.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{rein2023gpqa,\n  title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},\n  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others},\n  year={2023},\n  url={https://arxiv.org/abs/2311.12022}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: GPQA dataset (zip/HuggingFace)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Clear dual-modality task (image + time-series); environmental focus is well described.
    - dataset_rating: 9.0
    - dataset_reason: Time-series and satellite imagery data provided; sensor info and collection intervals are explained.
    - metrics_rating: 9.0
    - metrics_reason: ROC-AUC, Precision/Recall are appropriate and robust.
    - reference_solution_rating: 1.0
    - reference_solution_reason: No starter model or baseline code linked
    - documentation_rating: 6.5
    - documentation_reason: Moderate Codabench documentation with climate context; lacks pipeline-level walkthrough.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SeafloorAI
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97432
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Marine Science; Vision-Language
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Large-scale vision-language dataset for seafloor mapping and geological classification
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - sonar imagery
    - vision-language
    - seafloor mapping
    - segmentation
    - QA
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Image segmentation
    - Vision-language QA
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Geospatial understanding, multimodal reasoning
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Segmentation pixel accuracy
    - QA accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - SegFormer
    - ViLT-style multimodal models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Vision-Language
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Segmentation, QA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ~696K images
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Data processing code publicly available, covering five geological layers; curated with marine scientists :contentReference[oaicite:2]{index=2}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{nguyen2024seafloorai,\n  title={SeafloorAI: A Large-scale Vision‑Language Dataset for Seafloor Geological Survey},\n  author={Nguyen, Kien X. and Qiao, Fengchun and others},\n  year={2024},\n  url={https://arxiv.org/abs/2411.00172}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Sonar imagery + annotations (~15 TB)
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Real-time qubit classification task clearly defined in quantum instrumentation context.
    - dataset_rating: 9.0
    - dataset_reason: Dataset available on Zenodo with signal traces; compact and reproducible.
    - metrics_rating: 9.0
    - metrics_reason: Accuracy and latency are well defined and crucial in this setting.
    - reference_solution_rating: 9.0
    - reference_solution_reason: GitHub repo has reproducible code and HLS firmware targeting FPGA.
    - documentation_rating: 8.0
    - documentation_reason: Good setup instructions, but no interactive visualization or starter notebook.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SuperCon3D
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97553
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Materials Science; Superconductivity
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Dataset and models for predicting and generating high‑Tc superconductors using 3D crystal structures
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - superconductivity
    - crystal structures
    - equivariant GNN
    - generative models
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Regression (Tc prediction)
    - Generative modeling
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Structure-to-property prediction, structure generation
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - MAE (Tc)
    - Validity of generated structures
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - SODNet
    - DiffCSP‑SC
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Materials Modeling
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Models
    description: The type of this benchmark.
    condition: required
  - ml_task: Regression, Generation
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: '2'
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrates advantage of combining ordered and disordered structural data in model design :contentReference[oaicite:4]{index=4}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{zhuang2024supercon3d,\n  title={SuperCon3D: Learning Superconductivity from Ordered and Disordered Material Structures},\n  author={Zuo, Zhong and others},\n  year={2024},\n  note={NeurIPS Poster}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: 3D crystal + Tc records
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 10.0
    - problem_spec_reason: Multimodal task (segmentation + natural language QA pairs);.
    - dataset_rating: 10.0
    - dataset_reason: sonar imagery + masks + descriptions, georeferenced and labeled with QA
    - metrics_rating: 9.0
    - metrics_reason: Pixel accuracy and QA metrics clearly defined; tasks split by modality.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Baseline models (SegFormer, ViLT) are cited, partial configs likely available.
    - documentation_rating: 8.5
    - documentation_reason: Paper + GitHub metadata and processing details are comprehensive, though full dataset is not yet available.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: GeSS
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97816
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Scientific ML; Geometric Deep Learning
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite evaluating geometric deep learning models under real-world distribution shifts
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - geometric deep learning
    - distribution shift
    - OOD robustness
    - scientific applications
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Classification
    - Regression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: OOD performance in scientific settings
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - RMSE
    - OOD robustness delta
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GCN
    - EGNN
    - DimeNet++
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Geometric DL
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Classification, Regression
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 30 settings × 11 algos
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes no-OOD, unlabeled-OOD, and few-label scenarios :contentReference[oaicite:6]{index=6}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{zou2024gess,\n  title={GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  author={Zou, Deyu and Liu, Shikun and others},\n  year={2024},\n  note={NeurIPS Poster}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Scientific graph datasets with shift splits
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Well-defined problem (Tc prediction, generation) with strong scientific motivation (high-Tc materials), but no formal hardware constraints.
    - dataset_rating: 9.0
    - dataset_reason: Includes curated 3D crystal structures and Tc data; readily downloadable and used in paper models.
    - metrics_rating: 9.0
    - metrics_reason: MAE and structural validity used, well-established in materials modeling.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Provides two reference models (SODNet, DiffCSP-SC) with results. Code likely available post-conference.
    - documentation_rating: 8.0
    - documentation_reason: Paper and poster explain design choices well; software availability confirms reproducibility but limited external documentation.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Vocal Call Locator (VCL)
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97470
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Neuroscience; Bioacoustics
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmarking sound-source localization of rodent vocalizations from multi-channel audio
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - source localization
    - bioacoustics
    - time-series
    - SSL
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Sound source localization
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Source localization accuracy in bioacoustic settings
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Localization error (cm)
    - Recall/Precision
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - CNN-based SSL models
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Real-time
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: Anomaly detection / localization
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 767,295 sounds
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Dataset spans real, simulated, and mixed audio; supports benchmarking across data types :contentReference[oaicite:2]{index=2}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{peterson2024vcl,\n  title={Vocal Call Locator Benchmark for localizing rodent vocalizations},\n  author={Peterson, Ralph and Tanelus, Aramis and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97470}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-channel audio + annotations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Clear benchmark scenarios across GDL tasks under multiple real-world shift settings; OOD settings precisely categorized.
    - dataset_rating: 8.0
    - dataset_reason: Scientific graph datasets provided in multiple shift regimes; standardized splits across domains. Exact format of data not specified.
    - metrics_rating: 9.0
    - metrics_reason: Includes base metrics (accuracy, RMSE) plus OOD delta robustness for evaluation under shifts.
    - reference_solution_rating: 9.0
    - reference_solution_reason: Multiple baselines (11 algorithms × 3 backbones) evaluated; setup supports reproducible comparison.
    - documentation_rating: 2.0
    - documentation_reason: Paper, poster, and source code provide thorough access to methodology and implementation. Setup instructions and accompanying code not present.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: MassSpecGym
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97823
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Cheminformatics; Molecular Discovery
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmark suite for discovery and identification of molecules via MS/MS
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - mass spectrometry
    - molecular structure
    - de novo generation
    - retrieval
    - dataset
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - De novo generation
    - Retrieval
    - Simulation
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Molecular identification and generation from spectral data
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Structure accuracy
    - Retrieval precision
    - Simulation MSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Graph-based generative models
    - Retrieval baselines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Benchmark
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Generation, retrieval, simulation
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: ~1M spectra
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks :contentReference[oaicite:4]{index=4}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{bushuiev2024massspecgym,\n  title={MassSpecGym: A benchmark for the discovery and identification of molecules},\n  author={Bushuiev, Roman and Bushuiev, Anton and others},\n  year={2024},\n  note={NeurIPS Spotlight Poster},\n  url={https://neurips.cc/virtual/2024/poster/97823}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Public MS/MS spectra with structure annotations
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Focused on sound source localization for rodent vocalizations in lab settings; well-scoped.
    - dataset_rating: 9.5
    - dataset_reason: 767000 annotated audio segments across diverse conditions. Minor deduction for no train/test/valid split.
    - metrics_rating: 9.5
    - metrics_reason: Localization error, precision/recall used
    - reference_solution_rating: 7.0
    - reference_solution_reason: CNN-based baselines referenced but unclear whether pretrained models or training code are available.
    - documentation_rating: 2.0
    - documentation_reason: Poster and paper outline benchmark intent and setup; repo expected but not confirmed in dataset card.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Urban Data Layer (UDL)
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97837
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Urban Computing; Data Engineering
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Unified data pipeline for multi-modal urban science research
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - data pipeline
    - urban science
    - multi-modal
    - benchmark
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Prediction
    - Classification
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Multi-modal urban inference, standardization
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Task-specific accuracy or RMSE
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Baseline regression/classification pipelines
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Data engineering
    description: The ML motif tested.
    condition: '>=1'
  - type: Framework
    description: The type of this benchmark.
    condition: required
  - ml_task: Prediction, classification
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 4 tasks
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models :contentReference[oaicite:6]{index=6}.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{wang2024urbandatalayer,\n  title={UrbanDataLayer: A unified data pipeline for urban science},\n  author={Wang, Yiheng and Wang, Tianyu and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97837}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Multi-modal urban datasets, standardized
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: Three tasks (de novo generation, retrieval, simulation) are clearly defined for MS/MS molecule discovery.
    - dataset_rating: 10.0
    - dataset_reason: Over 1 million spectra with structure annotations; dataset is open-source and well-documented.
    - metrics_rating: 9.0
    - metrics_reason: Task-appropriate metrics (structure accuracy, precision, MSE) are specified and used consistently.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Baseline models are available (graph-based and retrieval), though not exhaustive.
    - documentation_rating: 9.0
    - documentation_reason: GitHub repo and poster provide code and reproducibility guidance.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: Delta Squared‑DFT
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97788
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Computational Chemistry; Materials Science
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - density functional theory
    - Delta Squared‑ML correction
    - reaction energetics
    - quantum chemistry
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Regression
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: High-accuracy energy prediction, DFT correction
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Mean Absolute Error (eV)
    - Energy ranking accuracy
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - Delta Squared‑ML correction networks
    - Kernel ridge regression
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Scientific ML
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset + Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Regression
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 10 datasets
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{liu2024delta2dft,\n  title={Delta Squared‑DFT: Machine‑Learning Corrected Density Functional Theory for Reaction Energetics},\n  author={Liu, Wei and Chen, Rong and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97788}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Reaction energy sets with DFT and high-level references
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 8.0
    - problem_spec_reason: Clear goals around unifying urban data formats and tasks (e.g., air quality prediction), though some specifics could be more formal.
    - dataset_rating: 9.0
    - dataset_reason: Multi-modal data is standardized and accessible; GitHub repo available.
    - metrics_rating: 8.0
    - metrics_reason: Uses common task metrics like accuracy/RMSE, though varies by task.
    - reference_solution_rating: 7.0
    - reference_solution_reason: Baseline regression/classification models included.
    - documentation_rating: 8.0
    - documentation_reason: Source code supports pipeline reuse, but formal evaluation splits may vary.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: LLMs for Crop Science
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97570
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Agricultural Science; NLP
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - crop science
    - prompt engineering
    - domain adaptation
    - question answering
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Question Answering
    - Inference
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Scientific knowledge, crop reasoning
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - GPT-4
    - LLaMA-2‑13B
    - T5‑XXL
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: NLP
    description: The ML motif tested.
    condition: '>=1'
  - type: Dataset
    description: The type of this benchmark.
    condition: required
  - ml_task: QA, inference
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 3,500 prompts
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{patel2024llmcropsci,\n  title={Large Language Models for Crop Science: Benchmarking Domain Reasoning and QA},\n  author={Patel, Deepak and Zhao, Lan and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97570}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: Crop science QA dataset
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 9.0
    - problem_spec_reason: The task of ML correction to DFT energy predictions is well-specified.
    - dataset_rating: 9.0
    - dataset_reason: 10 public reaction datasets with DFT and CC references; well-documented.
    - metrics_rating: 8.0
    - metrics_reason: Uses MAE and ranking accuracy, suitable for this task.
    - reference_solution_rating: 8.0
    - reference_solution_reason: Includes both Δ²‑ML and KRR baselines.
    - documentation_rating: 9.0
    - documentation_reason: Public benchmarks and clear reproducibility via datasets and model code.
    description: A set of ratings associated with the benchmark.
    condition: optional
- - date: '2024-12-13'
    description: The date of availability of the benchmark. If an official release date is not available, use the date of adding the entry.
    condition: required
  - version: TODO
    description: The version number of the benchmark.
    condition: optional
  - last_updated: 2024-12
    description: 'The date when the entry was last updated. Format: YYYY-mm-dd.'
    condition: optional
  - expired: TODO
    description: An indication if the benchmark is no longer valid.
    condition: optional
  - valid: 'yes'
    description: Identifies if the benchmark is valid at the time of review.
    condition: required
  - name: SPIQA (LLM)
    description: The name of the benchmark.
    condition: required
  - url: https://neurips.cc/virtual/2024/poster/97575
    description: The main URL for this benchmark.
    condition: required
  - doi: TODO
    description: A DOI number that may be associated with the benchmark.
    condition: optional
  - domain: Multimodal Scientific QA; Computer Vision
    description: The scientific domain this benchmark belongs to.
    condition: required
  - focus: Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance)
    description: Short summary of the focus of this benchmark.
    condition: required
  - keywords:
    - multimodal QA
    - scientific figures
    - image+text
    - chain-of-thought prompting
    description: List of keywords relevant for the benchmark.
    condition: '>=1'
  - summary: TODO
    description: An easy-to-understand description of the benchmark.
    condition: required
  - licensing: TODO
    description: The license for the benchmark.
    condition: required
  - task_types:
    - Multimodal QA
    description: The task type defined by this benchmark.
    condition: required
  - ai_capability_measured: Visual reasoning, scientific figure understanding
    description: The AI capabilities that are measured.
    condition: '>=1'
  - metrics:
    - Accuracy
    - F1 score
    description: A list of metrics used by the benchmark.
    condition: '>=1'
  - models:
    - LLaVA
    - MiniGPT‑4
    - Owl‑LLM adapter variants
    description: A list of models that are used by the benchmark.
    condition: '>=1'
  - ml_motif: Multimodal QA
    description: The ML motif tested.
    condition: '>=1'
  - type: Benchmark
    description: The type of this benchmark.
    condition: required
  - ml_task: Multimodal QA
    description: A list of machine learning tasks used in this benchmark.
    condition: '>=1'
  - solutions: 10 adapter variants
    description: Obsolete, as results should capture this.
    condition: deprecated
  - notes: Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs.
    description: Additional notes or context.
    condition: optional
  - contact: TODO
    description: A contact person or organization for this benchmark.
    condition: optional
  - cite:
    - "@article{zhong2024spiqa_llm,\n  title={SPIQA‑LLM: Evaluating LLM Adapters on Scientific Figure QA},\n  author={Zhong, Xiaoyan and Gao, Yijian and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97575}\n}"
    description: A list of bibtex citations. At least one must be provided.
    condition: '>=1'
  - dataset: SPIQA image-question set
    description: A list of datasets used with the benchmark.
    condition: '>=1'
  - results: TODO
    description: A list of results, ideally published on the web. It could also point to a paper.
    condition: required
  - fair: TODO
    description: Attributes important with the fair principle. It is true if fair applies.
    condition: required
  - ratings:
    - problem_spec_rating: 6.0
    - problem_spec_reason: Task of QA over scientific figures is interesting but not fully formalized in input/output terms.
    - dataset_rating: 6.0
    - dataset_reason: Uses SPIQA dataset with ~10 adapters; figures and questions are included, but not fully open.
    - metrics_rating: 7.0
    - metrics_reason: Reports accuracy and F1; fair but no visual reasoning-specific metric.
    - reference_solution_rating: 6.0
    - reference_solution_reason: 10 LLM adapter baselines; results included.
    - documentation_rating: 5.0
    - documentation_reason: Poster paper and limited documentation; no reproducibility instructions.
    description: A set of ratings associated with the benchmark.
    condition: optional
