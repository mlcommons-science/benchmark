# MLCommons Science Working Group AI Benchmark Collection

This repository holds a list of scientific AI benchmarks maintained by the MLCommons Science Working Group. Benchmark definitions live in YAML, and we provide scripts to validate, summarize, and publish the collection. Contributions from the community are welcome.

## Quick Links

- Benchmark website: <https://mlcommons-science.github.io/benchmark/>
- Catalogue sources (current): `source/benchmarks.yaml`, `source/benchmarks-addon.yaml`
- YAML schema and example: `source/benchmarks-format.yaml`, `source/benchmarks-sample.yaml`
- Benchmark format guide: `docs/benchmark-format.md`
- Tooling reference: `docs/tooling.md`
- Rating system: `docs/ratings.md`
- Contribution guide: `CONTRIBUTING.md`

## What Lives in This Repository

- Versioned benchmark metadata expressed in YAML, including citations, FAIR scores, and ratings.
- A Python toolchain (`bin/generate.py` and supporting modules) that validates entries and produces Markdown, LaTeX, and MkDocs outputs.
- Automation for building the public benchmark site in `www/science-ai-benchmarks/`.

All generated artifacts are derived from the YAML sources. Do not edit rendered files manually.

## Quick Start

### Local Python environment

```bash
git clone https://github.com/mlcommons-science/benchmark.git
cd benchmark
python3 -m venv .venv
source .venv/bin/activate
make install           # installs dependencies from requirements.txt
```

### Docker workflow

A Docker image is provided with Python, LaTeX, and the repository tooling preconfigured:

```bash
docker build -t benchmark .
docker run --rm -it -v "$PWD":/workspace -e SERVE_HOST=0.0.0.0 -p 8000:8000 benchmark
```

You can execute the Makefile targets and scripts inside the container.

## Add or Update a Benchmark

1. Create a feature branch in your fork (see `CONTRIBUTING.md` for the full workflow).
2. Use `source/benchmarks-sample.yaml` together with the schema in `source/benchmarks-format.yaml` as a reference. Additional YAML source files can be introduced following the same structure.
3. Apply edits with two-space indentation and use the literal string `"unknown"` when information is unavailable.
4. Run the catalogue checks before committing:
   ```bash
   make check          # ensures required fields are present and flags non-ASCII characters
   make check_url      # optional; validates that referenced URLs resolve
   ```
5. Review your diff so the pull request contains only YAML and documentation updates. Avoid committing changes under `content/` or `www/`.
6. Open a pull request. Maintainers will regenerate Markdown, LaTeX, and MkDocs outputs during review before publishing.

Consult `docs/tooling.md` for the full list of Make targets and script options.

## Repository Layout

- `bin/` – validation and publishing scripts.
- `content/` – generated Markdown, TeX, and MkDocs assets that feed the published site.
- `source/` – benchmark catalogue, schema, and MkDocs homepage (`index.md`).
- `tests/` – unit tests for key scripts.
- `www/` – MkDocs output generated by `make mkdocs`; contains the static site published via GitHub Pages and is ignored by Git.

## Citation

If you reference this benchmark collection in academic work, please cite:

Gregor von Laszewski, Reece Shiraishi, Anjay Krishnan, Nhan Tran, Benjamin Hawks, Marco Colombo, and Geoffrey C. Fox. *AI Scientific Benchmarks Comparison*. MLCommons Science Working Group, 2025. Available at <https://mlcommons-science.github.io/benchmark/benchmarks.pdf>.

BibTeX:

```
@misc{www-las-mlcommons-benchmark-coolection,
      author = {
        Gregor von Laszewski and 
        Ben Hawks and 
        Marco Colombo and
        Reece Shiraishi and
        Anjay Krishnan and
        Nhan Tran and
        Geoffrey C. Fox},
      title = {MLCommons Science Working Group AI Benchmarks Collection},
      url = {https://mlcommons-science.github.io/benchmark/benchmarks.pdf},
      note = "Online Collection: \url={https://mlcommons-science.github.io/benchmark/}",
      month = jun,
      year = 2025,
      howpublished = "GitHub"
    } 
```

## License

This project is released under the [Apache 2.0 License](LICENSE).

For questions or suggestions, please open an issue or start a discussion on the MLCommons Science Working Group forums.
