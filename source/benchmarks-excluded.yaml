# This file contains benchmarks that are excluded from the main benchmarks.yaml file.
# They may be excluded for various reasons, such as being a platform rather than a specific benchmark,
# or not meeting certain criteria. This file is maintained for record-keeping and potential future reference

# Exclusion Reason: Platform for hosting benchmarks, not a benchmark itself
- date: "2022-01-01"
  version: "v1.0"
  last_updated: "2025-03"
  expired: "unknown"
  valid: "yes"
  valid_date: "2022-01-01"
  name: "Codabench"
  url: "https://www.codabench.org/"
  doi: "https://doi.org/10.1016/j.patter.2022.100543"
  domain:
  - General ML
  - Multiple
  focus: "Open-source platform for organizing reproducible AI benchmarks and competitions"
  keywords:
  - "benchmark platform"
  - "code submission"
  - "competitions"
  - "meta-benchmark"
  summary: |
    Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks
    and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues .
  licensing: "https://github.com/codalab/codalab-competitions/wiki/Privacy"
  task_types:
  - "Multiple"
  ai_capability_measured:
  - "Model reproducibility"
  - "performance across datasets"
  metrics:
  - "Submission count"
  - "Leaderboard ranking"
  - "Task-specific metrics"
  models:
  - "Arbitrary code submissions"
  ml_motif:
  - "Multiple"
  type: "Platform"
  ml_task:
  - "Multiple"
  solutions: "Several"
  notes: |
    Hosts 51 public competitions, ~26 k users, 177 k submissions
  contact:
    name: "Isabelle Guyon (Université Paris-Saclay)"
    email: "unknown"
  cite:
  - |
    @article{xu-2022,
      author    = {Xu, Zhen and Escalera, Sergio and Pavão, Adrien and Richard, Magali and Tu, Wei-Wei and Yao, Quanming and Zhao, Huan and Guyon, Isabelle},
      doi       = {10.1016/j.patter.2022.100543},
      issn      = {2666-3899},
      journal   = {Patterns},
      month     = jul,
      number    = {7},
      pages     = {100543},
      publisher = {Elsevier BV},
      title     = {Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},
      url       = {http://dx.doi.org/10.1016/j.patter.2022.100543},
      volume    = {3},
      year      = {2022}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1sIwNDCs01s790DApVt5leiG8UaBDFVINA4bOixZ1CUw/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    specification:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    dataset:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    metrics:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    reference_solution:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    documentation:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.


# Exclusion Reason: Dashboard & Benchmark for pure inference performance of vLLM Library, not a scientific benchmark
- date: "2022-06-22"
  version: "v1.0"
  last_updated: "2025-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2022-06-22"
  name: "vLLM Performance Dashboard"
  url: "https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "Interactive dashboard showing inference performance of vLLM"
  keywords:
  - "Dashboard"
  - "Throughput visualization"
  - "Latency analysis"
  - "Metric tracking"
  summary: |
    A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and hardware configurations.
  licensing: "unknown"
  task_types:
  - "Performance visualization"
  ai_capability_measured:
  - "Throughput"
  - "latency"
  - "hardware utilization"
  metrics:
  - "Tokens/sec"
  - "TTFT"
  - "Memory usage"
  models:
  - "LLaMA-2"
  - "Mistral"
  - "Qwen"
  ml_motif:
  - "HPC/inference"
  type: "Framework"
  ml_task:
  - "Visualization"
  solutions: "0"
  notes: |
    Built using ObservableHQ; integrates live data from vLLM benchmarks.
    The URL requires a login to access the content.
  contact:
    name: "Simon Mo"
    email: "unknown"
  cite:
  - |
    @misc{mo2024vllm_dashboard,
      title={vLLM Performance Dashboard},
      author={Mo, Simon},
      year={2024},
      url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "(none)"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Interactive dashboard built with ObservableHQ and linked to vLLM benchmarks.
        Source code is not fully open, but backend integration with vLLM is well-maintained.
    specification:
      rating: 4
      reason: |
        While primarily a visualization tool, it includes benchmark configurations,
        metric definitions, and supports comparison across models and hardware.
    dataset:
      rating: 2
      reason: |
        No datasets are bundled; the dashboard visualizes metrics derived from model
        inference logs or external endpoints, not a formal dataset.
    metrics:
      rating: 4
      reason: |
        Tracks tokens/sec, TTFT, memory usage, and platform comparisons. Metrics are clear
        but focused on visualization rather than statistical robustness.
    reference_solution:
      rating: 3
      reason: |
        Dashboards include reproducible views of benchmarked models, but do not ship
        with runnable model code. Relies on external serving infrastructure.
    documentation:
      rating: 4
      reason: |
        Public dashboard with instructions and tooltips; documentation is clear, though access
        is restricted (login required) and backend setup is opaque to users.

# Exclusion Reason: Library for serving LLMs, Benchmark is for library's inference performance, not a scientific benchmark
- date: "2023-09-12"
  version: "v0.10.0"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-09-12"
  name: "vLLM Inference and Serving Engine"
  url: "https://github.com/vllm-project/vllm/tree/main/benchmarks"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "High-throughput, memory-efficient inference and serving engine for LLMs"
  keywords:
  - "LLM inference"
  - "PagedAttention"
  - "CUDA graph"
  - "streaming API"
  - "quantization"
  summary: |
    vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large language models, 
    featuring PagedAttention, continuous batching, and support for quantized and pipelined model execution. 
    Benchmarks compare it to TensorRT-LLM, SGLang, and others.
  licensing: "Apache License 2.0"
  task_types:
  - "Inference Benchmarking"
  ai_capability_measured:
  - "Throughput"
  - "latency"
  - "memory efficiency"
  metrics:
  - "Tokens/sec"
  - "Time to First Token (TTFT)"
  - "Memory footprint"
  models:
  - "LLaMA"
  - "Mixtral"
  - "FlashAttention-based models"
  ml_motif:
  - "HPC/inference"
  type: "Framework"
  ml_task:
  - "Inference"
  solutions: "0"
  notes: |
    Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers
  contact:
    name: "Woosuk Kwon (vLLM Team)"
    email: "unknown"
  cite:
  - |
    @inproceedings{10.1145/3600006.3613165,
      author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
      title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
      year = {2023},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3600006.3613165},
      doi = {10.1145/3600006.3613165},
      abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
      booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
      pages = {611-626},
      numpages = {16},
      location = {Koblenz, Germany},
      series = {SOSP '23}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1nPZyRZEZHciGXYNJShR9FCJbre7ImJLKf2MG6g4-3gQ/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained open-source project under Apache 2.0. GitHub repo includes
        full serving engine, benchmarking scripts, CUDA integration, and deployment examples.
    specification:
      rating: 5
      reason: |
        Inference benchmarks are well-defined with clear input/output formats and platform-specific constraints.
        Covers multiple models, hardware backends, and batching configurations.
    dataset:
      rating: 3
      reason: |
        No traditional dataset is included. Instead, it uses structured configs and logs suitable for inference benchmarking.
        FAIR principles are only partially applicable.
    metrics:
      rating: 5
      reason: |
        Comprehensive performance metrics like tokens/sec, time-to-first-token (TTFT), and memory footprint
        are consistently applied and benchmarked across frameworks.
    reference_solution:
      rating: 4
      reason: |
        Provides runnable scripts and configs for several models (LLaMA, Mixtral, etc.) across platforms.
        Baselines are reproducible, though not all models are fully wrapped or hosted.
    documentation:
      rating: 4
      reason: |
        Well-structured GitHub documentation with setup instructions, config examples, benchmarking comparisons,
        and performance tuning guides.
# Exclusion Reason: Library and framework for serving LLMs and VLMs, not a scientific benchmark
- date: "2023-12-12"
  version: "v0.4.9"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-12-12"
  name: "SGLang Framework"
  url: "https://github.com/sgl-project/sglang/tree/main/benchmark"
  doi: "10.48550/arXiv.2312.07104"
  domain:
  - LLM Vision
  focus: "Fast serving framework for LLMs and vision-language models"
  keywords:
  - "LLM serving"
  - "vision-language"
  - "RadixAttention"
  - "performance"
  - "JSON decoding"
  summary: |
    A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching, quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives.
  licensing: "Apache License 2.0"
  task_types:
  - "Model serving framework"
  ai_capability_measured:
  - "Serving throughput"
  - "JSON/task-specific latency"
  metrics:
  - "Tokens/sec"
  - "Time-to-first-token"
  - "Throughput gain vs baseline"
  models:
  - "LLaVA"
  - "DeepSeek"
  - "Llama"
  ml_motif:
  - "LLM Vision"
  type: "Framework"
  ml_task:
  - "Model serving"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025.
  contact:
    name: "SGLang Team"
    email: "unknown"
  cite:
  - |
    @misc{zheng2024sglangefficientexecutionstructured,
      archiveprefix = {arXiv},
      author        = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      eprint        = {2312.07104},
      primaryclass  = {cs.AI},
      title         = {SGLang: Efficient Execution of Structured Language Model Programs},
      url           = {https://arxiv.org/abs/2312.07104},
      year          = {2024}
    }
  datasets:
    links:
    - name: "Benchmark configs"
      url: ""
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained and production-deployed (e.g., xAI, NVIDIA); source code available under
        Apache 2.0. Includes efficient backends (RadixAttention, quantization, batching) and full
        serving infrastructure.
    specification:
      rating: 4
      reason: |
        The framework clearly defines performance targets, serving logic, and model integration.
        Input/output expectations are consistent, but not all benchmarks are standardized.
    dataset:
      rating: 2
      reason: |
        Does not introduce new datasets; instead, it evaluates performance using existing model benchmarks.
        Only configuration files are included.
    metrics:
      rating: 5
      reason: |
        Serving-related metrics such as tokens/sec, time-to-first-token, and throughput gain vs. baselines
        are well-defined and consistently applied.
    reference_solution:
      rating: 3
      reason: |
        Provides benchmark configs and example integrations (e.g., with LLaVA, DeepSeek), but not all
        models or scripts are runnable out-of-the-box.
    documentation:
      rating: 4
      reason: |
        Strong GitHub documentation, install guides, and benchmarks. Some advanced topics (e.g.,
        scaling, hardware tuning) could use deeper walkthroughs.

# Exclusion Reason: Not a scientific benchmark specifically
- date: "2020-01-01"
  version: "v1.0"
  last_updated: "2020-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2020-01-01"
  name: "BenchCouncil AIBench"
  url: "https://www.benchcouncil.org/AIBench/"
  doi: "10.48550/arXiv.1908.08998"
  domain:
  - General
  focus: "End-to-end AI benchmarking across micro, component, and application levels"
  keywords:
  - "benchmarking"
  - "AI systems"
  - "application-level evaluation"
  summary: |
    AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component, application) across hardware systems-covering image generation, object detection, translation, recommendation, video prediction, etc.
  licensing: "Apache License 2.0"
  task_types:
  - "Training"
  - "Inference"
  - "End-to-end AI workloads"
  ai_capability_measured:
  - "System-level AI workload performance"
  metrics:
  - "Throughput"
  - "Latency"
  - "Accuracy"
  models:
  - "ResNet"
  - "BERT"
  - "GANs"
  - "Recommendation systems"
  ml_motif:
  - "General"
  type: "Benchmark"
  ml_task:
  - "NA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Covers scenario-distilling, micro, component, and end-to-end benchmarks.
  contact:
    name: "Wanling Gao (BenchCouncil)"
    email: "unknown"
  cite:
  - |
    @misc{gao2019aibenchindustrystandardinternet,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Fei Tang and Lei Wang and Jianfeng Zhan and Chunxin Lan and Chunjie Luo and Yunyou Huang and Chen Zheng and Jiahui Dai and Zheng Cao and Daoyi Zheng and Haoning Tang and Kunlin Zhan and Biao Wang and Defei Kong and Tong Wu and Minghe Yu and Chongkang Tan and Huan Li and Xinhui Tian and Yatao Li and Junchao Shao and Zhenyu Wang and Xiaoyu Wang and Hainan Ye},
      eprint        = {1908.08998},
      primaryclass  = {cs.CV},
      title         = {AIBench: An Industry Standard Internet Service AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1908.08998},
      year          = {2019}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1scxhARd4vzEaWpVfwKPF_nTSxv4DirlQqcGlSG0yzJc/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        No containerized or automated implementation provided for full benchmark suite
    specification:
      rating: 4
      reason: |
        Task coverage is broad and well-scoped, but system constraints and expected outputs are not uniformly defined
    dataset:
      rating: 3
      reason: |
        Multiple datasets are mentioned, but not consistently FAIR-documented, versioned, or linked
    metrics:
      rating: 4
      reason: |
        Metrics are appropriate, but standardization and reproducibility across tasks vary
    reference_solution:
      rating: 3
      reason: |
        Reference models (e.g., ResNet, BERT) described; no turnkey implementation or results repository for all levels
    documentation:
      rating: 3
      reason: |
        Paper is comprehensive, but minimal user-facing documentation or structured reproduction guide
# Exclusion Reason: Not a scientific benchmark specifically
- date: "2020-01-01"
  version: "v1.0"
  last_updated: "2020-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2020-01-01"
  name: "BenchCouncil BigDataBench"
  url: "https://www.benchcouncil.org/BigDataBench/"
  doi: "10.48550/arXiv.1802.08254"
  domain:
  - General
  focus: "Big data and AI benchmarking across structured, semi-structured, and unstructured
    data workloads"
  keywords:
  - "big data"
  - "AI benchmarking"
  - "data analytics"
  summary: |
    BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI.
  licensing: "Apache License 2.0"
  task_types:
  - "Data preprocessing"
  - "Inference"
  - "End-to-end data pipelines"
  ai_capability_measured:
  - "Data processing and AI model inference performance at scale"
  metrics:
  - "Data throughput"
  - "Latency"
  - "Accuracy"
  models:
  - "CNN"
  - "LSTM"
  - "SVM"
  - "XGBoost"
  ml_motif:
  - "General"
  type: "Benchmark"
  ml_task:
  - "NA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
  contact:
    name: "Jianfeng Zhan (BenchCouncil)"
    email: "unknown"
  cite:
  - |
    @misc{gao2018bigdatabenchscalableunifiedbig,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Jianfeng Zhan and Lei Wang and Chunjie Luo and Daoyi Zheng and Xu Wen and Rui Ren and Chen Zheng and Xiwen He and Hainan Ye and Haoning Tang and Zheng Cao and Shujie Zhang and Jiahui Dai},
      eprint        = {1802.08254},
      primaryclass  = {cs.DC},
      title         = {BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1802.08254},
      year          = {2018}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1FlvWeGm_J5QabOL7J0RWN3udzl0QFDs7wafptXx8sRU/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: "https://docs.google.com/document/d/1VFRxhR2G5A83S8PqKBrP99LLVgcCGvX2WW4vTtwxmQ4/edit?usp=sharing"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        No automated setup across all tasks; some components require manual integration.
    specification:
      rating: 4
      reason: |
        Specific I/O formats and hardware constraints are not uniformly detailed across all tasks.
    dataset:
      rating: 4
      reason: |
        Some datasets lack consistent versioning or rich metadata annotations.
    metrics:
      rating: 5
      reason: |
        None
    reference_solution:
      rating: 4
      reason: |
        Not all benchmark components have fully reproducible baselines; deployment across platforms is fragmented.
    documentation:
      rating: 4
      reason: |
        Setup requires manual steps; some task-specific instructions lack clarity.
# Exclusion Reason: Platform for hosting code for scientific papers, not a benchmark itself
- date: "ongoing"
  version: "v1.0"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "ongoing"
  name: "Papers With Code (SOTA Platform)"
  url: "https://paperswithcode.com/sota"
  doi: "unknown"
  domain:
  - General ML
  - All domains
  focus: "Open platform tracking state-of-the-art results, benchmarks, and implementations
    across ML tasks and papers"
  keywords:
  - "leaderboard"
  - "benchmarking"
  - "reproducibility"
  - "open-source"
  summary: |
    Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research:
    12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.
  licensing: "Apache License 2.0"
  task_types:
  - "Multiple (Classification, Detection, NLP, etc.)"
  ai_capability_measured:
  - "Model performance across tasks (accuracy"
  - "F1"
  - "BLEU"
  - "etc.)"
  metrics:
  - "Task-specific (Accuracy, F1, BLEU, etc.)"
  models:
  - "All published models with code"
  ml_motif:
  - "Multiple"
  type: "Platform"
  ml_task:
  - "Multiple"
  solutions: "0"
  notes: |
    Community-driven open platform; automatic data extraction and versioning.
  contact:
    name: "Papers With Code Team"
    email: "unknown"
  cite:
  - |
    @InProceedings{pmlr-v37-blum15,
      title =    {The Ladder: A Reliable Leaderboard for Machine Learning Competitions},
      author =   {Blum, Avrim and Hardt, Moritz},
      booktitle =        {Proceedings of the 32nd International Conference on Machine Learning},
      pages =    {1006--1014},
      year =     {2015},
      editor =   {Bach, Francis and Blei, David},
      volume =   {37},
      series =   {Proceedings of Machine Learning Research},
      address =          {Lille, France},
      month =    jul,
      publisher =    {PMLR},
      pdf =      {http://proceedings.mlr.press/v37/blum15.pdf},
      url =      {https://proceedings.mlr.press/v37/blum15.html},
      abstract =         {The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1pbn_K20t6Kr0zxdUSAL68ChruOu6B_x5sZbp-dcrG_g/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained open-source platform (https://paperswithcode.com) under Apache 2.0 license;
        includes automatic integration with GitHub, datasets, and models for reproducibility.
    specification:
      rating: 4
      reason: |
        Task and benchmark structures are well organized and standardized, but due to its broad coverage,
        input/output formats vary significantly between tasks and are not always tightly controlled.
    dataset:
      rating: 3
      reason: |
        Relies on external datasets submitted by the community. While links are available, FAIR compliance
        is not guaranteed or systematically enforced across all benchmarks.
    metrics:
      rating: 5
      reason: |
        Tracks state-of-the-art using task-specific metrics like Accuracy, F1, BLEU, etc., with consistent
        aggregation and historical SOTA tracking.
    reference_solution:
      rating: 3
      reason: |
        Provides links to implementations of many SOTA models, but no single unified reference baseline
        is required or maintained per benchmark.
    documentation:
      rating: 4
      reason: |
        Strong front-end documentation and metadata on benchmarks, tasks, and models; however, some benchmark-specific
        instructions are sparse or dependent on external paper links.
# Exclusion Reason: LLM Performance evaluation suite, not a scientific benchmark specifically
- date: "2024-10-31"
  version: "v1.0"
  last_updated: "2024-11"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-10-31"
  name: "LLM-Inference-Bench"
  url: "https://github.com/argonne-lcf/LLM-Inference-Bench"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "Hardware performance benchmarking of LLMs on AI accelerators"
  keywords:
  - "LLM"
  - "inference benchmarking"
  - "GPU"
  - "accelerator"
  - "throughput"
  summary: |
    A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics.
  licensing: "BSD 3-Clause New or Revised License"
  task_types:
  - "Inference Benchmarking"
  ai_capability_measured:
  - "Inference throughput"
  - "latency"
  - "hardware utilization"
  metrics:
  - "Token throughput (tok/s)"
  - "Latency"
  - "Framework-hardware mix performance"
  models:
  - "LLaMA-2-7B"
  - "LLaMA-2-70B"
  - "Mistral-7B"
  - "Qwen-7B"
  ml_motif:
  - "HPC/inference"
  type: "Dataset"
  ml_task:
  - "Inference Benchmarking"
  solutions: "0"
  notes: |
    Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators.
  contact:
    name: "Krishna Teja Chitty-Venkata (Argonne LCF)"
    email: "unknown"
  cite:
  - |
    @INPROCEEDINGS{10820566,
      author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},
      booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
      title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, 
      year={2024},
      volume={},
      number={},
      pages={1362-1379},
      doi={10.1109/SCW63240.2024.00178}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1I3UvByGn4KaruQC1pi6XcfoAOzt4iiA61S0nR9ovC94/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Public GitHub repository under BSD-3 license.
        Includes scripts, configurations, and dashboards for running and visualizing LLM inference benchmarks
        across multiple accelerator platforms.
    specification:
      rating: 5
      reason: |
        Benchmark scope, models, accelerator targets, and supported frameworks are clearly specified.
        Input configurations and output metrics are standardized across hardware types.
    dataset:
      rating: 2
      reason: |
        No novel dataset is introduced; benchmark relies on pre-trained LLMs and synthetic inference inputs.
        Dataset structure and FAIR considerations are minimal.
    metrics:
      rating: 5
      reason: |
        Hardware-specific metrics (token throughput, latency, utilization) are well-defined, consistently measured,
        and aggregated in dashboards.
    reference_solution:
      rating: 3
      reason: |
        Inference configurations and baseline performance results are provided, but there are no
        full reference training pipelines or model implementations.
    documentation:
      rating: 4
      reason: |
        GitHub repo provides clear usage instructions, setup guides, and interactive dashboard tooling.
        Some areas like benchmarking extensions or advanced tuning are less detailed.

# Exclusion Reason: Not specifically a scientific benchmark, focuses on common sense questions
- date: "2019-11-20"
  version: "1"
  last_updated: "2019-11-20"
  expired: "false"
  valid: "yes"
  valid_date: "2019-11-20"
  name: "CommonSenseQA"
  url: "https://huggingface.co/datasets/tau/commonsense_qa"
  doi: "10.48550/arXiv.1811.00937"
  domain:
  - Computational Science & AI
  focus: "Commonsense question answering"
  keywords:
  - "ConceptNet"
  - "multiple-choice"
  - "adversarial"
  summary: |
    CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
    requiring models to apply commonsense knowledge to select the correct answer 
    among five choices.
  licensing: "MIT"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Commonsense reasoning and knowledge integration"
  metrics:
  - "Accuracy"
  models:
  - "BERT-large"
  - "RoBERTa"
  - "GPT-3"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "2"
  notes: "Baseline 56%, human 89%"
  contact:
    name: "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant"
    email: "Unknown"
  cite:
  - |
    @misc{talmor2019commonsenseqaquestionansweringchallenge,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00937}, 
    }
  datasets:
    links:
    - name: "CommonsenseQA Dataset (Hugging Face)"
      url: "https://huggingface.co/datasets/commonsense_qa"
  results:
    links:
    - name: "Papers With Code Leaderboard for CommonsenseQA"
      url: "https://paperswithcode.com/dataset/commonsenseqa"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        All code given on Github site
    specification:
      rating: 4
      reason: |
        Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.
    metrics:
      rating: 5
      reason: |
        Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
    reference_solution:
      rating: 4
      reason: |
        Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not run with hardware constraints
    documentation:
      rating: 5
      reason: |
        Given in paper.

# Exclusion Reason: Not specifically a scientific benchmark, focuses on fill in the blank problems
- date: "2019-07-24"
  version: "1"
  last_updated: "2019-07-24"
  expired: "false"
  valid: "yes"
  valid_date: "2019-07-24"
  name: "Winogrande"
  url: "https://leaderboard.allenai.org/winogrande/submissions/public"
  doi: "10.48550/arXiv.1907.10641"
  domain:
  - Computational Science & AI
  focus: "Winograd Schema-style pronoun resolution"
  keywords:
  - "adversarial"
  - "pronoun resolution"
  summary: |
    WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
    questions with reduced bias using AFLite, serving as both a benchmark and transfer 
    learning resource.
  licensing: "CC-BY"
  task_types:
  - "Pronoun resolution"
  ai_capability_measured:
  - "Robust commonsense reasoning"
  metrics:
  - "Accuracy"
  - "AUC"
  models:
  - "RoBERTa"
  - "BERT"
  - "GPT-2"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "2"
  notes: "Human ~94%"
  contact:
    name: "Keisuke Sakaguchi"
    email: "keisukes@allenai.org"
  cite:
  - |
    @misc{sakaguchi2019winograndeadversarialwinogradschema,
      archiveprefix = {arXiv},
      author        = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      eprint        = {1907.10641},
      primaryclass  = {cs.CL},
      title         = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
      url           = {https://arxiv.org/abs/1907.10641},
      year          = {2019}
    }
  datasets:
    links:
    - name: "Hugging Face / AllenAI"
      url: "https://huggingface.co/datasets/allenai/winogrande"
  results:
    links:
    - name: "Papers With Code leaderboard"
      url: "https://paperswithcode.com/dataset/winogrande"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        No template code provided
    specification:
      rating: 5
      reason: |
        Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata.
    metrics:
      rating: 5
      reason: |
        Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations.
    reference_solution:
      rating: 4
      reason: |
        Baseline results available, requiring users to submit their methods along with their submissions. Constraints are not required in submissions.
    documentation:
      rating: 5
      reason: |
        Dataset page and paper provide sufficient detail

# Exclusion Reason: Not specifically a scientific benchmark, some scientific tasks present, but very minimal compared to other tasks.
- date: "2022-06-09"
  version: "1"
  last_updated: "2022-06-09"
  expired: "false"
  valid: "yes"
  valid_date: "2022-06-09"
  name: "BIG-Bench (Beyond the Imitation Game Benchmark)"
  url: "https://github.com/google/BIG-bench"
  doi: "10.48550/arXiv.2206.04615"
  domain:
  - Computational Science & AI
  focus: "Diverse reasoning and generalization tasks"
  keywords:
  - "few-shot"
  - "multi-task"
  - "bias analysis"
  summary: |
    BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning, 
    knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
  licensing: "Apache-2.0"
  task_types:
  - "Few-shot evaluation"
  - "Multi-task evaluation"
  ai_capability_measured:
  - "Reasoning and generalization across diverse tasks"
  metrics:
  - "Accuracy"
  - "Task-specific metrics"
  models:
  - "GPT-3"
  - "Dense Transformers"
  - "Sparse Transformers"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Multiple, including human baselines"
  notes: "Human baselines included"
  contact:
    name: "Aarohi Srivastava et al."
    email: "bigbench@googlegroups.com"
  cite:
  - |
    @misc{srivastava2023imitationgamequantifyingextrapolating,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri\`{a} Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlm\"{u}ller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka\c{s} and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart\lomiej Bojanowski and Batuhan \"{O}zyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C\'{e}sar Ferri Ram\'{i}rez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu\'{i} Gonz\'{a}lez and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart\'{i}nez-Plumed and Francesca Happ\'{e} and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ\'{a}n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L\'{o}pez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch\"{u}tze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern\'{a}ndez Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco\'{n} and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and J\"{o}rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col\'{o}n and Luke Metz and L\"{u}tfi Kerem \c{S}enel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram\'{i}rez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M\'{a}ty\'{a}s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha\l Sw\k{e}drowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi\lkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Rapha\"{e}l Milli\`{e}re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Th\'{e}o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.04615}, 
    }
  datasets:
    links:
    - name: "BIG-Bench GitHub Repository (contains tasks and data)"
      url: "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks"
  results:
    links:
    - name: "BIG-Bench GitHub Repository (results in papers and code)"
      url: "https://github.com/google/BIG-bench"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4.5
      reason: |
        Quick start notebook provided, but instructions on how to run it are lacking.
    specification:
      rating: 4.5
      reason: |
        Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and well-documented; FAIR overall
    metrics:
      rating: 5
      reason: |
        Many tasks use standard quantitative metrics (accuracy, BLEU, F1). Others involve subjective ratings (e.g., Likert), which reduces cross-task comparability.
    reference_solution:
      rating: 2
      reason: |
        Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey.
    documentation:
      rating: 5
      reason: |
        Explained in the associated paper.
# Exclusion Reason: Too general of a benchmark, despite including some scientific questions
- date: "2025-01-24"
  version: "1"
  last_updated: "2025-01-24"
  expired: "false"
  valid: "yes"
  valid_date: "2025-01-24"
  name: "Humanity's Last Exam"
  url: "https://arxiv.org/abs/2501.14249"
  doi: "10.48550/arXiv.2501.14249"
  domain:
  - Computational Science & AI
  focus: "Broad cross-domain academic reasoning"
  keywords:
  - "cross-domain"
  - "academic exam"
  - "multiple-choice"
  - "multidisciplinary"
  summary: |
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  licensing: "MIT License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Cross-domain academic reasoning"
  metrics:
  - "Accuracy"
  models:
  - "unkown"
  ml_motif:
  - Reasoning & Generalization
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "HLE team"
    email: "agibenchmark@safe.ai"
  cite:
  - |
    @misc{phan2025humanitysexam,
      archiveprefix = {arXiv},
      author        = {Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and S\oren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and G\"{o}zdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Br\"{u}ssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Mart\'{i} Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and V\'{a}clav Rozho\v{n} and Vincent Ginis and Christian Stump and Niv Cohen and Rafa\l Po\'{s}wiata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givr\'{e} and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar \"{A}ngquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and J\'{e}r\'{e}my Andr\'{e}oletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Kh\'{a}nh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Bir\'{o} B\'{a}lint and Eve J. Y. Lo and Jiaqi Wang and Maria In\^{e}s S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciob\^{a}c\u{a} and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekstr\"{o}m and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Pe\~{n}aflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yi\u{g}it Yalin and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Bosc\'{a} and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle H\"{a}ggstr\"{o}m and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hern\'{a}ndez-C\'{a}mara and Emanuele Rodol\`{a} and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro Jos\'{e} Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Ra\'{u}l Adri\'{a}n Huerta Rodr\'{i}guez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benj\'{a}min Borb\'{a}s and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran \DJuc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub \Lucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels M\"{u}ndler and S\"{o}ren M\"{o}ller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and M\'{a}ty\'{a}s Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubi\'{c} and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vil\'{e}m Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Micka\"{e}l Noy\'{e} and Micha\l Pere\lkiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and D\'{a}niel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Gin\'{e}s and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han L\`{u} and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Bria\'{n}ski and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanovi\'{c} and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Ga\"{e}l Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and K\'{a}roly Zsolnai-Feh\'{e}r and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gon\c{c}alves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Y\"{u}cel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks},
      eprint        = {2501.14249},
      primaryclass  = {cs.LG},
      title         = {Humanity's Last Exam},
      url           = {https://arxiv.org/abs/2501.14249},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/cais/hle"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Code for testing models posted on the github. Unknown how to run a custom model.
    specification:
      rating: 2
      reason: |
        Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified
    dataset:
      rating: 2
      reason: |
        Data accessible through Hugging Face, but requires giving contact information to access
    metrics:
      rating: 5
      reason: |
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 2
      reason: |
        Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result
    documentation:
      rating: 5
      reason: |
        Paper available with necessary information

# Exclusion Reason: Benchmark Leaderboard/collection of material science benchmarks, not a single benchmark
- date: "2023-06-20"
  version: "1"
  last_updated: "2023-06-20"
  expired: "false"
  valid: "yes"
  valid_date: "2023-06-20"
  name: "JARVIS-Leaderboard"
  url: "https://arxiv.org/abs/2306.11688"
  doi: "10.48550/arXiv.2306.11688"
  domain:
  - Materials Science
  focus: "Comparative evaluation of materials design methods"
  keywords:
  - "leaderboards"
  - "materials methods"
  - "simulation"
  summary: |
    JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
    structure, force-fields, quantum computing, and experimental methods across hundreds
    of materials science tasks.
  licensing: "NIST"
  task_types:
  - "Method benchmarking"
  - "Leaderboard ranking"
  ai_capability_measured:
  - "Performance comparison across diverse materials design methods"
  metrics:
  - "MAE"
  - "RMSE"
  - "Accuracy"
  models:
  - "unknown"
  ml_motif:
  - Regression
  - Classification
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "1281 contributions across 274 benchmarks"
  contact:
    name: "Kamal Choudhary"
    email: "kamal.choudhary@nist.gov"
  cite:
  - |
    @article{choudhary2024jarvis,
      title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},
    author = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},
      journal = {npj Computational Materials},
      volume = {10},
      number = {1},
      pages = {93},
      year = {2024},
      doi = {10.1038/s41524-024-01259-w},
      url = {https://doi.org/10.1038/s41524-024-01259-w}
    }
  datasets:
    links:
    - name: "AI model specific benchmarks"
      url: "https://pages.nist.gov/jarvis_leaderboard/AI/"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 2
      reason: |
        Setup script provided, but no code provided
    specification:
      rating: 4
      reason: |
        Tasks, inputs, and outputs are well specified. No system constraints mentioned.
    dataset:
      rating: 4
      reason: |
        Data is public and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks. No splits.
    metrics:
      rating: 5
      reason: |
        Metrics stated for each benchmark.
    reference_solution:
      rating: 4
      reason: |
        Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); no constraints specified.
    documentation:
      rating: 5
      reason: |
        All information available in paper and website
# Exclusion Reason: Not a benchmark, baseline/foundation models for time series/neural forcasting
- date: "2023-10-05"
  version: "v3.0.2"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-10-05"
  name: "Nixtla Neural Forecast TimeGPT - Forecasting"
  url: "https://github.com/Nixtla/neuralforecast"
  doi: "10.48550/arXiv.2310.03589"
  domain:
  - Computational Science & AI
  focus: "Time-series foundation model \"TimeGPT\" for forecasting and anomaly detection"
  keywords:
  - "TimeGPT"
  - "foundation model"
  - "time-series"
  - "generative model"
  summary: |
    TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for
    zero-shot forecasting and anomaly detection via API .
  licensing: "Apache License 2.0"
  task_types:
  - "Time-series forecasting"
  - "Anomaly Detection"
  ai_capability_measured:
  - "Zero-shot forecasting"
  - "anomaly detection"
  metrics:
  - "RMSE"
  - "Anomaly Detection metrics"
  models:
  - "TimeGPT"
  ml_motif:
  - Anomaly Detection
  - Sequence Prediction/Forecasting
  type: "Platform"
  ml_task:
  - "Forecasting"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Offered via Nixtla API and Azure Studio; enterprise-grade support available.
  contact:
    name: "Azul Garza (Nixtla)"
    email: "unknown"
  cite:
  - |
    @misc{garza2024timegpt1,
      archiveprefix = {arXiv},
      author        = {Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},
      eprint        = {2310.03589},
      primaryclass  = {cs.LG},
      title         = {TimeGPT-1},
      url           = {https://arxiv.org/abs/2310.03589},
      year          = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1KmKs9JtcfpKe40fNuLrgdFcOmBnfj3ZG7AwcfTH4tXE/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Fully open-source Apache 2.0 implementation integrated in NeuralForecast,
        supporting training and evaluation via API. Production-grade deployment available via Nixtla API and Azure.
    specification:
      rating: 3
      reason: |
        Concept and forecasting goals are described, but formal input/output definitions
        and task constraints are not rigorously specified.
    dataset:
      rating: 3
      reason: |
        Evaluated on existing open datasets, but consolidated data release, splits, and FAIR
        metadata are not provided.
    metrics:
      rating: 4
      reason: |
        Uses standard forecasting metrics such as RMSE, MASE, SMAPE, and anomaly detection
        metrics consistently across evaluations.
    reference_solution:
      rating: 3
      reason: |
        TimeGPT implementation is available, but baseline comparisons and additional reference
        models are limited.
    documentation:
      rating: 3
      reason: |
        Basic README with installation and usage examples; more detailed API docs and tutorials
        would improve usability.

# Exclusion Reason: Not a benchmark, baseline/foundation models for time series/neural forcasting
- date: "2023-10-05"
  version: "v3.0.2"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-10-05"
  name: "Nixtla Neural Forecast TimeGPT - Anomaly Detection"
  url: "https://github.com/Nixtla/neuralforecast"
  doi: "10.48550/arXiv.2310.03589"
  domain:
  - Computational Science & AI
  focus: "Time-series foundation model \"TimeGPT\" for forecasting and anomaly detection"
  keywords:
  - "TimeGPT"
  - "foundation model"
  - "time-series"
  - "generative model"
  summary: |
    TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for
    zero-shot forecasting and anomaly detection via API .
  licensing: "Apache License 2.0"
  task_types:
  - "Time-series forecasting"
  - "Anomaly Detection"
  ai_capability_measured:
  - "Zero-shot forecasting"
  - "anomaly detection"
  metrics:
  - "RMSE"
  - "Anomaly Detection metrics"
  models:
  - "TimeGPT"
  ml_motif:
  - Anomaly Detection
  type: "Platform"
  ml_task:
  - "Forecasting"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Offered via Nixtla API and Azure Studio; enterprise-grade support available.
  contact:
    name: "Azul Garza (Nixtla)"
    email: "unknown"
  cite:
  - |
    @misc{garza2024timegpt1,
      archiveprefix = {arXiv},
      author        = {Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},
      eprint        = {2310.03589},
      primaryclass  = {cs.LG},
      title         = {TimeGPT-1},
      url           = {https://arxiv.org/abs/2310.03589},
      year          = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1KmKs9JtcfpKe40fNuLrgdFcOmBnfj3ZG7AwcfTH4tXE/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Fully open-source Apache 2.0 implementation integrated in NeuralForecast,
        supporting training and evaluation via API. Production-grade deployment available via Nixtla API and Azure.
    specification:
      rating: 3
      reason: |
        Concept and forecasting goals are described, but formal input/output definitions
        and task constraints are not rigorously specified.
    dataset:
      rating: 3
      reason: |
        Evaluated on existing open datasets, but consolidated data release, splits, and FAIR
        metadata are not provided.
    metrics:
      rating: 4
      reason: |
        Uses standard forecasting metrics such as RMSE, MASE, SMAPE, and anomaly detection
        metrics consistently across evaluations.
    reference_solution:
      rating: 3
      reason: |
        TimeGPT implementation is available, but baseline comparisons and additional reference
        models are limited.
    documentation:
      rating: 3
      reason: |
        Basic README with installation and usage examples; more detailed API docs and tutorials
        would improve usability.

# Exclusion Reason: Not a benchmark, baseline/foundation models for time series/neural forcasting
- date: "2023-06-01"
  version: "v3.0.2"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-06-01"
  name: "Nixtla Neural Forecast NHITS"
  url: "https://github.com/Nixtla/neuralforecast"
  doi: "unknown"
  domain:
  - Computational Science & AI
  focus: "Official NHITS implementation for long-horizon time series forecasting"
  keywords:
  - "NHITS"
  - "long-horizon forecasting"
  - "neural interpolation"
  - "time-series"
  summary: |
    NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art model that
    improved accuracy by ~25% and reduced compute by 50x compared to Transformer baselines,
    using hierarchical interpolation and multi-rate sampling .
  licensing: "Apache License 2.0"
  task_types:
  - "Time-series forecasting"
  ai_capability_measured:
  - "Accuracy"
  - "compute efficiency for long series"
  metrics:
  - "RMSE"
  - "MAPE"
  models:
  - "NHITS"
  ml_motif:
  - Sequence Prediction/Forecasting
  type: "Platform"
  ml_task:
  - "Forecasting"
  solutions: "0"
  notes: |
    Official implementation in NeuralForecast, included since its AAAI 2023 release.
  contact:
    name: "Kin G. Olivares (Nixtla)"
    email: "unknown"
  cite:
  - |
    @inproceedings{challu2023nhits,
     title={Nhits: Neural hierarchical interpolation for time series forecasting},
     author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},
     booktitle={Proceedings of the AAAI conference on artificial intelligence},
     volume={37},
     number={6},
     pages={6989--6997},
     year={2023}
     }
  datasets:
    links:
    - name: "Standard forecast datasets, M4"
      url: ""
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/15Hm5ekGu99aQWsdtiUIwX6JMoaoFpRbIhDylrWqSoHY/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Implemented within the open-source NeuralForecast library under Apache 2.0.
        Includes training, evaluation, and hyperparameter tuning pipelines. Actively maintained.
    specification:
      rating: 5
      reason: |
        The NHITS forecasting task is clearly defined with structured input/output formats.
        Model design targets long-horizon accuracy and compute efficiency.
    dataset:
      rating: 3
      reason: |
        Uses standard benchmark datasets like M4, but does not bundle them directly.
        FAIR compliance depends on external dataset sources and user setup.
    metrics:
      rating: 5
      reason: |
        Evaluated using RMSE, MAPE, and other standard forecasting metrics, integrated
        into training and evaluation APIs.
    reference_solution:
      rating: 4
      reason: |
        Official NHITS implementation is fully reproducible with training/eval configs,
        though pretrained weights are not always provided.
    documentation:
      rating: 4
      reason: |
        Well-documented on GitHub and in AAAI paper, with code examples, training guidance,
        and usage tutorials. More model-specific docs could improve clarity further.


# Exclusion Reason: Not a benchmark, baseline/foundation models for time series/neural forcasting
- date: "2023-10-03"
  version: "v3.0.2"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-10-03"
  name: "Nixtla Neural Forecast TimeLLM"
  url: "https://github.com/Nixtla/neuralforecast"
  doi: "10.48550/arXiv.2310.01728"
  domain:
  - Computational Science & AI
  focus: "Reprogramming LLMs for time series forecasting"
  keywords:
  - "Time-LLM"
  - "language model"
  - "time-series"
  - "reprogramming"
  summary: |
    Time-LLM uses reprogramming layers to adapt frozen LLMs for time series forecasting, treating
    forecasting as a language task .
  licensing: "Apache License 2.0"
  task_types:
  - "Time-series forecasting"
  ai_capability_measured:
  - "Model reuse via LLM"
  - "few-shot forecasting"
  metrics:
  - "RMSE"
  - "MAPE"
  models:
  - "Time-LLM"
  ml_motif:
  - Sequence Prediction/Forecasting
  type: "Platform"
  ml_task:
  - "Forecasting"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Fully open-source; transforms forecasting using LLM text reconstruction.
  contact:
    name: "Ming Jin (Nixtla)"
    email: "unknown"
  cite:
  - |
    @misc{jin2024timellmtimeseriesforecasting,
      title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models}, 
      author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},
      year={2024},
      eprint={2310.01728},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.01728}, 
    }
  datasets:
    links:
    - name: "Standard forecast datasets, M4"
      url: ""
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1xXGzRt-qhUFTvnBGQi2IbcoBdYyo-ZrAn3IOkswd3fw/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Fully open-source under Apache 2.0, integrated into the NeuralForecast library.
        Includes Time-LLM implementation with example usage and training scripts.
    specification:
      rating: 3
      reason: |
        High-level framing of forecasting as language modeling is clear, but detailed input/output
        specifications, constraints, and task formalization are minimal.
    dataset:
      rating: 3
      reason: |
        Evaluated on standard datasets like M4 and ETT, but dataset splits and versioning are not
        bundled or explicitly FAIR-compliant.
    metrics:
      rating: 4
      reason: |
        Standard forecasting metrics such as RMSE, MAPE, and SMAPE are reported.
        Evaluation is consistent, though deeper metric justification is limited.
    reference_solution:
      rating: 3
      reason: |
        Time-LLM implementation is open and reproducible, but limited baselines or comparative
        implementations are included directly.
    documentation:
      rating: 3
      reason: |
        GitHub README provides installation and quick usage examples, but lacks detailed API docs,
        training walkthroughs, or extended tutorials.

# Exclusion Reason: Not a benchmark, baseline/foundation models for time series/neural forcasting
- date: "2022-04-01"
  version: "v3.0.2"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2022-04-01"
  name: "Nixtla NeuralForecast"
  url: "https://github.com/Nixtla/neuralforecast"
  doi: "unknown"
  domain:
  - Computational Science & AI
  focus: "High-performance neural forecasting library with >30 models"
  keywords:
  - "time-series"
  - "neural forecasting"
  - "NBEATS, NHITS, TFT"
  - "probabilistic forecasting"
  - "usability"
  summary: |
    NeuralForecast offers scalable, user-friendly implementations of over 30 neural forecasting models (NBEATS, NHITS, TFT, DeepAR, etc.),
    emphasizing quality, usability, interpretability, and performance.
  licensing: "Apache License 2.0"
  task_types:
  - "Time-series forecasting"
  ai_capability_measured:
  - "Forecast accuracy"
  - "interpretability"
  - "speed"
  metrics:
  - "RMSE"
  - "MAPE"
  - "CRPS"
  models:
  - "NBEATS"
  - "NHITS"
  - "TFT"
  - "DeepAR"
  ml_motif:
  - Sequence Prediction/Forecasting
  type: "Platform"
  ml_task:
  - "Forecasting"
  solutions: "0"
  notes: |
    AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. First official NHITS implementation. contentReference oaicite:4 ndex=4
  contact:
    name: "Kin G. Olivares (Nixtla)"
    email: "unknown"
  cite:
  - |
    @misc{olivares2022library_neuralforecast,
      author={Kin G. Olivares and Cristian Chall\'{u} and Federico Garza and Max Mergenthaler Canseco and Artur Dubrawski},
      title = {NeuralForecast: User friendly state-of-the-art neural forecasting models.},
      year={2022},
      howpublished={PyCon Salt Lake City, Utah, US 2022},
      url={https://github.com/Nixtla/neuralforecast}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1VzhaUubIm-SHK7cfKWyoi8GtykpCuOH2qPM-k_g8bKU/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained open-source library under Apache 2.0. Offers a clean API,
        extensive model zoo (>30 models), integration with Ray, Optuna, and supports
        scalable training and inference workflows.
    specification:
      rating: 5
      reason: |
        Forecasting task is well-defined with clear input/output structures. Framework supports
        probabilistic and deterministic forecasting, with unified interfaces and support for batch evaluation.
    dataset:
      rating: 3
      reason: |
        NeuralForecast does not include its own datasets but supports standard datasets (e.g., M4, M5, ETT).
        FAIR compliance depends on user-supplied data.
    metrics:
      rating: 5
      reason: |
        RMSE, MAPE, CRPS, and other domain-relevant metrics are well supported and integrated into the evaluation loop.
    reference_solution:
      rating: 4
      reason: |
        Includes runnable model baselines and training scripts for all supported models.
        Some models have pretrained weights, but not all are fully benchmarked out-of-the-box.
    documentation:
      rating: 5
      reason: |
        Rich documentation with examples, API references, tutorials, notebooks, and CLI support.
        PyPI, GitHub, and official blog posts offer clear guidance for usage and extension.
# Exclusion Reason: Not a benchmark, but a framework that uses external benchmarks/models - should use those as seperate entries instead
- date: "2021-09-27"
  version: "v1.0"
  last_updated: "2023-07"
  expired: "unknown"
  valid: "yes"
  valid_date: "2021-09-27"
  name: "Sabath (SBI-FAIR)"
  url: "https://sbi-fair.github.io/docs/software/sabath/"
  doi: "unknown"
  domain:
  - Climate & Earth Science
  - High Energy Physics
  - Materials Science
  focus: "FAIR metadata framework for ML-driven surrogate workflows in HPC systems"
  keywords:
  - "meta-benchmark"
  - "metadata"
  - "HPC"
  - "surrogate modeling"
  summary: |
    Sabath is a metadata framework from the SBI-FAIR group (UTK, Argonne, Virginia) facilitating
    FAIR-compliant benchmarking and surrogate execution logging across HPC systems .
  licensing: "BSD 3-Clause License"
  task_types:
  - "Systems benchmarking"
  ai_capability_measured:
  - "Metadata tracking"
  - "reproducible HPC workflows"
  metrics:
  - "Metadata completeness"
  - "FAIR compliance"
  models:
  - "NA"
  ml_motif:
  - Generative
  - Classification
  - Regression
  type: "Platform"
  ml_task:
  - "NA"
  solutions: "0"
  notes: |
    Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc.
  contact:
    name: "Piotr Luszczek"
    email: "luszczek@utk.edu"
  cite:
  - |
    @techreport{luszczek2021sabath,
      title={SABATH: FAIR Metadata Technology for Surrogate Benchmarks},
      author={Luszczek, Piotr},
      year={2021},
      institution={University of Tennessee},
      url={https://github.com/icl-utk-edu/slip/tree/sabath}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "(none)"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "N/A"
  ratings:
    software:
      rating: 4
      reason: |
        Actively maintained GitHub repository (https://github.com/icl-utk-edu/slip/tree/sabath)
        with BSD-licensed tooling for FAIR metadata capture; integrates with existing surrogate
        modeling benchmarks.
    specification:
      rating: 4
      reason: |
        FAIR metadata structure and logging goals are clearly described. Input/output definitions
        are implied through integrations (e.g., MiniWeatherML), though not always formalized.
    dataset:
      rating: 4
      reason: |
        Datasets used in surrogate benchmarks are publicly available, well-structured, and
        FAIR-aligned, but not independently hosted by Sabath itself.
    metrics:
      rating: 4
      reason: |
        Emphasizes metadata completeness and FAIR compliance. Metrics are clear and well-matched
        to its metadata-focused benchmarking context.
    reference_solution:
      rating: 3
      reason: |
        Includes integration with multiple surrogate benchmarks and models, though not all are
        fully documented or packaged as standardized reference solutions.
    documentation:
      rating: 3
      reason: |
        Basic instructions and code are provided on GitHub, but more detailed walkthroughs,
        use-case examples, or tutorials are limited.