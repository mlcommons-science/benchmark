# MLCommons Science Working Group AI Benchmarks Collection

This site curates a collection of AI benchmarks.  
The main artifact is the PDF report:

- **Report (PDF):** [benchmarks.pdf](benchmarks.pdf)

If you cite this work, please reference the PDF (not the Markdown pages). A BibTeX entry is provided below.

```bibtex
@misc{www-las-mlcommons-benchmark-coolection,
  author = {
    Gregor von Laszewsk and 
    Ben Hawks and 
    Marco Colombo and
    Reece Shiraishi and
    Anjay Krishnan and
    Nhan Tran and
    Geoffrey C. Fox},
  title = {MLCommons Science Working Group AI Benchmarks Collection},
  url = {https://mlcommons-science.github.io/benchmark/benchmarks.pdf}
  note = "Online Collection: \url={https://mlcommons-science.github.io/benchmark/}",
  month = jun,
  year = 2025,
  howpublished = "GitHub"
}
```

For online browsing, we provide two views:

- **[Benchmarks (cards)](md/benchmarks/)**: filterable and sortable index with detail pages.
- **[Benchmarks (table)](md/benchmarks_table)**: compact Markdown table with links to the same detail pages.

> **Note:** The Markdown pages are generated for web browsing and should **not** be cited. Please cite the PDF report above.

All pages are generated automatically, please donâ€™t edit them directly.  
To propose changes, update the source files here:

- <https://github.com/mlcommons-science/benchmark/tree/main/source>

For program improvements, please contact **Gregor von Laszewski** at `laszewski at gmail.com`.

The project README is available at:

- <https://github.com/mlcommons-science/benchmark>
