- date: "2020-09-07"
  version: "1"
  last_updated: "2020-09-07"
  expired: "false"
  valid: "yes"
  valid_date: "2025-07-28"
  name: "MMLU (Massive Multitask Language Understanding)"
  url: "https://paperswithcode.com/dataset/mmlu"
  doi: "10.48550/arXiv.2009.03300"
  domain: "Multidomain"
  focus: "Academic knowledge and reasoning across 57 subjects"
  keywords:
  - "multitask"
  - "multiple-choice"
  - "zero-shot"
  - "few-shot"
  - "knowledge probing"
  summary: |
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  licensing: "MIT License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "General reasoning, subject-matter understanding"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4o"
  - "Gemini 1.5 Pro"
  - "o1"
  - "DeepSeek-R1"
  ml_motif:
  - "General knowledge"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "1"
  notes: "Good"
  contact:
    name: "Dan Hendrycks"
    email: "dan (at) safe.ai"
  cite:
  - |
    @misc{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav},
      journal={arXiv preprint arXiv:2009.03300},
      year={2021},
      url={https://arxiv.org/abs/2009.03300}
    }
  datasets:
    links:
    - name: "Papers with Code datasets"
      url: "https://github.com/paperswithcode/paperswithcode-data"
  results:
    links:
    - name: "Chinchilla"
      url: "https://arxiv.org/abs/2203.15556"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No instructions to download or run data given on the site
    specification:
      rating: 4
      reason: |
        No system constraints
    dataset:
      rating: 5
      reason: |
        Meets all FAIR principles and properly versioned.
    metrics:
      rating: 5
      reason: |
        Fully defined, represents a solution's performance.
    reference_solution:
      rating: 2
      reason: |
        Reference models are available (i.e. GPT-3), but are not trainable or publicly documented
    documentation:
      rating: 5
      reason: |
        Well-explained in a provided paper.

- date: "2023-11-20"
  version: "1"
  last_updated: "2023-11-20"
  expired: "false"
  valid: "yes"
  valid_date: "2023-11-20"
  name: "GPQA Diamond"
  url: "https://arxiv.org/abs/2311.12022"
  doi: "10.48550/arXiv.2311.12022"
  domain: "Science"
  focus: "Graduate-level scientific reasoning"
  keywords:
  - "Google-proof"
  - "graduate-level"
  - "science QA"
  - "chemistry"
  - "physics"
  summary: |
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is Google-proof - experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  licensing: "unknown"
  task_types:
  - "Multiple choice"
  - "Multi-step QA"
  ai_capability_measured:
  - "Scientific reasoning, deep knowledge"
  metrics:
  - "Accuracy"
  models:
  - "o1"
  - "DeepSeek-R1"
  ml_motif:
  - "Science and STEM fields"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Julian Michael"
    email: "julianjm@nyu.edu"
  cite:
  - |
    @misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
      author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},
      year={2023},
      url={https://arxiv.org/abs/2311.12022}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        Python version and requirements specified on Github site
    specification:
      rating: 2
      reason: |
        No system constraints or I/O specified
    dataset:
      rating: 5
      reason: |
        Easily able to access dataset. Comes with predefined splits as mentioned in the paper
    metrics:
      rating: 5
      reason: |
        Each question has a correct answer, representing the tested model's performance.
    reference_solution:
      rating: 1
      reason: |
        Common models such as GPT-3.5 were compared. They are not open and don't provide requirements
    documentation:
      rating: 5
      reason: |
        All information is listed in the associated paper

- date: "2018-03-14"
  version: "1"
  last_updated: "2018-03-14"
  expired: "false"
  valid: "yes"
  valid_date: "2018-03-14"
  name: "ARC-Challenge (Advanced Reasoning Challenge)"
  url: "https://allenai.org/data/arc"
  doi: "NA"
  domain: "Science"
  focus: "Grade-school science with reasoning emphasis"
  keywords:
  - "grade-school"
  - "science QA"
  - "challenge set"
  - "reasoning"
  summary: |
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  licensing: "Apache 2.0 License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Commonsense and scientific reasoning"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4"
  - "Claude"
  ml_motif:
  - "Elementary science"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @inproceedings{clark2018think,
      title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
      author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren},
      booktitle={EMNLP 2018},
      pages={237-248},
      year={2018},
      url={https://allenai.org/data/arc}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/allenai/ai2_arc"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No link to code or documentation
    specification:
      rating: 2
      reason: |
        Task is clear, but no constraints or format is mentioned
    dataset:
      rating: 4
      reason: |
        Data accessible, offers instructions on how to download the data via CLI tools. No splits.
    metrics:
      rating: 5
      reason: |
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 1
      reason: |
        There are over 300 models listed, but very few, if any, show performance on the dataset or list constraints
    documentation:
      rating: 5
      reason: |
        Explains all necessary information inside a paper

- date: "2025-01-24"
  version: "1"
  last_updated: "2025-01-24"
  expired: "false"
  valid: "yes"
  valid_date: "2025-01-24"
  name: "Humanity's Last Exam"
  url: "https://arxiv.org/abs/2501.14249"
  doi: "10.48550/arXiv.2501.14249"
  domain: "Multidomain"
  focus: "Broad cross-domain academic reasoning"
  keywords:
  - "cross-domain"
  - "academic exam"
  - "multiple-choice"
  - "multidisciplinary"
  summary: |
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  licensing: "MIT License"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Cross-domain academic reasoning"
  metrics:
  - "Accuracy"
  models: 
  -  "unkown"
  ml_motif:
  - "Multi-domain"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "HLE team"
    email: "agibenchmark@safe.ai"
  cite:
  - |
    @misc{phan2025humanitysexam,
      archiveprefix = {arXiv},
      author        = {Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and Søren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and Gözdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Brüssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Martí Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and Václav Rozhoň and Vincent Ginis and Christian Stump and Niv Cohen and Rafał Poświata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givré and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar Ängquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and Jérémy Andréoletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Khánh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Biró Bálint and Eve J. Y. Lo and Jiaqi Wang and Maria Inês S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciobâcă and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekström and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Peñaflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yiğit Yalin and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Boscá and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle Häggström and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hernández-Cámara and Emanuele Rodolà and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro José Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Raúl Adrián Huerta Rodríguez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benjámin Borbás and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran Đuc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub Łucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels Mündler and Sören Möller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and Mátyás Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubić and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vilém Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Mickaël Noyé and Michał Perełkiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and Dániel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Ginés and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han Lù and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Briański and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanović and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Gaël Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and Károly Zsolnai-Fehér and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gonçalves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Yücel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks},
      eprint        = {2501.14249},
      primaryclass  = {cs.LG},
      title         = {Humanity's Last Exam},
      url           = {https://arxiv.org/abs/2501.14249},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/cais/hle"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 4
      reason: |
        Code for testing models posted on the github. Unknown how to run a custom model.
    specification:
      rating: 2
      reason: |
        Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified
    dataset:
      rating: 2
      reason: |
        Data accessible through Hugging Face, but requires giving contact information to access
    metrics:
      rating: 5
      reason: |
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 2
      reason: |
        Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result
    documentation:
      rating: 5
      reason: |
        Paper available with necessary information

- date: "2024-11-07"
  version: "1"
  last_updated: "2024-11-07"
  expired: "false"
  valid: "yes"
  valid_date: "2024-11-07"
  name: "FrontierMath"
  url: "https://arxiv.org/abs/2411.04872"
  doi: "10.48550/arXiv.2411.04872"
  domain: "Mathematics"
  focus: "Challenging advanced mathematical reasoning"
  keywords:
  - "symbolic reasoning"
  - "number theory"
  - "algebraic geometry"
  - "category theory"
  summary: |
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs 
    ability to solve problems requiring deep abstract reasoning.
  licensing: "unknown"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Symbolic and abstract mathematical reasoning"
  metrics:
  - "Accuracy"
  models: 
  - unkown
  ml_motif:
  - "Math problem solving"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "FrontierMath team"
    email: "math_evals@epochai.org"
  cite:
  - |
    @misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
      archiveprefix = {arXiv},
      author        = {Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli Järviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},
      eprint        = {2411.04872},
      primaryclass  = {cs.AI},
      title         = {FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
      url           = {https://arxiv.org/abs/2411.04872},
      year          = {2024}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No link to code provided
    specification:
      rating: 3
      reason: |
        Well-specified process for asking questions and receiving answers. No software or hardware constraints
    dataset:
      rating: 0
      reason: |
        Paper and website had no link to any dataset. It may still exist somewhere
    metrics:
      rating: 5
      reason: |
        (by default) All questions in the dataset have a correct answer
    reference_solution:
      rating: 2
      reason: |
        Displays result of leading models on the benchmark, but none are trainable or list constraints
    documentation:
      rating: 0
      reason: |
        No specified way to reproduce the reference solution

- date: "2024-07-18"
  version: "1"
  last_updated: "2024-07-18"
  expired: "false"
  valid: "yes"
  valid_date: "2024-07-18"
  name: "SciCode"
  url: "https://arxiv.org/abs/2407.13168"
  doi: "10.48550/arXiv.2407.13168"
  domain: "Scientific Programming"
  focus: "Scientific code generation and problem solving"
  keywords:
  - "code synthesis"
  - "scientific computing"
  - "programming benchmark"
  summary: |
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  licensing: "unknown"
  task_types:
  - "Coding"
  ai_capability_measured:
  - "Program synthesis, scientific computing"
  metrics:
  - "Solve rate (%)"
  models:
  - "Claude3.5-Sonnet"
  ml_motif:
  - "Coding"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "unknown"
  notes: "Good"
  contact:
    name: "Minyang Tian"
    email: "mtian8@illinois.edu"
  cite:
  - |
    @misc{tian2024scicoderesearchcodingbenchmark,
      archiveprefix = {arXiv},
      author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
      eprint        = {2407.13168},
      primaryclass  = {cs.AI},
      title         = {SciCode: A Research Coding Benchmark Curated by Scientists},
      url           = {https://arxiv.org/abs/2407.13168},
      year          = {2024}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        Code to run exists on github repo
    specification:
      rating: 4.5
      reason: |
        Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.
    dataset:
      rating: 0
      reason: |
        Paper and website had no link to any dataset. It may still exist somewhere
    metrics:
      rating: 2
      reason: |
        Metrics stated, but method of grading is not specified
    reference_solution:
      rating: 1
      reason: |
        Models presented with scores, but none are open or list constraints
    documentation:
      rating: 4
      reason: |
        Paper containing all needed info except for evlauation criteria

- date: "2025-03-13"
  version: "1"
  last_updated: "2025-03-13"
  expired: "false"
  valid: "yes"
  valid_date: "2025-03-13"
  name: "AIME (American Invitational Mathematics Examination)"
  url: "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  doi: "NA"
  domain: "Mathematics"
  focus: "Pre-college advanced problem solving"
  keywords:
  - "algebra"
  - "combinatorics"
  - "number theory"
  - "geometry"
  summary: |
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  licensing: "unknown"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Mathematical problem-solving and reasoning"
  metrics:
  - "Accuracy"
  models: 
  - "unkown"
  ml_motif:
  - "Math problem solving"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Designed for human test-takers"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{www-aime,
      author = {TBD},
      title = {AIME},
      url = {https://www.vals.ai/benchmarks/aime-2025-03-13},
      month = mar,
      year = 2025,
      note = {[Online accessed 2025-06-24]}
    }
  datasets:
    links:
    - name: "AoPS website"
      url: "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No code available
    specification:
      rating: 0
      reason: |
        Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints
    dataset:
      rating: 4
      reason: |
        Easily accessible data with problems and solutions, but no splits
    metrics:
      rating: 5
      reason: |
        (by default) Answer is correct or it's not
    reference_solution:
      rating: 0
      reason: |
        Not given. Human performance stats exist, but no mentions of AI performance
    documentation:
      rating: 0
      reason: |
        Not given

- date: "2025-02-15"
  version: "1"
  last_updated: "2025-02-15"
  expired: "false"
  valid: "yes"
  valid_date: "2025-02-15"
  name: "MATH-500"
  url: "https://huggingface.co/datasets/HuggingFaceH4/MATH-500"
  doi: "unknown"
  domain: "Mathematics"
  focus: "Math reasoning generalization"
  keywords:
  - "calculus"
  - "algebra"
  - "number theory"
  - "geometry"
  summary: |
    MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
    high-school to advanced levels, designed to evaluate LLMs mathematical reasoning and 
    generalization.
  licensing: "MIT License"
  task_types:
  - "Problem solving"
  ai_capability_measured:
  - "Math reasoning and generalization"
  metrics:
  - "Accuracy"
  models:
  - "unkown"
  ml_motif:
  - "Math problem solving"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Dataset hosted on Hugging Face. Data comes from a subset of OpenAI's dataset"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{huggingface2025math500,
      title={MATH-500},
      author={HuggingFaceH4},
      year={2025},
      url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/HuggingFaceH4/MATH-500"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No code provided
    specification:
      rating: 0
      reason: |
        No method of presentation and evaluation is not stated. No constraints
    dataset:
      rating: 5
      reason: |
        Problems and solutions are easily downloaded. Could not find a way to download the data
    metrics:
      rating: 2
      reason: |
        Problem spec states that all of the AI reasoning steps are subject to grading, but no specified way to evaluate the steps
    reference_solution:
      rating: 0
      reason: |
        Not given
    documentation:
      rating: 0
      reason: |
        Not given. Implicit instructions to download dataset.

- date: "2024-04-02"
  version: "1"
  last_updated: "2024-04-02"
  expired: "false"
  valid: "yes"
  valid_date: "2024-04-02"
  name: "CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)"
  url: "https://arxiv.org/abs/2503.13517"
  doi: "10.48550/arXiv.2503.13517"
  domain: "Multidomain Science"
  focus: "Long-context scientific reasoning"
  keywords:
  - "long-context"
  - "information extraction"
  - "multimodal"
  summary: |
    CURIE is a benchmark of 580 problems across six scientific disciplines-materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics-
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  licensing: "Apache 2.0 License"
  task_types:
  - "Information extraction"
  - "Reasoning"
  - "Concept tracking"
  - "Aggregation"
  - "Algebraic manipulation"
  - "Multimodal comprehension"
  ai_capability_measured:
  - "Long-context understanding and scientific reasoning"
  metrics:
  - "Accuracy"
  models:
  - "unkown"
  ml_motif:
  - "Scientific problem solving"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Subhashini Venugopalan"
    email: "vsubhashini@google.com"
  cite:
  - |
    @misc{cui2025curieevaluatingllmsmultitask,
      title={CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning}, 
      author={Hao Cui and Zahra Shamsi and Gowoon Cheon and Xuejian Ma and Shutong Li and Maria Tikhanovskaya and Peter Norgaard and Nayantara Mudur and Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and Victor V. Albert and Pranesh Srinivasan and Haining Pan and Philippe Faist and Brian Rohr and Ekin Dogus Cubuk and Muratahan Aykol and Amil Merchant and Michael J. Statt and Dan Morris and Drew Purves and Elise Kleeman and Ruth Alcantara and Matthew Abraham and Muqthar Mohammad and Ean Phing VanLee and Chenfei Jiang and Elizabeth Dorfman and Eun-Ah Kim and Michael P Brenner and Viren Jain and Sameera Ponda and Subhashini Venugopalan},
      year={2025},
      eprint={2503.13517},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.13517}, 
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 4
      reason: |
        Code is available, but not well documented
    specification:
      rating: 1
      reason: |
        Explains types of problems in detail, but does not state exactly how to administer them.
    dataset:
      rating: 4
      reason: |
        Dataset is available via Github, but hard to find
    metrics:
      rating: 5
      reason: |
        Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem.
    reference_solution:
      rating: 1
      reason: |
        Exists, but is not open
    documentation:
      rating: 5
      reason: |
        Associated paper explains all criteria

- date: "2023-01-26"
  version: "1"
  last_updated: "2023-01-26"
  expired: "false"
  valid: "no"
  valid_date: "2023-01-26"
  name: "FEABench (Finite Element Analysis Benchmark)"
  url: "https://github.com/google/feabench"
  doi: "unknown"
  domain: "Computational Engineering"
  focus: "FEA simulation accuracy and performance"
  keywords:
  - "finite element"
  - "simulation"
  - "PDE"
  summary: |
    Does not exist
  licensing: "unknown"
  task_types:
  - "Simulation"
  - "Performance evaluation"
  ai_capability_measured:
  - "Numerical simulation accuracy and efficiency"
  metrics:
  - "Solve time"
  - "Error norm"
  models:
  - "FEniCS"
  - "deal.II"
  ml_motif:
  - "unknown"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "unknown"
  notes: "OK"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{mudur2025feabenchevaluatinglanguagemodels,
      title={FEABench: Evaluating Language Models on Multiphysics Reasoning Ability}, 
      author={Nayantara Mudur and Hao Cui and Subhashini Venugopalan and Paul Raccuglia and Michael P. Brenner and Peter Norgaard},
      year={2025},
      eprint={2504.06260},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.06260}, 
      }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 4
      reason: |
        Code is available, but poorly documented
    specification:
      rating: 1.5
      reason: |
        Output is defined and task clarity is questionable
    dataset:
      rating: 4
      reason: |
        Available, but not split into sets
    metrics:
      rating: 5
      reason: |
        Fully defined metrics
    reference_solution:
      rating: 4
      reason: |
        Three open-source models were used. No system constraints.
    documentation:
      rating: 5
      reason: |
        In associated paper

- date: "2024-07-12"
  version: "1"
  last_updated: "2024-07-12"
  expired: "false"
  valid: "yes"
  valid_date: "2024-07-12"
  name: "SPIQA (Scientific Paper Image Question Answering)"
  url: "https://arxiv.org/abs/2407.09413"
  doi: "10.48550/arXiv.2407.09413"
  domain: "Computer Science"
  focus: "Multimodal QA on scientific figures"
  keywords:
  - "multimodal QA"
  - "figure understanding"
  - "table comprehension"
  - "chain-of-thought"
  summary: |
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  licensing: "Apache 2.0 License"
  task_types:
  - "Question answering"
  - "Multimodal QA"
  - "Chain-of-Thought evaluation"
  ai_capability_measured:
  - "Visual-textual reasoning in scientific contexts"
  metrics:
  - "Accuracy"
  - "F1 score"
  models:
  - "Chain-of-Thought models"
  - "Multimodal QA systems"
  ml_motif:
  - "Scientific paper reading"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Good"
  contact:
    name: "Subhashini Venugopalan"
    email: "vsubhashini@google.com"
  cite:
  - |
    @misc{zhong2024spiqa,
      title={SPIQA: Scientific Paper Image Question Answering},
      author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
      year={2024},
      url={https://arxiv.org/abs/2407.09413}
    }
  datasets:
    links:
    - name: "Hugging Face"
      url: "https://huggingface.co/datasets/google/spiqa"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        Not provided
    specification:
      rating: 5
      reason: |
        Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.
    dataset:
      rating: 4.5
      reason: |
        Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.
    metrics:
      rating: 5
      reason: |
        Uses quantitative metrics (Accuracy, F1) aligned with the task
    reference_solution:
      rating: 2
      reason: |
        Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.
    documentation:
      rating: 5
      reason: |
        All information provided in paper

- date: "2020-09-28"
  version: "1"
  last_updated: "2020-09-28"
  expired: "false"
  valid: "yes"
  valid_date: "2020-09-28"
  name: "MedQA"
  url: "https://arxiv.org/abs/2009.13081"
  doi: "10.48550/arXiv.2009.13081"
  domain: "Medical Question Answering"
  focus: "Medical board exam QA"
  keywords:
  - "USMLE"
  - "diagnostic QA"
  - "medical knowledge"
  - "multilingual"
  summary: |
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  licensing: "Under Association for the Advancement of Artificial Intelligence"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Medical diagnosis and knowledge retrieval"
  metrics:
  - "Accuracy"
  models:
  - "Neural reader"
  - "Retrieval-based QA systems"
  ml_motif:
  - "Medical diagnosis"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Multilingual (English, Simplified and Traditional Chinese)"
  contact:
    name: "Di Jin"
    email: "jindi15@mit.edu"
  cite:
  - |
    @misc{jin2020diseasedoespatienthave,
        archiveprefix = {arXiv},
        author        = {Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
        eprint        = {2009.13081},
        primaryclass  = {cs.CL},
        title         = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
        url           = {https://arxiv.org/abs/2009.13081},
        year          = {2020}
      }
  datasets:
    links:
    - name: "Github"
      url: "https://github.com/jind11/MedQA"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        All code available on the github
    specification:
      rating: 3
      reason: |
        Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.
    dataset:
      rating: 4
      reason: |
        Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.
    metrics:
      rating: 5
      reason: |
        Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.
    reference_solution:
      rating: 0
      reason: |
        No reference solution mentioned.
    documentation:
      rating: 4
      reason: |
        Paper is available. Evaluation criteria are not mentioned.

- date: "2025-05-13"
  version: "1"
  last_updated: "2025-05-13"
  expired: "false"
  valid: "yes"
  valid_date: "2025-05-13"
  name: "BaisBench (Biological AI Scientist Benchmark)"
  url: "https://arxiv.org/abs/2505.08341"
  doi: "10.48550/arXiv.2505.08341"
  domain: "Computational Biology"
  focus: "Omics-driven AI research tasks"
  keywords:
  - "single-cell annotation"
  - "biological QA"
  - "autonomous discovery"
  summary: |
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  licensing: "MIT License"
  task_types:
  - "Cell type annotation"
  - "Multiple choice"
  ai_capability_measured:
  - "Autonomous biological research capabilities"
  metrics:
  - "Annotation accuracy"
  - "QA accuracy"
  models:
  - "LLM-based AI scientist agents"
  ml_motif:
  - "Scientific research"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Underperforms human experts; aims to advance AI-driven discovery"
  contact:
    name: "Xuegong Zhang"
    email: "zhangxg@mail.tsinghua.edu.cn"
  cite:
  - |
    @misc{luo2025benchmarkingaiscientistsomics,
      archiveprefix = {arXiv},
      author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},
      eprint        = {2505.08341},
      primaryclass  = {cs.AI},
      title         = {Benchmarking AI scientists in omics data-driven biological research},
      url           = {https://arxiv.org/abs/2505.08341},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Github"
      url: "https://github.com/EperLuo/BaisBench"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        Instructions for environment setup available
    specification:
      rating: 4
      reason: |
        Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.
    dataset:
      rating: 5
      reason: |
        Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.
    metrics:
      rating: 5
      reason: |
        Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.
    reference_solution:
      rating: 0
      reason: |
        Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.
    documentation:
      rating: 5
      reason: |
        Dataset and paper accessible; IPYNB files for setup are available on the github repo.

- date: "2023-01-26"
  version: "1"
  last_updated: "2023-01-26"
  expired: "false"
  valid: "yes"
  valid_date: "2023-01-26"
  name: "MOLGEN"
  url: "https://github.com/zjunlp/MolGen"
  doi: "10.48550/arXiv.2301.11259"
  domain: "Computational Chemistry"
  focus: "Molecular generation and optimization"
  keywords:
  - "SELFIES"
  - "GAN"
  - "property optimization"
  summary: |
    MolGen is a pre-trained molecular language model that generates chemically valid
    molecules using SELFIES and reinforcement learning, guided by chemical feedback 
    to optimize properties such as logP, QED, and docking score.
  licensing: "MIT License"
  task_types:
  - "Distribution learning"
  - "Goal-oriented generation"
  ai_capability_measured:
  - "Generation of valid and optimized molecular structures"
  metrics:
  - "Validity%"
  - "Novelty%"
  - "QED"
  - "Docking score"
  models:
  - "MolGen"
  ml_motif:
  - "Chemical generation"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "This is a model, not a benchmark"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @misc{fang2024domainagnosticmoleculargenerationchemical,
      archiveprefix = {arXiv},
      author        = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
      eprint        = {2301.11259},
      primaryclass  = {cs.LG},
      title         = {Domain-Agnostic Molecular Generation with Chemical Feedback},
      url           = {https://arxiv.org/abs/2301.11259},
      year          = {2024}
    }
  datasets:
    links:
    - name: "unknown"
      url: "unknown"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        This is a pre-trained model
    specification:
      rating: 0
      reason: |
        This is a pre-trained model
    dataset:
      rating: 0
      reason: |
        This is a pre-trained model
    metrics:
      rating: 0
      reason: |
        This is a pre-trained model
    reference_solution:
      rating: 0
      reason: |
        This is a pre-trained model
    documentation:
      rating: 0
      reason: |
        This is a pre-trained model

- date: "2020-05-02"
  version: "1"
  last_updated: "2020-05-02"
  expired: "false"
  valid: "yes"
  valid_date: "2020-05-02"
  name: "Open Graph Benchmark (OGB) - Biology"
  url: "https://ogb.stanford.edu/docs/home/"
  doi: "10.48550/arXiv.2005.00687"
  domain: "Graph ML"
  focus: "Biological graph property prediction"
  keywords:
  - "node prediction"
  - "link prediction"
  - "graph classification"
  summary: |
    OGB-Biology is a suite of large-scale biological network datasets (protein-protein
    interaction, drug-target, etc.) with standardized splits and evaluation protocols 
    for node, link, and graph property prediction tasks.
  licensing: "MIT License"
  task_types:
  - "Node property prediction"
  - "Link property prediction"
  - "Graph property prediction"
  ai_capability_measured:
  - "Scalability and generalization in graph ML for biology"
  metrics:
  - "Accuracy"
  - "ROC-AUC"
  models:
  - "GCN"
  - "GraphSAGE"
  - "GAT"
  ml_motif:
  - "Chemical biology"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Community-driven updates"
  contact:
    name: "OGB Team"
    email: "ogb@cs.stanford.edu"
  cite:
  - |
    @misc{hu2021opengraphbenchmarkdatasets,
        archiveprefix = {arXiv},
        author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
        eprint        = {2005.00687},
        primaryclass  = {cs.LG},
        title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
        url           = {https://arxiv.org/abs/2005.00687},
        year          = {2021}
    }
  datasets:
    links:
    - name: "OGB Webpage"
      url: "https://ogb.stanford.edu/docs/dataset_overview/"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        All necessary information is provided on the Github
    specification:
      rating: 4
      reason: |
        Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined. No constraints.
    dataset:
      rating: 5
      reason: |
        Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.
    metrics:
      rating: 5
      reason: |
        Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.
    reference_solution:
      rating: 3
      reason: |
        Multiple baselines implemented and documented (GCN, GAT, GraphSAGE). No contraints.
    documentation:
      rating: 5
      reason: |
        All necessary information is included in a paper.

- date: "2011-10-01"
  version: "1"
  last_updated: "2011-10-01"
  expired: "false"
  valid: "yes"
  valid_date: "2011-10-01"
  name: "Materials Project"
  url: "https://materialsproject.org/"
  doi: "unknown"
  domain: "Materials Science"
  focus: "DFT-based property prediction"
  keywords:
  - "DFT"
  - "materials genome"
  - "high-throughput"
  summary: |
    The Materials Project provides an open-access database of computed properties for
    inorganic materials via high-throughput density functional theory (DFT), accelerating 
    materials discovery.
  licensing: "https://next-gen.materialsproject.org/about/terms"
  task_types:
  - "Property prediction"
  ai_capability_measured:
  - "Prediction of inorganic material properties"
  metrics:
  - "MAE"
  - "R^2"
  models:
  - "Automatminer"
  - "Crystal Graph Neural Networks"
  ml_motif:
  - "Material properties"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Core component of the Materials Genome Initiative"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @article{jain2013materials,
      title={The Materials Project: A materials genome approach},
      author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},
      journal={APL Materials},
      volume    = {1},
      number    = {1},
      year={2013},
      doi       = {10.1063/1.4812323},
      url={https://materialsproject.org/}
    }
  datasets:
    links:
    - name: "Materials Project Catalysis Explorer"
      url: "https://next-gen.materialsproject.org/catalysis"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No instructions available
    specification:
      rating: 1.5
      reason: |
        The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.
    dataset:
      rating: 3
      reason: |
        API key required to access data. No predefined splits.
    metrics:
      rating: 5
      reason: |
        Uses numerical metrics like MAE and R^2
    reference_solution:
      rating: 2
      reason: |
        Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed.
    documentation:
      rating: 0
      reason: |
        No explanations or paper provided

- date: "2020-10-20"
  version: "1"
  last_updated: "2020-10-20"
  expired: "false"
  valid: "yes"
  valid_date: "2020-10-20"
  name: "OCP (Open Catalyst Project)"
  url: "https://opencatalystproject.org/"
  doi: "unknown"
  domain: "Chemistry; Materials Science"
  focus: "Catalyst adsorption energy prediction"
  keywords:
  - "DFT relaxations"
  - "adsorption energy"
  - "graph neural networks"
  summary: |
    The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate 
    relaxation datasets, challenging ML models to predict energies and forces for 
    renewable energy applications.
  licensing: "OCP Terms of Use"
  task_types:
  - "Energy prediction"
  - "Force prediction"
  ai_capability_measured:
  - "Prediction of adsorption energies and forces"
  metrics:
  - "MAE (energy)"
  - "MAE (force)"
  models:
  - "CGCNN"
  - "SchNet"
  - "DimeNet++"
  - "GemNet-OC"
  ml_motif:
  - "Chemistry"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "Public leaderboards; active community development"
  contact:
    name: "unknown"
    email: "unknown"
  cite:
  - |
    @article{chanussot2021oc20,
      title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},
      author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
      journal   = {ACS Catalysis},
      volume    = {11},
      number    = {10},
      pages     = {6059--6072},
      year      = {2021},
      doi       = {10.1021/acscatal.0c04525},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}
    }
  - |
    @article{tran2023oc22,
      title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
      author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
      journal   = {ACS Catalysis},
      volume    = {13},
      number    = {5},
      pages     = {3066--3084},
      year      = {2023},
      doi       = {10.1021/acscatal.2c05426},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}
    }
  - |
    @article{doi:10.1021/acscatal.0c04525,
      author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
    title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
      journal = {ACS Catalysis},
      volume = {11},
      number = {10},
      pages = {6059-6072},
      year = {2021},
      doi = {10.1021/acscatal.0c04525},
      URL = {https://doi.org/10.1021/acscatal.0c04525},eprint = {https://doi.org/10.1021/acscatal.0c04525}}"
  - |
    @article{tran2023b,
      title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
      volume={13},
      ISSN={2155-5435},
      url={http://dx.doi.org/10.1021/acscatal.2c05426},
      DOI={10.1021/acscatal.2c05426},
      number={5},
      journal={ACS Catalysis},
      publisher={American Chemical Society (ACS)},
      author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
      year={2023},
      month=feb, pages={3066-3084} 
    }
  datasets:
    links:
    - name: "OCP Dataset"
      url: "https://fair-chem.github.io/catalysts/datasets/summary"
  results:
    links:
    - name: "OCP Pretrained Models"
      url: "https://fair-chem.github.io/catalysts/models.html"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        Data provided in Github links
    specification:
      rating: 5
      reason: |
        Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.
    dataset:
      rating: 5
      reason: |
        Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.
    metrics:
      rating: 5
      reason: |
        MAE (energy and force) are standard and reproducible.
    reference_solution:
      rating: 4
      reason: |
        Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed.
    documentation:
      rating: 1
      reason: |
        Paper exists, but content is behind a paywall.

- date: "2023-06-20"
  version: "1"
  last_updated: "2023-06-20"
  expired: "false"
  valid: "yes"
  valid_date: "2023-06-20"
  name: "JARVIS-Leaderboard"
  url: "https://arxiv.org/abs/2306.11688"
  doi: "10.48550/arXiv.2306.11688"
  domain: "Materials Science; Benchmarking"
  focus: "Comparative evaluation of materials design methods"
  keywords:
  - "leaderboards"
  - "materials methods"
  - "simulation"
  summary: |
    JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
    structure, force-fields, quantum computing, and experimental methods across hundreds
    of materials science tasks.
  licensing: "NIST"
  task_types:
  - "Method benchmarking"
  - "Leaderboard ranking"
  ai_capability_measured:
  - "Performance comparison across diverse materials design methods"
  metrics:
  - "MAE"
  - "RMSE"
  - "Accuracy"
  models:
  - "unkown"
  ml_motif:
  - "Material science"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "0"
  notes: "1281 contributions across 274 benchmarks"
  contact:
    name: "Kamal Choudhary"
    email: "kamal.choudhary@nist.gov"
  cite:
  - |
    @article{choudhary2024jarvis,
      title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},
    author = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},
      journal = {npj Computational Materials},
      volume = {10},
      number = {1},
      pages = {93},
      year = {2024},
      doi = {10.1038/s41524-024-01259-w},
      url = {https://doi.org/10.1038/s41524-024-01259-w}
    }
  datasets:
    links:
    - name: "AI model specific benchmarks"
      url: "https://pages.nist.gov/jarvis_leaderboard/AI/"
  results:
    links:
    - name: "unknown"
      url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 1
      reason: |
        Setup script provided, but no code provided
    specification:
      rating: 1
      reason: |
        Only dataset format is defined.
    dataset:
      rating: 4
      reason: |
        Data is public and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks. No splits.
    metrics:
      rating: 5
      reason: |
        Metrics stated for each benchmark.
    reference_solution:
      rating: 4
      reason: |
        Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); no constraints specified.
    documentation:
      rating: 1
      reason: |
        Only the task is specified.

- date: "2022-02-22"
  version: "1"
  last_updated: "2022-02-22"
  expired: "false"
  valid: "yes"
  valid_date: "2022-02-22"
  name: "Quantum Computing Benchmarks (QML)"
  url: "https://github.com/XanaduAI/qml-benchmarks"
  doi: "10.48550/arXiv.2307.03901"
  domain: "Quantum Computing"
  focus: "Quantum algorithm performance evaluation"
  keywords:
    - "quantum circuits"
    - "state preparation"
    - "error correction"
  summary: |
    A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
    preparation, circuit optimization, and error correction across multiple platforms.
  licensing: "Apache-2.0"
  task_types:
    - "Circuit benchmarking"
    - "State classification"
  ai_capability_measured:
    - "Quantum algorithm performance and fidelity"
  metrics:
    - "Fidelity"
    - "Success probability"
  models:
    - "IBM Q"
    - "IonQ"
    - "AQT@LBNL"
  ml_motif:
    - "Performance Evaluation"
  type: "Benchmark"
  ml_task:
    - "Supervised Learning"
  solutions: "Varies per benchmark"
  notes: "Hardware-agnostic, application-level metrics. The citation may not be correct."
  contact:
    name: "Xanadu AI"
    email: "support@xanadu.ai"
  cite:
    - |
      @inproceedings{kiwit2023,
        title={Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK},
        url={http://dx.doi.org/10.1109/QCE57702.2023.00061},
        DOI={10.1109/qce57702.2023.00061},
        booktitle={2023 IEEE International Conference on Quantum Computing and Engineering (QCE)},
        publisher={IEEE},
        author={Kiwit, Florian J. and Marso, Marwa and Ross, Philipp and Riofrío, Carlos A. and Klepsch, Johannes and Luckow, Andre},
        year={2023},
        month=sep, pages={475-484}
      }
  datasets:
    links:
      - name: "PennyLane QML Benchmarks Datasets"
        url: "https://pennylane.ai/datasets/collection/qml-benchmarks"
  results:
    links:
      - name: "QML Benchmarks GitHub Repository (Results section)"
        url: "https://github.com/XanaduAI/qml-benchmarks#results-and-leaderboards"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 4
      reason: |
        Run instructions exist, but are not easy to follow
    specification:
      rating: 3
      reason: |
        No system constraints. Task clarity and dataset format are not clearly specified.
    dataset:
      rating: 4
      reason: |
        Datasets are accessible, but not split.
    metrics: 3
      rating: 
      reason: |
        Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured.
    reference_solution:
      rating: 0
      reason: |
        Not provided
    documentation:
      rating: 1
      reason: |
        Only the task is defined. 

- date: "2024-10-01"
  version: "1"
  last_updated: "2024-10-01"
  expired: "false"
  valid: "yes"
  valid_date: "2024-10-01"
  name: "CFDBench (Fluid Dynamics)"
  url: "https://arxiv.org/abs/2310.05963"
  doi: "10.48550/arXiv.2310.05963"
  domain: "Fluid Dynamics; Scientific ML"
  focus: "Neural operator surrogate modeling"
  keywords:
    - "neural operators"
    - "CFD"
    - "FNO"
    - "DeepONet"
  summary: |
    CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
    assessing neural operators' ability to generalize to unseen PDE parameters and domains.
  licensing: "CC-BY-4.0"
  task_types:
    - "Surrogate modeling"
  ai_capability_measured:
    - "Generalization of neural operators for PDEs"
  metrics:
    - "L2 error"
    - "MAE"
  models:
    - "FNO"
    - "DeepONet"
    - "U-Net"
  ml_motif:
    - "Generalization"
  type: "Benchmark"
  ml_task:
    - "Supervised Learning"
  solutions: "Numerous, as it's a benchmark for ML models"
  notes: "302K frames across 739 cases"
  contact:
    name: "Yining Luo"
    email: "yining.luo@mail.utoronto.ca"
  cite:
    - |
      @misc{luo2024cfdbenchlargescalebenchmarkmachine,
        title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},
        author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},
        year={2024},
        url={https://arxiv.org/abs/2310.05963}
      }
  datasets:
    links:
      - name: "unknown"
        url: "unknown"
  results:
    links:
      - name: "unknown"
        url: "unknown"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation
    specification:
      rating: 0
      reason: |
        Not listed
    dataset:
      rating: 0
      reason: |
        Not given
    metrics:
      rating: 5
      reason: |
        Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.
    reference_solution:
      rating: 5
      reason: |
        Baseline models like FNO and DeepONet are implemented, hardware specified.
    documentation:
      rating: 5
      reason: |
        Associated paper gives all necessary information.

- date: "2023-04-23"
  version: "1"
  last_updated: "2023-04-23"
  expired: "false"
  valid: "yes"
  valid_date: "2023-04-23"
  name: "SatImgNet"
  url: "https://huggingface.co/datasets/saral-ai/satimagnet"
  doi: "10.48550/arXiv.2304.11619"
  domain: "Remote Sensing"
  focus: "Satellite imagery classification"
  keywords:
    - "land-use"
    - "zero-shot"
    - "multi-task"
  summary: |
    SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
    imagery classification datasets evaluating zero-shot transfer of vision-language models
    across diverse remote sensing tasks.
  licensing: "CC-BY-4.0"
  task_types:
    - "Image classification"
  ai_capability_measured:
    - "Zero-shot land-use classification"
  metrics:
    - "Accuracy"
  models:
    - "CLIP"
    - "BLIP"
    - "ALBEF"
  ml_motif:
    - "Transfer Learning"
  type: "Benchmark"
  ml_task:
    - "Supervised Learning"
  solutions: "Numerous, evaluated via leaderboard"
  notes: "Public leaderboard available"
  contact:
    name: "Jonathan Roberts"
    email: "j.roberts@cs.ox.ac.uk"
  cite:
    - |
      @article{roberts2023satin,
        title={SATIN: A multi-task metadataset for classifying satellite imagery using vision-language models},
        author={Roberts, Jonathan and Han, Kai and Albanie, Samuel},
        journal={arXiv preprint arXiv:2304.11619},
        year={2023}
      }
  datasets:
    links:
      - name: "SatImgNet on Hugging Face"
        url: "https://huggingface.co/datasets/saral-ai/satimagnet"
  results:
    links:
      - name: "SatImgNet Leaderboard"
        url: "https://huggingface.co/spaces/saral-ai/satin-leaderboard"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No scripts or environment information provided
    specification:
      rating: 9
      reason: |
        Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.
    dataset:
      rating: 5
      reason: |
        Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.
    metrics:
      rating: 5
      reason: |
        Accuracy of classification is an appropriate metric
    reference_solution:
      rating: 4
      reason: |
        Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified
    documentation:
      rating: 5
      reason: |
        Paper provides all required information

- date: "2023-07-19"
  version: "1"
  last_updated: "2023-07-19"
  expired: "false"
  valid: "yes"
  valid_date: "2023-07-19"
  name: "ClimateLearn"
  url: "https://arxiv.org/abs/2307.01909"
  doi: "10.48550/arXiv.2307.01909"
  domain: "Climate Science; Forecasting"
  focus: "ML for weather and climate modeling"
  keywords:
    - "medium-range forecasting"
    - "ERA5"
    - "data-driven"
  summary: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  licensing: "CC-BY-4.0"
  task_types:
    - "Forecasting"
  ai_capability_measured:
    - "Global weather prediction (3-5 days)"
  metrics:
    - "RMSE"
    - "Anomaly correlation"
  models:
    - "CNN baselines"
    - "ResNet variants"
  ml_motif:
    - "Forecasting"
    - "Benchmarking"
  type: "Benchmark"
  ml_task:
    - "Supervised Learning"
  solutions: "Multiple baseline models provided"
  notes: "Includes physical and ML baselines."
  contact:
    name: "Jason Jewik"
    email: "jason.jewik@ucla.edu"
  cite:
    - |
      @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
        title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
        author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
        year={2023}, eprint={2307.01909}, 
        archivePrefix={arXiv}, 
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2307.01909}
      }
  datasets:
    links:
      - name: "ClimateLearn GitHub Repository (data loaders and processing)"
        url: "https://github.com/aditya-grover/climate-learn"
  results:
    links:
      - name: "ClimateLearn Paper (results section)"
        url: "https://arxiv.org/abs/2307.01909"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        Quickstart notebook makes for easy usage
    specification:
      rating: 5
      reason: |
        Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    dataset:
      rating: 5
      reason: |
        Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    metrics:
      rating: 5
      reason: |
        ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    reference_solution:
      rating: 0
      reason: |
        The benchmark is geared for CNN architectures, but no specific model was mentioned.
    documentation:
      rating: 5
      reason: |
        Explained in the benchmark's paper. 

- date: "2022-06-09"
  version: "1"
  last_updated: "2022-06-09"
  expired: "false"
  valid: "yes"
  valid_date: "2022-06-09"
  name: "BIG-Bench (Beyond the Imitation Game Benchmark)"
  url: "https://github.com/google/BIG-bench"
  doi: "10.48550/arXiv.2206.04615"
  domain: "NLP; AI Evaluation"
  focus: "Diverse reasoning and generalization tasks"
  keywords:
    - "few-shot"
    - "multi-task"
    - "bias analysis"
  summary: |
    BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning, 
    knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
  licensing: "Apache-2.0"
  task_types:
    - "Few-shot evaluation"
    - "Multi-task evaluation"
  ai_capability_measured:
    - "Reasoning and generalization across diverse tasks"
  metrics:
    - "Accuracy"
    - "Task-specific metrics"
  models:
    - "GPT-3"
    - "Dense Transformers"
    - "Sparse Transformers"
  ml_motif:
    - "LLM evaluation"
  type: "Benchmark"
  ml_task:
    - "Supervised Learning"
  solutions: "Multiple, including human baselines"
  notes: "Human baselines included"
  contact:
    name: "Aarohi Srivastava et al."
    email: "bigbench@googlegroups.com"
  cite:
    - |
      @misc{srivastava2023imitationgamequantifyingextrapolating,
        title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
        author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
        year={2023},
        eprint={2206.04615},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2206.04615}, 
      }
  datasets:
    links:
      - name: "BIG-Bench GitHub Repository (contains tasks and data)"
        url: "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks"
  results:
    links:
      - name: "BIG-Bench GitHub Repository (results in papers and code)"
        url: "https://github.com/google/BIG-bench"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 4.5
      reason: |
        Quick start notebook provided, but instructions on how to run it are lacking.
    specification:
      rating: 4.5
      reason: |
        Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and well-documented; FAIR overall
    metrics:
      rating: 5
      reason: |
        Many tasks use standard quantitative metrics (accuracy, BLEU, F1). Others involve subjective ratings (e.g., Likert), which reduces cross-task comparability.
    reference_solution:
      rating: 2
      reason: |
        Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey.
    documentation:
      rating: 5
      reason: |
        Explained in the associated paper.

- date: "2019-11-20"
  version: "1"
  last_updated: "2019-11-20"
  expired: "false"
  valid: "yes"
  valid_date: "2019-11-20"
  name: "CommonSenseQA"
  url: "https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge"
  doi: "10.48550/arXiv.1811.00937"
  domain: "NLP; Commonsense"
  focus: "Commonsense question answering"
  keywords:
  - "ConceptNet"
  - "multiple-choice"
  - "adversarial"
  summary: |
    CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
    requiring models to apply commonsense knowledge to select the correct answer 
    among five choices.
  licensing: "MIT"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Commonsense reasoning and knowledge integration"
  metrics:
  - "Accuracy"
  models:
  - "BERT-large"
  - "RoBERTa"
  - "GPT-3"
  ml_motif:
  - "Commonsense question answering"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "2"
  notes: "Baseline 56%, human 89%"
  contact:
    name: "Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant"
    email: "Unknown"
  cite:
  - |
    @misc{talmor2019commonsenseqaquestionansweringchallenge,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00937}, 
    }
  datasets:
    links:
    - name: "CommonsenseQA Dataset (Hugging Face)"
      url: "https://huggingface.co/datasets/commonsense_qa"
  results:
    links:
    - name: "Papers With Code Leaderboard for CommonsenseQA"
      url: "https://paperswithcode.com/dataset/commonsenseqa"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 5
      reason: |
        All code given on Github site
    specification:
      rating: 4
      reason: |
        Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.
    metrics:
      rating: 5
      reason: |
        Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
    reference_solution:
      rating: 4
      reason: |
        Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not run with hardware constraints
    documentation:
      rating: 5
      reason: |
        Given in paper.

- date: "2019-07-24"
  version: "1"
  last_updated: "2019-07-24"
  expired: "false"
  valid: "yes"
  valid_date: "2019-07-24"
  name: "Winogrande"
  url: "https://leaderboard.allenai.org/winogrande/submissions/public"
  doi: "10.48550/arXiv.1907.10641"
  domain: "NLP; Commonsense"
  focus: "Winograd Schema-style pronoun resolution"
  keywords:
  - "adversarial"
  - "pronoun resolution"
  summary: |
    WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
    questions with reduced bias using AFLite, serving as both a benchmark and transfer 
    learning resource.
  licensing: "CC-BY"
  task_types:
  - "Pronoun resolution"
  ai_capability_measured:
  - "Robust commonsense reasoning"
  metrics:
  - "Accuracy"
  - "AUC"
  models:
  - "RoBERTa"
  - "BERT"
  - "GPT-2"
  ml_motif:
  - "Commonsense reasoning"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "2"
  notes: "Human ~94%"
  contact:
    name: "Keisuke Sakaguchi"
    email: "keisukes@allenai.org"
  cite:
  - |
    @misc{sakaguchi2019winograndeadversarialwinogradschema,
      archiveprefix = {arXiv},
      author        = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      eprint        = {1907.10641},
      primaryclass  = {cs.CL},
      title         = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
      url           = {https://arxiv.org/abs/1907.10641},
      year          = {2019}
    }
  datasets:
    links:
    - name: "Hugging Face / AllenAI"
      url: "https://huggingface.co/datasets/allenai/winogrande"
  results:
    links:
    - name: "Papers With Code leaderboard"
      url: "https://paperswithcode.com/dataset/winogrande"
  fair:
    reproducible: true
    benchmark_ready: true
  ratings:
    software:
      rating: 0
      reason: |
        No template code provided
    specification:
      rating: 5
      reason: |
        Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included.
    dataset:
      rating: 5
      reason: |
        Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata.
    metrics:
      rating: 5
      reason: |
        Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations.
    reference_solution:
      rating: 4
      reason: |
        Baseline results available, requiring users to submit their methods along with their submissions. Constraints are not required in submissions.
    documentation:
      rating: 5
      reason: |
        Dataset page and paper provide sufficient detail
