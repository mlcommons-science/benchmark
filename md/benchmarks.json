[
  {
    "date": "2020-09-07",
    "version": "1",
    "last_updated": "2020-09-07",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2025-07-28",
    "name": "MMLU (Massive Multitask Language Understanding)",
    "url": "https://huggingface.co/datasets/cais/mmlu",
    "doi": "10.48550/arXiv.2009.03300",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Academic knowledge and reasoning across 57 subjects",
    "keywords": [
      "multitask",
      "multiple-choice",
      "zero-shot",
      "few-shot",
      "knowledge probing"
    ],
    "summary": "Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 \nmultiple-choice tasks covering elementary mathematics, US history, computer science, \nlaw, and more, designed to evaluate a model's breadth and depth of knowledge in \nzero-shot and few-shot settings.\n",
    "licensing": "MIT License",
    "task_types": [
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "General reasoning, subject-matter understanding"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "GPT-4o",
      "Gemini 1.5 Pro",
      "o1",
      "DeepSeek-R1"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "1",
    "notes": "Good",
    "contact": {
      "name": "Dan Hendrycks",
      "email": "dan (at) safe.ai"
    },
    "cite": [
      "@misc{hendrycks2021measuring,\n  title={Measuring Massive Multitask Language Understanding},\n  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav},\n  journal={arXiv preprint arXiv:2009.03300},\n  year={2021},\n  url={https://arxiv.org/abs/2009.03300}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Huggingface Dataset",
          "url": "https://huggingface.co/datasets/cais/mmlu"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Measuring Massive Multitask Language Understanding - Test Leaderboard",
          "url": "https://github.com/hendrycks/test?tab=readme-ov-file#test-leaderboard"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 2,
        "reason": "Some code is available on github to reproduce results via OpenAI API, but not well documented\n"
      },
      "specification": {
        "rating": 4,
        "reason": "No system constraints\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Meets all FAIR principles and properly versioned.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Fully defined, represents a solution's performance.\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Reference models are available (i.e. GPT-3), but are not trainable or publicly documented\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Well-explained in a provided paper.\n"
      }
    },
    "id": "mmlu_massive_multitask_language_understanding"
  },
  {
    "date": "2023-07-19",
    "version": "1",
    "last_updated": "2023-07-19",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-07-19",
    "name": "ClimateLearn - Weather Forcasting",
    "url": "https://arxiv.org/abs/2307.01909",
    "doi": "10.48550/arXiv.2307.01909",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "ML for weather and climate modeling",
    "keywords": [
      "medium-range forecasting",
      "ERA5",
      "data-driven"
    ],
    "summary": "ClimateLearn provides standardized datasets and evaluation protocols for machine \nlearning models in medium-range weather and climate forecasting using ERA5 reanalysis.\n",
    "licensing": "CC-BY-4.0",
    "task_types": [
      "Forecasting"
    ],
    "ai_capability_measured": [
      "Global weather prediction (3-5 days)"
    ],
    "metrics": [
      "RMSE",
      "Anomaly correlation"
    ],
    "models": [
      "CNN baselines",
      "ResNet variants"
    ],
    "ml_motif": [
      "Sequence Prediction/Forecasting"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Multiple baseline models provided",
    "notes": "Includes physical and ML baselines.",
    "contact": {
      "name": "Jason Jewik",
      "email": "jason.jewik@ucla.edu"
    },
    "cite": [
      "@misc{nguyen2023climatelearnbenchmarkingmachinelearning, \n  title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, \n  author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},\n  year={2023}, eprint={2307.01909}, \n  archivePrefix={arXiv}, \n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2307.01909}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "ClimateLearn GitHub Repository (data loaders and processing)",
          "url": "https://github.com/aditya-grover/climate-learn"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "ClimateLearn Paper (results section)",
          "url": "https://arxiv.org/abs/2307.01909"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Quickstart notebook makes for easy usage\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A Quickstart notebook is provided that uses ResNet as a baseline model\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Explained in the benchmark's paper. \n"
      }
    },
    "id": "climatelearn_-_weather_forcasting"
  },
  {
    "date": "2023-07-19",
    "version": "1",
    "last_updated": "2023-07-19",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-07-19",
    "name": "ClimateLearn - Downscaling",
    "url": "https://arxiv.org/abs/2307.01909",
    "doi": "10.48550/arXiv.2307.01909",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "ML for weather and climate modeling",
    "keywords": [
      "medium-range forecasting",
      "ERA5",
      "data-driven"
    ],
    "summary": "ClimateLearn provides standardized datasets and evaluation protocols for machine \nlearning models in medium-range weather and climate forecasting using ERA5 reanalysis.\n",
    "licensing": "CC-BY-4.0",
    "task_types": [
      "Forecasting"
    ],
    "ai_capability_measured": [
      "Global weather prediction (3-5 days)"
    ],
    "metrics": [
      "RMSE",
      "Anomaly correlation"
    ],
    "models": [
      "CNN baselines",
      "ResNet variants"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Multiple baseline models provided",
    "notes": "Includes physical and ML baselines.",
    "contact": {
      "name": "Jason Jewik",
      "email": "jason.jewik@ucla.edu"
    },
    "cite": [
      "@misc{nguyen2023climatelearnbenchmarkingmachinelearning, \n  title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, \n  author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},\n  year={2023}, eprint={2307.01909}, \n  archivePrefix={arXiv}, \n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2307.01909}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "ClimateLearn GitHub Repository (data loaders and processing)",
          "url": "https://github.com/aditya-grover/climate-learn"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "ClimateLearn Paper (results section)",
          "url": "https://arxiv.org/abs/2307.01909"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Quickstart notebook makes for easy usage\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A Quickstart notebook is provided that uses ResNet as a baseline model\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Explained in the benchmark's paper. \n"
      }
    },
    "id": "climatelearn_-_downscaling"
  },
  {
    "date": "2023-07-19",
    "version": "1",
    "last_updated": "2023-07-19",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-07-19",
    "name": "ClimateLearn - Climate Projection",
    "url": "https://arxiv.org/abs/2307.01909",
    "doi": "10.48550/arXiv.2307.01909",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "ML for weather and climate modeling",
    "keywords": [
      "medium-range forecasting",
      "ERA5",
      "data-driven"
    ],
    "summary": "ClimateLearn provides standardized datasets and evaluation protocols for machine \nlearning models in medium-range weather and climate forecasting using ERA5 reanalysis.\n",
    "licensing": "CC-BY-4.0",
    "task_types": [
      "Forecasting"
    ],
    "ai_capability_measured": [
      "Global weather prediction (3-5 days)"
    ],
    "metrics": [
      "RMSE",
      "Anomaly correlation"
    ],
    "models": [
      "CNN baselines",
      "ResNet variants"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Multiple baseline models provided",
    "notes": "Includes physical and ML baselines.",
    "contact": {
      "name": "Jason Jewik",
      "email": "jason.jewik@ucla.edu"
    },
    "cite": [
      "@misc{nguyen2023climatelearnbenchmarkingmachinelearning, \n  title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, \n  author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},\n  year={2023}, eprint={2307.01909}, \n  archivePrefix={arXiv}, \n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2307.01909}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "ClimateLearn GitHub Repository (data loaders and processing)",
          "url": "https://github.com/aditya-grover/climate-learn"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "ClimateLearn Paper (results section)",
          "url": "https://arxiv.org/abs/2307.01909"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Quickstart notebook makes for easy usage\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A Quickstart notebook is provided that uses ResNet as a baseline model\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Explained in the benchmark's paper. \n"
      }
    },
    "id": "climatelearn_-_climate_projection"
  },
  {
    "date": "2024-10-01",
    "version": "1",
    "last_updated": "2024-10-01",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2024-10-01",
    "name": "CFDBench (Fluid Dynamics)",
    "url": "https://arxiv.org/abs/2310.05963",
    "doi": "10.48550/arXiv.2310.05963",
    "domain": [
      "Mathematics"
    ],
    "focus": "Neural operator surrogate modeling",
    "keywords": [
      "neural operators",
      "CFD",
      "FNO",
      "DeepONet"
    ],
    "summary": "CFDBench provides large-scale CFD data for four canonical fluid flow problems, \nassessing neural operators' ability to generalize to unseen PDE parameters and domains.\n",
    "licensing": "CC-BY-4.0",
    "task_types": [
      "Surrogate modeling"
    ],
    "ai_capability_measured": [
      "Generalization of neural operators for PDEs"
    ],
    "metrics": [
      "L2 error",
      "MAE"
    ],
    "models": [
      "FNO",
      "DeepONet",
      "U-Net"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Numerous, as it's a benchmark for ML models",
    "notes": "302K frames across 739 cases",
    "contact": {
      "name": "Yining Luo",
      "email": "yining.luo@mail.utoronto.ca"
    },
    "cite": [
      "@misc{luo2024cfdbenchlargescalebenchmarkmachine,\n  title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},\n  author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},\n  year={2024},\n  url={https://arxiv.org/abs/2310.05963}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation\n"
      },
      "specification": {
        "rating": 0,
        "reason": "Not listed\n"
      },
      "dataset": {
        "rating": 0,
        "reason": "Not given\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "Baseline models like FNO and DeepONet are implemented, hardware specified.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Associated paper gives all necessary information.\n"
      }
    },
    "id": "cfdbench_fluid_dynamics"
  },
  {
    "date": "2023-04-23",
    "version": "1",
    "last_updated": "2023-04-23",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-04-23",
    "name": "SatImgNet",
    "url": "https://satinbenchmark.github.io/",
    "doi": "10.48550/arXiv.2304.11619",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Satellite imagery classification",
    "keywords": [
      "land-use",
      "zero-shot",
      "multi-task"
    ],
    "summary": "SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite\nimagery classification datasets evaluating zero-shot transfer of vision-language models\nacross diverse remote sensing tasks.\n",
    "licensing": "CC-BY-4.0",
    "task_types": [
      "Image classification"
    ],
    "ai_capability_measured": [
      "Zero-shot land-use classification"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "CLIP",
      "BLIP",
      "ALBEF"
    ],
    "ml_motif": [
      "Multimodal Reasoning"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Numerous, evaluated via leaderboard",
    "notes": "Public leaderboard available",
    "contact": {
      "name": "Jonathan Roberts",
      "email": "j.roberts@cs.ox.ac.uk"
    },
    "cite": [
      "@article{roberts2023satin,\nauthor = \"Roberts, Jonathan and Han, Kai and Albanie, Samuel\",\ntitle = \"Satin: A multi-task metadataset for classifying satellite imagery using vision-language models\",\nyear = \"2023\",\nmonth = \"3\",\njournal = \"ICCV Workshop: Towards the Next Generation of Computer Vision Datasets\",\ndoi = \"10.48550/arXiv.2304.11619\"\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "SatImgNet on Hugging Face",
          "url": "https://huggingface.co/datasets/jonathan-roberts1/SATIN"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "SatImgNet Leaderboard",
          "url": "https://satinbenchmark.github.io/_pages/leaderboard/"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 0,
        "reason": "No scripts or environment information provided\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Accuracy of classification is an appropriate metric\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Paper provides all required information\n"
      }
    },
    "id": "satimgnet"
  },
  {
    "date": "2022-02-22",
    "version": "1",
    "last_updated": "2022-02-22",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2022-02-22",
    "name": "Quantum Computing Benchmarks (QML)",
    "url": "https://github.com/XanaduAI/qml-benchmarks",
    "doi": "10.48550/arXiv.2403.07059",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Quantum algorithm performance evaluation",
    "keywords": [
      "quantum circuits",
      "state preparation",
      "error correction"
    ],
    "summary": "A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state \npreparation, circuit optimization, and error correction across multiple platforms.\n",
    "licensing": "Apache-2.0",
    "task_types": [
      "Circuit benchmarking",
      "State classification"
    ],
    "ai_capability_measured": [
      "Quantum algorithm performance and fidelity"
    ],
    "metrics": [
      "Fidelity",
      "Success probability"
    ],
    "models": [
      "IBM Q",
      "IonQ",
      "AQT@LBNL"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Varies per benchmark",
    "notes": "Hardware-agnostic, application-level metrics. The citation may not be correct.",
    "contact": {
      "name": "Xanadu AI",
      "email": "support@xanadu.ai"
    },
    "cite": [
      "@misc{bowles2024betterclassicalsubtleart,\n  title={Better than classical? The subtle art of benchmarking quantum machine learning models}, \n  author={Joseph Bowles and Shahnawaz Ahmed and Maria Schuld},\n  year={2024},\n  eprint={2403.07059},\n  archivePrefix={arXiv},\n  primaryClass={quant-ph},\n  url={https://arxiv.org/abs/2403.07059}, \n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "PennyLane QML Benchmarks Datasets",
          "url": "https://pennylane.ai/datasets/collection/qml-benchmarks"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "QML Benchmarks GitHub Repository (Results section)",
          "url": "https://github.com/XanaduAI/qml-benchmarks#results-and-leaderboards"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 4,
        "reason": "Software is built upon multiple common frameworks for simulation, training, and benchmarking workflows.\n"
      },
      "specification": {
        "rating": 3,
        "reason": "No system constraints. Task clarity and dataset format are not clearly specified.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Datasets are accessible, but not split.\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured.\n"
      },
      "reference_solution": {
        "rating": 0,
        "reason": "Not provided\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Paper is available with all required information. \n"
      }
    },
    "id": "quantum_computing_benchmarks_qml"
  },
  {
    "date": "2020-10-20",
    "version": "1",
    "last_updated": "2020-10-20",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2020-10-20",
    "name": "OCP (Open Catalyst Project)",
    "url": "https://opencatalystproject.org/",
    "doi": "unknown",
    "domain": [
      "Chemistry",
      "Materials Science"
    ],
    "focus": "Catalyst adsorption energy prediction",
    "keywords": [
      "DFT relaxations",
      "adsorption energy",
      "graph neural networks"
    ],
    "summary": "The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate \nrelaxation datasets, challenging ML models to predict energies and forces for \nrenewable energy applications.\n",
    "licensing": "OCP Terms of Use",
    "task_types": [
      "Energy prediction",
      "Force prediction"
    ],
    "ai_capability_measured": [
      "Prediction of adsorption energies and forces"
    ],
    "metrics": [
      "MAE (energy)",
      "MAE (force)"
    ],
    "models": [
      "CGCNN",
      "SchNet",
      "DimeNet++",
      "GemNet-OC"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Public leaderboards; active community development",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@article{chanussot2021oc20,\n  title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},\n  author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},\n  journal   = {ACS Catalysis},\n  volume    = {11},\n  number    = {10},\n  pages     = {6059--6072},\n  year      = {2021},\n  doi       = {10.1021/acscatal.0c04525},\n  url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}\n}\n",
      "@article{tran2023oc22,\n  title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},\n  author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, F\\'{e}lix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},\n  journal   = {ACS Catalysis},\n  volume    = {13},\n  number    = {5},\n  pages     = {3066--3084},\n  year      = {2023},\n  doi       = {10.1021/acscatal.2c05426},\n  url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}\n}\n",
      "@article{doi:10.1021/acscatal.0c04525,\n  author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},\n  title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},\n  journal = {ACS Catalysis},\n  volume = {11},\n  number = {10},\n  pages = {6059-6072},\n  year = {2021},\n  doi = {10.1021/acscatal.0c04525},\n  URL = {https://doi.org/10.1021/acscatal.0c04525},\n  eprint = {https://doi.org/10.1021/acscatal.0c04525}\n}\n",
      "@article{tran2023b,\n  title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},\n  volume={13},\n  ISSN={2155-5435},\n  url={http://dx.doi.org/10.1021/acscatal.2c05426},\n  DOI={10.1021/acscatal.2c05426},\n  number={5},\n  journal={ACS Catalysis},\n  publisher={American Chemical Society (ACS)},\n  author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, F\\'{e}lix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},\n  year={2023},\n  month=feb, pages={3066-3084} \n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "OCP Dataset",
          "url": "https://fair-chem.github.io/catalysts/datasets/summary"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "OCP Pretrained Models",
          "url": "https://fair-chem.github.io/catalysts/models.html"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Data provided in Github links\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "MAE (energy and force) are standard and reproducible.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed.\n"
      },
      "documentation": {
        "rating": 1,
        "reason": "Paper exists, but content is behind a paywall.\n"
      }
    },
    "id": "ocp_open_catalyst_project"
  },
  {
    "date": "2020-09-28",
    "version": "1",
    "last_updated": "2020-09-28",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2020-09-28",
    "name": "MedQA",
    "url": "https://arxiv.org/abs/2009.13081",
    "doi": "10.48550/arXiv.2009.13081",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Medical board exam QA",
    "keywords": [
      "USMLE",
      "diagnostic QA",
      "medical knowledge",
      "multilingual"
    ],
    "summary": "MedQA is a large-scale multiple-choice dataset drawn from professional medical\nboard exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge \nquestions in English and Chinese.\n",
    "licensing": "Under Association for the Advancement of Artificial Intelligence",
    "task_types": [
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "Medical diagnosis and knowledge retrieval"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "Neural reader",
      "Retrieval-based QA systems"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Multilingual (English, Simplified and Traditional Chinese)",
    "contact": {
      "name": "Di Jin",
      "email": "jindi15@mit.edu"
    },
    "cite": [
      "@misc{jin2020diseasedoespatienthave,\n    archiveprefix = {arXiv},\n    author        = {Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},\n    eprint        = {2009.13081},\n    primaryclass  = {cs.CL},\n    title         = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},\n    url           = {https://arxiv.org/abs/2009.13081},\n    year          = {2020}\n  }\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Github",
          "url": "https://github.com/jind11/MedQA"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "All code available on the github\n"
      },
      "specification": {
        "rating": 3,
        "reason": "Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.\n"
      },
      "reference_solution": {
        "rating": 0,
        "reason": "No reference solution mentioned.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper is available. Evaluation criteria are not mentioned.\n"
      }
    },
    "id": "medqa"
  },
  {
    "date": "2011-10-01",
    "version": "1",
    "last_updated": "2011-10-01",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2011-10-01",
    "name": "Materials Project",
    "url": "https://materialsproject.org/",
    "doi": "unknown",
    "domain": [
      "Materials Science"
    ],
    "focus": "DFT-based property prediction",
    "keywords": [
      "DFT",
      "materials genome",
      "high-throughput"
    ],
    "summary": "The Materials Project provides an open-access database of computed properties for\ninorganic materials via high-throughput density functional theory (DFT), accelerating \nmaterials discovery.\n",
    "licensing": "https://next-gen.materialsproject.org/about/terms",
    "task_types": [
      "Property prediction"
    ],
    "ai_capability_measured": [
      "Prediction of inorganic material properties"
    ],
    "metrics": [
      "MAE",
      "R^2"
    ],
    "models": [
      "Automatminer",
      "Crystal Graph Neural Networks"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Core component of the Materials Genome Initiative",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@article{jain2013materials,\n  title={The Materials Project: A materials genome approach},\n  author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},\n  journal={APL Materials},\n  volume    = {1},\n  number    = {1},\n  year={2013},\n  doi       = {10.1063/1.4812323},\n  url={https://materialsproject.org/}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Materials Project Catalysis Explorer",
          "url": "https://next-gen.materialsproject.org/catalysis"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 0,
        "reason": "No instructions available\n"
      },
      "specification": {
        "rating": 1.5,
        "reason": "The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.\n"
      },
      "dataset": {
        "rating": 3,
        "reason": "API key required to access data. No predefined splits.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses numerical metrics like MAE and $R^2$\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed.\n"
      },
      "documentation": {
        "rating": 0,
        "reason": "No explanations or paper provided\n"
      }
    },
    "id": "materials_project"
  },
  {
    "date": "2023-11-20",
    "version": "1",
    "last_updated": "2023-11-20",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-11-20",
    "name": "GPQA Diamond",
    "url": "https://arxiv.org/abs/2311.12022",
    "doi": "10.48550/arXiv.2311.12022",
    "domain": [
      "Biology & Medicine",
      "Chemistry",
      "High Energy Physics"
    ],
    "focus": "Graduate-level scientific reasoning",
    "keywords": [
      "Google-proof",
      "graduate-level",
      "science QA",
      "chemistry",
      "physics"
    ],
    "summary": "GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,\nand chemistry, written by domain experts. It is Google-proof - experts score 65% \n(74% after error correction) while skilled non-experts with web access score only 34%. \nState-of-the-art LLMs like GPT-4 reach around 39% accuracy.\n",
    "licensing": "unknown",
    "task_types": [
      "Multiple choice",
      "Multi-step QA"
    ],
    "ai_capability_measured": [
      "Scientific reasoning, deep knowledge"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "o1",
      "DeepSeek-R1"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Good",
    "contact": {
      "name": "Julian Michael",
      "email": "julianjm@nyu.edu"
    },
    "cite": [
      "@misc{rein2023gpqagraduatelevelgoogleproofqa,\n  title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},\n  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},\n  year={2023},\n  url={https://arxiv.org/abs/2311.12022}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Python version and requirements specified on Github site\n"
      },
      "specification": {
        "rating": 2,
        "reason": "No system constraints or I/O specified\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Easily able to access dataset. Comes with predefined splits as mentioned in the paper\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Each question has a correct answer, representing the tested model's performance.\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "Common models such as GPT-3.5 were compared. They are not open and don't provide requirements\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "All information is listed in the associated paper\n"
      }
    },
    "id": "gpqa_diamond"
  },
  {
    "date": "2018-03-14",
    "version": "1",
    "last_updated": "2018-03-14",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2018-03-14",
    "name": "ARC-Challenge (Advanced Reasoning Challenge)",
    "url": "https://allenai.org/data/arc",
    "doi": "10.48550/arXiv.1803.05457",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Grade-school science with reasoning emphasis",
    "keywords": [
      "grade-school",
      "science QA",
      "challenge set",
      "reasoning"
    ],
    "summary": "The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school\nscience questions that retrieval-based and word co-occurrence algorithms both fail, \nrequiring advanced reasoning over a 14-million-sentence corpus.\n",
    "licensing": "Apache 2.0 License",
    "task_types": [
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "Commonsense and scientific reasoning"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "GPT-4",
      "Claude"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Good",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@article{allenai:arc,\n  author    = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and\n                Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},\n  title     = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},\n  journal   = {arXiv:1803.05457v1},\n  year      = {2018},\n  doi       = {10.48550/arXiv.1803.05457}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Hugging Face",
          "url": "https://huggingface.co/datasets/allenai/ai2_arc"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "ARC-Solvers",
          "url": "https://github.com/allenai/arc-solvers"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Code is available and well documented for evaluation.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task is clear and inputs/outputs are provided along with format on dataset card.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Data accessible, offers instructions on how to download the data via CLI tools. Splits provided on Huggingface\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "All questions in the dataset are multiple choice, all have a correct answer\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "Reference solution is available and containerized\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Explains all necessary information inside a paper\n"
      }
    },
    "id": "arc-challenge_advanced_reasoning_challenge"
  },
  {
    "date": "2024-11-07",
    "version": "1",
    "last_updated": "2024-11-07",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2024-11-07",
    "name": "FrontierMath",
    "url": "https://arxiv.org/abs/2411.04872",
    "doi": "10.48550/arXiv.2411.04872",
    "domain": [
      "Mathematics"
    ],
    "focus": "Challenging advanced mathematical reasoning",
    "keywords": [
      "symbolic reasoning",
      "number theory",
      "algebraic geometry",
      "category theory"
    ],
    "summary": "FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning\nnumber theory, real analysis, algebraic geometry, and category theory, measuring LLMs \nability to solve problems requiring deep abstract reasoning.\n",
    "licensing": "unknown",
    "task_types": [
      "Problem solving"
    ],
    "ai_capability_measured": [
      "Symbolic and abstract mathematical reasoning"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "unknown"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "More information available at https://epoch.ai/frontiermath/about",
    "contact": {
      "name": "FrontierMath team",
      "email": "math_evals@epochai.org"
    },
    "cite": [
      "@misc{glazer2024frontiermathbenchmarkevaluatingadvanced,\n  archiveprefix = {arXiv},\n  author        = {Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli J\\\"{a}rviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},\n  eprint        = {2411.04872},\n  primaryclass  = {cs.AI},\n  title         = {FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},\n  url           = {https://arxiv.org/abs/2411.04872},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "No",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 0,
        "reason": "No publicaly available code to run the benchmark\n"
      },
      "specification": {
        "rating": 3,
        "reason": "Well-specified process for asking questions and receiving answers. No software or hardware constraints\n"
      },
      "dataset": {
        "rating": 0,
        "reason": "Only samples of dataset exist, not publicly available\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "All questions in the dataset have a correct answer\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Displays result of leading models on the benchmark, but none are trainable or list constraints\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "All necessary information is in the paper and website\n"
      }
    },
    "id": "frontiermath"
  },
  {
    "date": "2024-07-18",
    "version": "1",
    "last_updated": "2024-07-18",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2024-07-18",
    "name": "SciCode",
    "url": "https://scicode-bench.github.io/",
    "doi": "10.48550/arXiv.2407.13168",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Scientific code generation and problem solving",
    "keywords": [
      "code synthesis",
      "scientific computing",
      "programming benchmark"
    ],
    "summary": "SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80\nreal research tasks across 16 scientific subfields, evaluating models on knowledge recall, \nreasoning, and code synthesis for scientific computing tasks.\n",
    "licensing": "unknown",
    "task_types": [
      "Coding"
    ],
    "ai_capability_measured": [
      "Program synthesis, scientific computing"
    ],
    "metrics": [
      "Solve rate (%)"
    ],
    "models": [
      "Claude3.5-Sonnet"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "unknown",
    "notes": "Good",
    "contact": {
      "name": "Minyang Tian",
      "email": "mtian8@illinois.edu"
    },
    "cite": [
      "@misc{tian2024scicoderesearchcodingbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},\n  eprint        = {2407.13168},\n  primaryclass  = {cs.AI},\n  title         = {SciCode: A Research Coding Benchmark Curated by Scientists},\n  url           = {https://arxiv.org/abs/2407.13168},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "SciCode on Huggingface",
          "url": "https://huggingface.co/datasets/SciCode1/SciCode"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "SciCode Learderboard",
          "url": "https://scicode-bench.github.io/leaderboard/"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Code to run exists on github repo\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset meets all FAIR principles, test and validation splits are available (no train split)\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Metrics stated, grading guidelines are provided in repo (problems are pass/fail)\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "Code to evaluate is available and well documented. Baseline models include closed and open weight models\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper containing all needed info except for evlauation criteria\n"
      }
    },
    "id": "scicode"
  },
  {
    "date": "2025-03-13",
    "version": "1",
    "last_updated": "2025-03-13",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2025-03-13",
    "name": "AIME (American Invitational Mathematics Examination)",
    "url": "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions",
    "doi": "NA",
    "domain": [
      "Mathematics"
    ],
    "focus": "Pre-college advanced problem solving",
    "keywords": [
      "algebra",
      "combinatorics",
      "number theory",
      "geometry"
    ],
    "summary": "The AIME is a 15-question, 3-hour exam for high-school students featuring challenging\nshort-answer math problems in algebra, number theory, geometry, and combinatorics, \nassessing depth of problem-solving ability.\n",
    "licensing": "unknown",
    "task_types": [
      "Problem solving"
    ],
    "ai_capability_measured": [
      "Mathematical problem-solving and reasoning"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "unknown"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Designed for human test-takers",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@misc{www-aime,\n  author = {TBD},\n  title = {AIME},\n  url = {https://www.vals.ai/benchmarks/aime-2025-03-13},\n  month = mar,\n  year = 2025,\n  note = {[Online accessed 2025-06-24]}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "AoPS website",
          "url": "https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 0,
        "reason": "No code available\n"
      },
      "specification": {
        "rating": 3,
        "reason": "Task and Inputs/Outputs are well specified. No system constraints or dataset format is mentioned\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Easily accessible data with problems and solutions, but no splits\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Correctness is measured, but no grading guidelines are provided.\n"
      },
      "reference_solution": {
        "rating": 0,
        "reason": "Not given. Human performance stats exist, but no mentions of AI performance\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Some background and other information is provided, but it is not comprehensive. No info on how to run an evaluation\n"
      }
    },
    "id": "aime_american_invitational_mathematics_examination"
  },
  {
    "date": "2023-05-30",
    "version": "1",
    "last_updated": "2023-05-30",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-05-30",
    "name": "PRM800K",
    "url": "https://github.com/openai/prm800k/tree/main",
    "doi": "10.48550/arXiv.2305.20050",
    "domain": [
      "Mathematics"
    ],
    "focus": "Math reasoning generalization",
    "keywords": [
      "calculus",
      "algebra",
      "number theory",
      "geometry"
    ],
    "summary": "PRM800K is a process supervision dataset containing 800,000 step-level correctness labels for model-generated solutions to problems from the MATH dataset.\n",
    "licensing": "MIT License",
    "task_types": [
      "Problem solving"
    ],
    "ai_capability_measured": [
      "Math reasoning and generalization"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "GPT-4"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Reasoning"
    ],
    "solutions": "0",
    "notes": "Math problems & Annotated reasoning steps based off of Dan Hendrycks' MATH dataset",
    "contact": {
      "name": "Karl Cobbe",
      "email": "karl@openai.com"
    },
    "cite": [
      "@article{lightman2023lets,\n      title={Let's Verify Step by Step}, \n      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},\n      journal={arXiv preprint arXiv:2305.20050},\n      year={2023},\n      Eprint = {arXiv:2305.20050},\n      doi = {10.48550/arXiv.2305.20050}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "PRM800K: A Process Supervision Dataset",
          "url": "https://github.com/openai/prm800k/tree/main"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Let's Verify Step by Step",
          "url": "https://arxiv.org/abs/2305.20050"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Code is provided in the PRM800K Repo for evaluation and grading, documentation is present but no environment details, baseline model, or training code is given\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task is well specified, format, inputs, and outputs are mentioned. No system constraints are provided.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset follows all FAIR Principles. Train/Test splits are available in the PRM800K repo\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Correctness is used as the primary metric, with grading guidelines provided.\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "A reference solution is mentioned in the \"Lets Verify Step by Step\" paper, but the model is not open-sourced.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Documentation is present in the PRM800K repo and \"Lets Verify Step by Step\" paper.\n"
      }
    },
    "id": "prmk"
  },
  {
    "date": "2024-04-02",
    "version": "1",
    "last_updated": "2024-04-02",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2024-04-02",
    "name": "CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)",
    "url": "https://arxiv.org/abs/2503.13517",
    "doi": "10.48550/arXiv.2503.13517",
    "domain": [
      "Materials Science",
      "High Energy Physics",
      "Biology & Medicine",
      "Chemistry",
      "Climate & Earth Science"
    ],
    "focus": "Long-context scientific reasoning",
    "keywords": [
      "long-context",
      "information extraction",
      "multimodal"
    ],
    "summary": "CURIE is a benchmark of 580 problems across six scientific disciplines-materials\nscience, quantum computing, biology, chemistry, climate science, and astrophysics-\ndesigned to evaluate LLMs on long-context understanding, reasoning, and information \nextraction in realistic scientific workflows.\n",
    "licensing": "Apache 2.0 License",
    "task_types": [
      "Information extraction",
      "Reasoning",
      "Concept tracking",
      "Aggregation",
      "Algebraic manipulation",
      "Multimodal comprehension"
    ],
    "ai_capability_measured": [
      "Long-context understanding and scientific reasoning"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "unkown"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Good",
    "contact": {
      "name": "Subhashini Venugopalan",
      "email": "vsubhashini@google.com"
    },
    "cite": [
      "@misc{cui2025curieevaluatingllmsmultitask,\n  title={CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning}, \n  author={Hao Cui and Zahra Shamsi and Gowoon Cheon and Xuejian Ma and Shutong Li and Maria Tikhanovskaya and Peter Norgaard and Nayantara Mudur and Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and Victor V. Albert and Pranesh Srinivasan and Haining Pan and Philippe Faist and Brian Rohr and Ekin Dogus Cubuk and Muratahan Aykol and Amil Merchant and Michael J. Statt and Dan Morris and Drew Purves and Elise Kleeman and Ruth Alcantara and Matthew Abraham and Muqthar Mohammad and Ean Phing VanLee and Chenfei Jiang and Elizabeth Dorfman and Eun-Ah Kim and Michael P Brenner and Viren Jain and Sameera Ponda and Subhashini Venugopalan},\n  year={2025},\n  eprint={2503.13517},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2503.13517}, \n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 4,
        "reason": "Code is available, but not well documented\n"
      },
      "specification": {
        "rating": 1,
        "reason": "Explains types of problems in detail, but does not state exactly how to administer them.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Dataset is available via Github, but hard to find\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem.\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "Exists, but is not open\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Associated paper explains all criteria\n"
      }
    },
    "id": "curie_scientific_long-context_understanding_reasoning_and_information_extraction"
  },
  {
    "date": "2023-01-26",
    "version": "1",
    "last_updated": "2023-01-26",
    "expired": "false",
    "valid": "no",
    "valid_date": "2023-01-26",
    "name": "FEABench (Finite Element Analysis Benchmark): Evaluating Language Models on Multiphysics Reasoning Ability",
    "url": "https://github.com/google/feabench",
    "doi": "unknown",
    "domain": [
      "Mathematics"
    ],
    "focus": "FEA simulation accuracy and performance",
    "keywords": [
      "finite element",
      "simulation",
      "PDE"
    ],
    "summary": "N/A\n",
    "licensing": "unknown",
    "task_types": [
      "Simulation",
      "Performance evaluation"
    ],
    "ai_capability_measured": [
      "Numerical simulation accuracy and efficiency"
    ],
    "metrics": [
      "Solve time",
      "Error norm"
    ],
    "models": [
      "FEniCS",
      "deal.II"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "unknown",
    "notes": "OK",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@misc{mudur2025feabenchevaluatinglanguagemodels,\n  title={FEABench: Evaluating Language Models on Multiphysics Reasoning Ability}, \n  author={Nayantara Mudur and Hao Cui and Subhashini Venugopalan and Paul Raccuglia and Michael P. Brenner and Peter Norgaard},\n  year={2025},\n  eprint={2504.06260},\n  archivePrefix={arXiv},\n  primaryClass={cs.AI},\n  url={https://arxiv.org/abs/2504.06260}, \n  }\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "FEABench Github",
          "url": "https://github.com/google/feabench?tab=readme-ov-file#datasets"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 4,
        "reason": "Code is available, but poorly documented\n"
      },
      "specification": {
        "rating": 1,
        "reason": "Output is defined and task clarity is questionable\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Available, but not split into sets\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Fully defined metrics\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Three open-source models were used. No system constraints.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "In associated paper\n"
      }
    },
    "id": "feabench_finite_element_analysis_benchmark_evaluating_language_models_on_multiphysics_reasoning_ability"
  },
  {
    "date": "2024-07-12",
    "version": "1",
    "last_updated": "2024-07-12",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2024-07-12",
    "name": "SPIQA (Scientific Paper Image Question Answering)",
    "url": "https://arxiv.org/abs/2407.09413",
    "doi": "10.48550/arXiv.2407.09413",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Multimodal QA on scientific figures",
    "keywords": [
      "multimodal QA",
      "figure understanding",
      "table comprehension",
      "chain-of-thought"
    ],
    "summary": "SPIQA assesses AI models' ability to interpret and answer questions about figures\nand tables in scientific papers by integrating visual and textual modalities \nwith chain-of-thought reasoning.\n",
    "licensing": "Apache 2.0 License",
    "task_types": [
      "Question answering",
      "Multimodal QA",
      "Chain-of-Thought evaluation"
    ],
    "ai_capability_measured": [
      "Visual-textual reasoning in scientific contexts"
    ],
    "metrics": [
      "Accuracy",
      "F1 score"
    ],
    "models": [
      "Chain-of-Thought models",
      "Multimodal QA systems"
    ],
    "ml_motif": [
      "Multimodal Reasoning"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Good",
    "contact": {
      "name": "Subhashini Venugopalan",
      "email": "vsubhashini@google.com"
    },
    "cite": [
      "@misc{zhong2024spiqa,\n  title={SPIQA: Scientific Paper Image Question Answering},\n  author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},\n  year={2024},\n  url={https://arxiv.org/abs/2407.09413}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Hugging Face",
          "url": "https://huggingface.co/datasets/google/spiqa"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 0,
        "reason": "Not provided\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses quantitative metrics (Accuracy, F1) aligned with the task\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "All information provided in paper\n"
      }
    },
    "id": "spiqa_scientific_paper_image_question_answering"
  },
  {
    "date": "2025-05-13",
    "version": "1",
    "last_updated": "2025-05-13",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2025-05-13",
    "name": "BaisBench (Biological AI Scientist Benchmark) - Question Answering",
    "url": "https://arxiv.org/abs/2505.08341",
    "doi": "10.48550/arXiv.2505.08341",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Omics-driven AI research tasks",
    "keywords": [
      "single-cell annotation",
      "biological QA",
      "autonomous discovery"
    ],
    "summary": "BaisBench evaluates AI scientists' ability to perform data-driven biological research\nby annotating cell types in single-cell datasets and answering MCQs derived from \nbiological study insights, measuring autonomous scientific discovery.\n",
    "licensing": "MIT License",
    "task_types": [
      "Cell type annotation",
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "Autonomous biological research capabilities"
    ],
    "metrics": [
      "Annotation accuracy",
      "QA accuracy"
    ],
    "models": [
      "LLM-based AI scientist agents"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Underperforms human experts; aims to advance AI-driven discovery",
    "contact": {
      "name": "Xuegong Zhang",
      "email": "zhangxg@mail.tsinghua.edu.cn"
    },
    "cite": [
      "@misc{luo2025benchmarkingaiscientistsomics,\n  archiveprefix = {arXiv},\n  author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},\n  eprint        = {2505.08341},\n  primaryclass  = {cs.AI},\n  title         = {Benchmarking AI scientists in omics data-driven biological research},\n  url           = {https://arxiv.org/abs/2505.08341},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Github",
          "url": "https://github.com/EperLuo/BaisBench"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Instructions for environment setup available\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.\n"
      },
      "reference_solution": {
        "rating": 0,
        "reason": "Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Dataset and paper accessible; IPYNB files for setup are available on the github repo.\n"
      }
    },
    "id": "baisbench_biological_ai_scientist_benchmark_-_question_answering"
  },
  {
    "date": "2025-05-13",
    "version": "1",
    "last_updated": "2025-05-13",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2025-05-13",
    "name": "BaisBench (Biological AI Scientist Benchmark) - Cell Type Annotation",
    "url": "https://arxiv.org/abs/2505.08341",
    "doi": "10.48550/arXiv.2505.08341",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Omics-driven AI research tasks",
    "keywords": [
      "single-cell annotation",
      "biological QA",
      "autonomous discovery"
    ],
    "summary": "BaisBench evaluates AI scientists' ability to perform data-driven biological research\nby annotating cell types in single-cell datasets and answering MCQs derived from \nbiological study insights, measuring autonomous scientific discovery.\n",
    "licensing": "MIT License",
    "task_types": [
      "Cell type annotation",
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "Autonomous biological research capabilities"
    ],
    "metrics": [
      "Annotation accuracy",
      "QA accuracy"
    ],
    "models": [
      "LLM-based AI scientist agents"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Underperforms human experts; aims to advance AI-driven discovery",
    "contact": {
      "name": "Xuegong Zhang",
      "email": "zhangxg@mail.tsinghua.edu.cn"
    },
    "cite": [
      "@misc{luo2025benchmarkingaiscientistsomics,\n  archiveprefix = {arXiv},\n  author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},\n  eprint        = {2505.08341},\n  primaryclass  = {cs.AI},\n  title         = {Benchmarking AI scientists in omics data-driven biological research},\n  url           = {https://arxiv.org/abs/2505.08341},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Github",
          "url": "https://github.com/EperLuo/BaisBench"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Instructions for environment setup available\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.\n"
      },
      "reference_solution": {
        "rating": 0,
        "reason": "Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Dataset and paper accessible; IPYNB files for setup are available on the github repo.\n"
      }
    },
    "id": "baisbench_biological_ai_scientist_benchmark_-_cell_type_annotation"
  },
  {
    "date": "2024-12-17",
    "version": "1",
    "last_updated": "2023-01-26",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2023-01-26",
    "name": "MOLGEN",
    "url": "https://github.com/zjunlp/MolGen",
    "doi": "10.48550/arXiv.2301.11259",
    "domain": [
      "Chemistry"
    ],
    "focus": "Molecular generation and optimization",
    "keywords": [
      "SELFIES",
      "GAN",
      "property optimization"
    ],
    "summary": "MolGen is a pre-trained molecular language model that generates chemically valid\nmolecules using SELFIES and reinforcement learning, guided by chemical feedback \nto optimize properties such as logP, QED, and docking score.\n",
    "licensing": "MIT License",
    "task_types": [
      "Distribution learning",
      "Goal-oriented generation"
    ],
    "ai_capability_measured": [
      "Generation of valid and optimized molecular structures"
    ],
    "metrics": [
      "Validity%",
      "Novelty%",
      "QED",
      "Docking score",
      "penalized logP"
    ],
    "models": [
      "MolGen"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "",
    "contact": {
      "name": "zhangningyu@zju.edu.cn",
      "email": "Ningyu Zhang"
    },
    "cite": [
      "@misc{fang2024domainagnosticmoleculargenerationchemical,\n  archiveprefix = {arXiv},\n  author        = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},\n  eprint        = {2301.11259},\n  primaryclass  = {cs.LG},\n  title         = {Domain-Agnostic Molecular Generation with Chemical Feedback},\n  url           = {https://arxiv.org/abs/2301.11259},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "MolGen: A Pre-trained Molecular Language Model",
          "url": "https://github.com/zjunlp/MolGen/tree/main"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Domain-Agnostic Molecular Generation with Chemical Feedback",
          "url": "https://arxiv.org/abs/2301.11259"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Code is available on the github repo, along with instructions to run the model and reproduce results.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task, datset format, and input/output formats are well specified. No system constraints are mentioned.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset and train/test splits are available through the github repo, as well as mentions of source datasets in the paper.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Metrics are well defined and appropriate for the task\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A pretrained model is provided, as well as training code and instructions\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "All necessary information is provided in the paper and github repo\n"
      }
    },
    "id": "molgen"
  },
  {
    "date": "2020-05-02",
    "version": "1",
    "last_updated": "2020-05-02",
    "expired": "false",
    "valid": "yes",
    "valid_date": "2020-05-02",
    "name": "Open Graph Benchmark (OGB) - Biology",
    "url": "https://ogb.stanford.edu/docs/home/",
    "doi": "10.48550/arXiv.2005.00687",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Biological graph property prediction",
    "keywords": [
      "node prediction",
      "link prediction",
      "graph classification"
    ],
    "summary": "OGB-Biology is a suite of large-scale biological network datasets (protein-protein\ninteraction, drug-target, etc.) with standardized splits and evaluation protocols \nfor node, link, and graph property prediction tasks.\n",
    "licensing": "MIT License",
    "task_types": [
      "Node property prediction",
      "Link property prediction",
      "Graph property prediction"
    ],
    "ai_capability_measured": [
      "Scalability and generalization in graph ML for biology"
    ],
    "metrics": [
      "Accuracy",
      "ROC-AUC"
    ],
    "models": [
      "GCN",
      "GraphSAGE",
      "GAT"
    ],
    "ml_motif": [
      "Sequence Prediction/Forecasting"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "0",
    "notes": "Community-driven updates",
    "contact": {
      "name": "OGB Team",
      "email": "ogb@cs.stanford.edu"
    },
    "cite": [
      "@misc{hu2021opengraphbenchmarkdatasets,\n    archiveprefix = {arXiv},\n    author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},\n    eprint        = {2005.00687},\n    primaryclass  = {cs.LG},\n    title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},\n    url           = {https://arxiv.org/abs/2005.00687},\n    year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "OGB Webpage",
          "url": "https://ogb.stanford.edu/docs/dataset_overview/"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "All necessary information is provided on the Github\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; splits are well-defined.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "Multiple baselines implemented and documented (GCN, GAT, GraphSAGE).\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "All necessary information is included in a paper.\n"
      }
    },
    "id": "open_graph_benchmark_ogb_-_biology"
  },
  {
    "date": "2024-05-01",
    "version": "v0.2.0",
    "last_updated": "2024-05",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-05-01",
    "name": "Jet Classification",
    "url": "https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify",
    "doi": "10.48550/arXiv.2207.07958",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time classification of particle jets using HL-LHC simulation features",
    "keywords": [
      "classification",
      "real-time ML",
      "jet tagging",
      "QKeras"
    ],
    "summary": "This benchmark evaluates ML models for real-time classification of\nparticle jets using high-level features derived from simulated LHC data. It\nincludes both full-precision and quantized models optimized for FPGA deployment.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "Real-time inference",
      "model compression performance"
    ],
    "metrics": [
      "Accuracy",
      "AUC"
    ],
    "models": [
      "Keras DNN",
      "QKeras quantized DNN"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Includes both float and quantized models using QKeras\n",
    "contact": {
      "name": "Jules Muhizi",
      "email": "unknown"
    },
    "cite": [
      "@misc{duarte2022fastml,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "OpenML: hls4ml_lhc_jets_hlf",
          "url": "https://www.openml.org/d/42468"
        },
        {
          "name": "JetClass",
          "url": "https://zenodo.org/record/6619768"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "unknown",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Not containerized; Setup automation/documentation could be improved\n"
      },
      "specification": {
        "rating": 4,
        "reason": "System constraints missing\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "None\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "HW/SW requirements missing; Reference not bundled as official starter kit\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Full reproducibility requires manual setup\n"
      }
    },
    "id": "jet_classification"
  },
  {
    "date": "2024-05-01",
    "version": "v0.2.0",
    "last_updated": "2024-05",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-05-01",
    "name": "Irregular Sensor Data Compression",
    "url": "https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression",
    "doi": "10.48550/arXiv.2207.07958",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time compression of sparse sensor data with autoencoders",
    "keywords": [
      "compression",
      "autoencoder",
      "sparse data",
      "irregular sampling"
    ],
    "summary": "This benchmark addresses lossy compression of irregularly sampled\nsensor data from particle detectors using real-time autoencoder architectures,\ntargeting latency-critical applications in physics experiments.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Compression"
    ],
    "ai_capability_measured": [
      "Reconstruction quality",
      "compression efficiency"
    ],
    "metrics": [
      "MSE",
      "Compression ratio"
    ],
    "models": [
      "Autoencoder",
      "Quantized autoencoder"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Unsupervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Based on synthetic but realistic physics sensor data\n",
    "contact": {
      "name": "Ben Hawks, Nhan Tran",
      "email": "unknown"
    },
    "cite": [
      "@misc{duarte2022fastmlsciencebenchmarksaccelerating2,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Reddi, Vijay Janapa},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Custom synthetic irregular sensor dataset",
          "url": "https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1Q_kENN-Lxod5_BmqUZuqC7yT0tG1KObU9mjS1AV3zK0"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Not containerized; Full automation and documentation could be improved\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Exact latency or resource constraints not numerically specified\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "All criteria met\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "All criteria met\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Not fully documented or automated for reproducibility\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Setup for deployment (e.g., FPGA pipeline) requires familiarity with tooling\n"
      }
    },
    "id": "irregular_sensor_data_compression"
  },
  {
    "date": "2024-05-01",
    "version": "v0.2.0",
    "last_updated": "2024-05",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-05-01",
    "name": "Beam Control",
    "url": "https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control",
    "doi": "10.48550/arXiv.2207.07958",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Reinforcement learning control of accelerator beam position",
    "keywords": [
      "RL",
      "beam stabilization",
      "control systems",
      "simulation"
    ],
    "summary": "Beam Control explores real-time reinforcement learning strategies for maintaining \nstable beam trajectories in particle accelerators. The benchmark is based on the \nBOOSTR environment for accelerator simulation.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Control"
    ],
    "ai_capability_measured": [
      "Policy performance in simulated accelerator control"
    ],
    "metrics": [
      "Stability",
      "Control loss"
    ],
    "models": [
      "DDPG",
      "PPO (planned)"
    ],
    "ml_motif": [
      "Reinforcement Learning/Control"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Reinforcement Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Environment defined, baseline RL implementation is in progress\n",
    "contact": {
      "name": "Ben Hawks, Nhan Tran",
      "email": "unknown"
    },
    "cite": [
      "@misc{duarte2022fastmlsciencebenchmarksaccelerating3,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}\n",
      "@misc{kafkes2021boostrdatasetacceleratorcontrol,\n  archiveprefix = {arXiv},\n  author        = {Diana Kafkes and Jason St. John},\n  eprint        = {2101.08359},\n  primaryclass  = {physics.acc-ph},\n  title         = {BOOSTR: A Dataset for Accelerator Control Systems},\n  url           = {https://arxiv.org/abs/2101.08359},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1dqOsPNlp7oLix6uDsqXi-j9xHq50DGf5wnQi-Jms2DQ"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "in progress",
      "benchmark_ready": "in progress"
    },
    "ratings": {
      "software": {
        "rating": 1,
        "reason": "Code not documented; Incomplete setup and not containerized\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Latency/resource constraints not fully quantified\n"
      },
      "dataset": {
        "rating": 3,
        "reason": "Not findable (no DOI/indexing); Not interoperable (format/schema unspecified)\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "All criteria met\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "HW/SW requirements missing; Metrics not evaluated with reference; Baseline not trainable/open\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Setup instructions and pretrained model details are missing\n"
      }
    },
    "id": "beam_control"
  },
  {
    "date": "2024-07-08",
    "version": "v1.0",
    "last_updated": "2024-07",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-07-08",
    "name": "Ultrafast jet classification at the HL-LHC",
    "url": "https://arxiv.org/pdf/2402.01876",
    "doi": "10.48550/arXiv.2402.01876",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "FPGA-optimized real-time jet origin classification at the HL-LHC",
    "keywords": [
      "jet classification",
      "FPGA",
      "quantization-aware training",
      "Deep Sets",
      "Interaction Networks"
    ],
    "summary": "Demonstrates three ML models (MLP, Deep Sets, Interaction Networks) optimized for FPGA deployment with O(100 ns) inference using quantized models and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260. \n",
    "licensing": "CC-BY",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "Real-time inference under FPGA constraints"
    ],
    "metrics": [
      "Accuracy",
      "Latency",
      "Resource utilization"
    ],
    "models": [
      "MLP",
      "Deep Sets",
      "Interaction Network"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Model",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Uses quantization-aware training; hardware synthesis evaluated via hls4ml\n",
    "contact": {
      "name": "Patrick Odagiu",
      "email": "podagiu@ethz.ch"
    },
    "cite": [
      "@misc{odagiu2024ultrafastjetclassificationfpgas,\n  archiveprefix = {arXiv},\n  author        = {Patrick Odagiu and Zhiqiang Que and Javier Duarte and Johannes Haller and Gregor Kasieczka and Artur Lobanov and Vladimir Loncar and Wayne Luk and Jennifer Ngadiuba and Maurizio Pierini and Philipp Rincke and Arpita Seksaria and Sioni Summers and Andre Sznajder and Alexander Tapper and Thea K. Aarrestad},\n  doi           = {https://doi.org/10.1088/2632-2153/ad5f10},\n  eprint        = {2402.01876},\n  primaryclass  = {hep-ex},\n  title         = {Ultrafast jet classification on FPGAs for the HL-LHC},\n  url           = {https://arxiv.org/abs/2402.01876},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Zenodo dataset",
          "url": "https://zenodo.org/records/3602260"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1Hk2zHauNv6BcRH4ZY5RH6v_oKDfeKzyjhoYyP0Xw4h4"
        },
        {
          "name": "ChatGPT LLM",
          "url": "https://docs.google.com/document/d/1gDf1CIYtfmfZ9urv1jCRZMYz_3WwEETkugUC65OZBdw"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Not containerized; Setup and automation incomplete\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Hardware constraints are referenced but not fully detailed or standardized\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "FAIR metadata limited; no clear mention of dataset format or splits\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "Metrics exist (accuracy, latency, utilization), but formal definitions and evaluation guidance are limited\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Reference implementations not fully reproducible; no evaluation pipeline or training setup provided\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "No linked GitHub repo or setup instructions; paper provides partial guidance only\n"
      }
    },
    "id": "ultrafast_jet_classification_at_the_hl-lhc"
  },
  {
    "date": "2024-10-15",
    "version": "v1.0",
    "last_updated": "2024-10",
    "expired": "no",
    "valid": "yes",
    "valid_date": "2024-10-15",
    "name": "Quench detection",
    "url": "https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf",
    "doi": "NA",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time detection of superconducting magnet quenches using ML",
    "keywords": [
      "quench detection",
      "autoencoder",
      "anomaly detection",
      "real-time"
    ],
    "summary": "Exploration of real-time quench detection using unsupervised and RL approaches, combining multi-modal sensor data (BPM, power supply, acoustic), operating on kHz-MHz streams with anomaly detection and frequency-domain features.\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Anomaly Detection",
      "Quench localization"
    ],
    "ai_capability_measured": [
      "Real-time anomaly detection with multi-modal sensors"
    ],
    "metrics": [
      "ROC-AUC",
      "Detection latency"
    ],
    "models": [
      "Autoencoder",
      "RL agents (in development)"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Reinforcement, Unsupervised Learning"
    ],
    "solutions": "0",
    "notes": "Precursor detection in progress; multi-modal and dynamic weighting methods\n",
    "contact": {
      "name": "Maira Khan",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{quench2024,\n  author = {Maira Khan and Steve Krave and Vittorio Marinozzi and Jennifer Ngadiuba and Stoyan Stoynev and Nhan Tran},\n  title = {Benchmarking and Interpreting Real Time Quench Detection Algorithms},\n  booktitle = {Fast Machine Learning for Science Conference 2024},\n  year = {2024},\n  month = oct,\n  address = {Purdue University, IN},\n  publisher = {indico.cern.ch},\n  url = {https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "BPM and power supply data from BNL",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1O7NGfSIKpXqFM1D_y0DWRueYHGm5Sqj0MaWNZzMzb6w"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "in progress",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 1,
        "reason": "Code not provided; no evidence of documentation or containerization\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Real-time detection task is clearly described, but exact constraints, inputs/outputs, and evaluation protocol are only partially specified\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "Dataset URL is missing; FAIR principles largely unmet\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "ROC-AUC and latency are mentioned, but metric definitions and formal evaluation setup are missing\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "No baseline or reproducible model implementation available\n"
      },
      "documentation": {
        "rating": 2,
        "reason": "Only a conference slide deck is available; lacks detailed instructions or repository for reproduction\n"
      }
    },
    "id": "quench_detection"
  },
  {
    "date": "2025-01-08",
    "version": "v1.0",
    "last_updated": "2025-01",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-01-08",
    "name": "Intelligent experiments through real-time AI",
    "url": "https://arxiv.org/pdf/2501.04845",
    "doi": "10.48550/arXiv.2501.04845",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time FPGA-based triggering and detector control for sPHENIX and future EIC",
    "keywords": [
      "FPGA",
      "Graph Neural Network",
      "hls4ml",
      "real-time inference",
      "detector control"
    ],
    "summary": "Research and Development demonstrator for real-time processing of high-rate tracking data from the sPHENIX detector (RHIC) and future EIC systems. Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events (heavy flavor, DIS electrons) within 10 micros latency. Demonstrated improved accuracy and latency on Alveo/FELIX platforms.\n",
    "licensing": "CC BY-NC-ND 4.0",
    "task_types": [
      "Trigger classification",
      "Detector control",
      "Real-time inference"
    ],
    "ai_capability_measured": [
      "Low-latency GNN inference on FPGA"
    ],
    "metrics": [
      "Accuracy (charm and beauty detection)",
      "Latency (micros)",
      "Resource utilization (LUT/FF/BRAM/DSP)"
    ],
    "models": [
      "Bipartite Graph Network with Set Transformers (BGN-ST)",
      "GarNet (edge-classifier)"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Model",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Achieved ~97.4% accuracy for beauty decay triggers; sub-10 micros latency on Alveo U280; hit-based FPGA design via hls4ml and FlowGNN.\n",
    "contact": {
      "name": "Jakub Kvapil",
      "email": "Jakub.Kvapil@lanl.gov"
    },
    "cite": [
      "@misc{kvapil2025intelligentexperimentsrealtimeai,\n  archiveprefix={arXiv},\n  author={J. Kvapil and G. Borca-Tasciuc and H. Bossi and K. Chen and Y. Chen and Y. Corrales Morales and H. Da Costa and C. Da Silva and C. Dean and J. Durham and S. Fu and C. Hao and P. Harris and O. Hen and H. Jheng and Y. Lee and P. Li and X. Li and Y. Lin and M. X. Liu and V. Loncar and J. P. Mitrevski and A. Olvera and M. L. Purschke and J. S. Renck and G. Roland and J. Schambach and Z. Shi and N. Tran and N. Wuerfel and B. Xu and D. Yu and H. Zhang},\n  eprint={2501.04845},\n  primaryclass={physics.ins-det},\n  title={Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors},\n  url={https://arxiv.org/abs/2501.04845},\n  year={2025}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Internal simulated tracking data (sPHENIX and EIC DIS-electron tagger)",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "No containerized or open-source setup provided\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Architectural/system specifications are incomplete\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "Dataset is internal and not publicly available or FAIR-compliant\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "Metrics relevant but not supported by evaluation scripts or baselines\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "No public or reproducible implementation released\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "No public GitHub or complete pipeline documentation\n"
      }
    },
    "id": "intelligent_experiments_through_real-time_ai"
  },
  {
    "date": "2025-01-09",
    "version": "v1.0",
    "last_updated": "2025-01",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-01-09",
    "name": "Neural Architecture Codesign for Fast Physics Applications",
    "url": "https://arxiv.org/abs/2501.05515",
    "doi": "10.48550/arXiv.2501.05515",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Automated neural architecture search and hardware-efficient model codesign for fast physics applications",
    "keywords": [
      "neural architecture search",
      "FPGA deployment",
      "quantization",
      "pruning",
      "hls4ml"
    ],
    "summary": "Introduces a two-stage neural architecture codesign (NAC) pipeline combining global and local search,\nquantization-aware training, and pruning to design efficient models for fast Bragg peak finding and\njet classification, synthesized for FPGA deployment with hls4ml. Achieves >30x reduction in BOPs\nand sub-100 ns inference latency on FPGA.\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Classification",
      "Peak finding"
    ],
    "ai_capability_measured": [
      "Hardware-aware model optimization; low-latency inference"
    ],
    "metrics": [
      "Accuracy",
      "Latency",
      "Resource utilization"
    ],
    "models": [
      "NAC-based BraggNN",
      "NAC-optimized Deep Sets (jet)"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced.\n",
    "contact": {
      "name": "Jason Weitz (UCSD), Nhan Tran (FNAL)",
      "email": "unknown"
    },
    "cite": [
      "@misc{weitz2025neuralarchitecturecodesignfast,\n  archiveprefix={arXiv},\n  author={Jason Weitz and Dmitri Demler and Luke McDermott and Nhan Tran and Javier Duarte},\n  eprint={2501.05515},\n  primaryclass={cs.LG},\n  title={Neural Architecture Codesign for Fast Physics Applications},\n  url={https://arxiv.org/abs/2501.05515},\n  year={2025}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1X6RvGHaF1rZGYSorZSEEAxlwGMYau9RQHVOn82vWv2I/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes (nac-opt, hls4ml)",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Toolchain (hls4ml, nac-opt) described but not yet containerized or fully packaged\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Fully specified task with constraints and target deployment; includes hardware context\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "Simulated datasets referenced but not publicly available or FAIR-compliant\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Clear, quantitative metrics aligned with task goals and hardware evaluation\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Models tested on hardware with source code references; full training pipeline not yet released\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Detailed paper and tools described; open repo planned but not yet complete\n"
      }
    },
    "id": "neural_architecture_codesign_for_fast_physics_applications"
  },
  {
    "date": "2024-06-24",
    "version": "v1.0",
    "last_updated": "2024-06",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-06-24",
    "name": "Smart Pixels for LHC",
    "url": "https://arxiv.org/abs/2406.14860",
    "doi": "10.48550/arXiv.2406.14860",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors",
    "keywords": [
      "smart pixel",
      "on-sensor inference",
      "data reduction",
      "trigger"
    ],
    "summary": "Presents a 256x256-pixel ROIC in 28 nm CMOS with embedded 2-layer NN for cluster filtering\nat 25 ns, achieving 54-75% data reduction while maintaining noise and latency constraints. Prototype\nconsumes ~300 microW/pixel and operates in combinatorial digital logic.\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Image Classification",
      "Data filtering"
    ],
    "ai_capability_measured": [
      "On-chip",
      "low-power inference; data reduction"
    ],
    "metrics": [
      "Data rejection rate",
      "Power per pixel"
    ],
    "models": [
      "2-layer pixel NN"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Image Classification"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades.\n",
    "contact": {
      "name": "Lindsey Gray; Jennet Dickinson",
      "email": "unknown"
    },
    "cite": [
      "@misc{parpillon2024smartpixelsinpixelai,\n  archiveprefix = {arXiv},\n  author        = {Benjamin Parpillon and Chinar Syal and Jieun Yoo and Jennet Dickinson and Morris Swartz and Giuseppe Di Guglielmo and Alice Bean and Douglas Berry and Manuel Blanco Valentin and Karri DiPetrillo and Anthony Badea and Lindsey Gray and Petar Maksimovic and Corrinne Mills and Mark S. Neubauer and Gauri Pradhan and Nhan Tran and Dahai Wen and Farah Fahim},\n  eprint        = {2406.14860},\n  primaryclass  = {physics.ins-det},\n  title         = {Smart Pixels: In-pixel AI for on-sensor data filtering},\n  url           = {https://arxiv.org/abs/2406.14860},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1Fevo7IGGAFC8pHrGGGA4t9V-nUwZkDezncAKDHN4v0E/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes (Zenodo:7331128)"
    },
    "ratings": {
      "software": {
        "rating": 2,
        "reason": "No packaged code or setup scripts available; replication depends on hardware description and paper\n"
      },
      "specification": {
        "rating": 5,
        "reason": "None\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "No dataset links; not publicly hosted or FAIR-compliant\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "In-pixel 2-layer NN described and evaluated, but reproducibility and source files are not released\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Paper contains detailed descriptions, but no repo or external guide for reproducing results\n"
      }
    },
    "id": "smart_pixels_for_lhc"
  },
  {
    "date": "2023-10-03",
    "version": "v1.0",
    "last_updated": "2023-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-10-03",
    "name": "HEDM (BraggNN)",
    "url": "https://arxiv.org/abs/2008.08198",
    "doi": "10.48550/arXiv.2008.08198",
    "domain": [
      "Materials Science"
    ],
    "focus": "Fast Bragg peak analysis using deep learning in diffraction microscopy",
    "keywords": [
      "BraggNN",
      "diffraction",
      "peak finding",
      "HEDM"
    ],
    "summary": "Uses BraggNN, a deep neural network, for rapid Bragg peak localization in \nhigh-energy diffraction microscopy, achieving about 13x speedup compared \nto Voigt-based methods while maintaining sub-pixel accuracy.\n",
    "licensing": "DOE Public Access Plan",
    "task_types": [
      "Peak detection"
    ],
    "ai_capability_measured": [
      "High-throughput peak localization"
    ],
    "metrics": [
      "Localization accuracy",
      "Inference time"
    ],
    "models": [
      "BraggNN"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "Peak finding"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Enables real-time HEDM workflows; basis for NAC case study.\n",
    "contact": {
      "name": "Jason Weitz (UCSD)",
      "email": "unknown"
    },
    "cite": [
      "@misc{liu2021braggnnfastxraybragg,\n  archiveprefix = {arXiv},\n  author        = {Zhengchun Liu and Hemant Sharma and Jun-Sang Park and Peter Kenesei and Antonino Miceli and Jonathan Almer and Rajkumar Kettimuthu and Ian Foster},\n  eprint        = {2008.08198},\n  primaryclass  = {eess.IV},\n  title         = {BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning},\n  url           = {https://arxiv.org/abs/2008.08198},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1wdUwyMyOi00QzQmkI8VBfwseTVXndxPAurwGsuvoQmQ/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 2,
        "reason": "No standalone code repository or setup instructions provided\n"
      },
      "specification": {
        "rating": 5,
        "reason": "None\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "No dataset links or FAIR metadata; unclear public access\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Only localization accuracy and inference time mentioned; not formally benchmarked with scripts\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "BraggNN model is described and evaluated, but no direct implementation or inference scripts available\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Paper is clear, but lacks a GitHub repo or full reproducibility pipeline\n"
      }
    },
    "id": "hedm_braggnn"
  },
  {
    "date": "2023-12-03",
    "version": "v1.0",
    "last_updated": "2023-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-12-03",
    "name": "4D-STEM",
    "url": "https://openreview.net/pdf?id=7yt3N0o0W9",
    "doi": "unknown",
    "domain": [
      "Materials Science"
    ],
    "focus": "Real-time ML for scanning transmission electron microscopy",
    "keywords": [
      "4D-STEM",
      "electron microscopy",
      "real-time",
      "image processing"
    ],
    "summary": "Proposes ML methods for real-time analysis of 4D scanning transmission electron microscopy\ndatasets; framework details in progress.\n",
    "licensing": "unknown",
    "task_types": [
      "Image Classification",
      "Streamed data inference"
    ],
    "ai_capability_measured": [
      "Real-time large-scale microscopy inference"
    ],
    "metrics": [
      "Classification accuracy",
      "Throughput"
    ],
    "models": [
      "CNN models (prototype)"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Model",
    "ml_task": [
      "Image Classification"
    ],
    "solutions": "0",
    "notes": "In-progress; model design under development.\n",
    "contact": {
      "name": "Shuyu Qin",
      "email": "shq219@lehigh.edu"
    },
    "cite": [
      "@inproceedings{qin2023extremely,\n  title={Extremely Noisy 4D-TEM Strain Mapping Using Cycle Consistent Spatial Transforming Autoencoders},\n  author={Shuyu Qin and Joshua Agar and Nhan Tran},\n  booktitle={AI for Accelerated Materials Design - NeurIPS 2023 Workshop},\n  year={2023},\n  url={https://openreview.net/forum?id=7yt3N0o0W9}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1RhoGej2LmTOb0ZF3mPzhPqV2aCct805dF40LARh_YZE/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "in progress",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 2,
        "reason": "No standalone code repository or setup instructions provided\n"
      },
      "specification": {
        "rating": 5,
        "reason": "None\n"
      },
      "dataset": {
        "rating": 2,
        "reason": "No dataset links or FAIR metadata; unclear public access\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Only localization accuracy and inference time mentioned; not formally benchmarked with scripts\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "BraggNN model is described and evaluated, but no direct implementation or inference scripts available\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Paper is clear, but lacks a GitHub repo or full reproducibility pipeline\n"
      }
    },
    "id": "d-stem"
  },
  {
    "date": "2023-12-05",
    "version": "v1.0",
    "last_updated": "2023-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-12-05",
    "name": "In-Situ High-Speed Computer Vision",
    "url": "https://arxiv.org/abs/2312.00128",
    "doi": "10.48550/arXiv.2312.00128",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time image classification for in-situ plasma diagnostics",
    "keywords": [
      "plasma",
      "in-situ vision",
      "real-time ML"
    ],
    "summary": "Applies low-latency CNN models for image classification of plasma diagnostics streams; supports deployment on embedded platforms.\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Image Classification"
    ],
    "ai_capability_measured": [
      "Real-time diagnostic inference"
    ],
    "metrics": [
      "Accuracy",
      "FPS"
    ],
    "models": [
      "CNN"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Model",
    "ml_task": [
      "Image Classification"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Embedded/deployment details in progress.\n",
    "contact": {
      "name": "unknown",
      "email": "unknown"
    },
    "cite": [
      "@misc{wei2024lowlatencyopticalbasedmode,\n  archiveprefix = {arXiv},\n  author        = {Yumou Wei and Ryan F. Forelli and Chris Hansen and Jeffrey P. Levesque and Nhan Tran and Joshua C. Agar and Giuseppe Di Guglielmo and Michael E. Mauel and Gerald A. Navratil},\n  doi           = {https://doi.org/10.1063/5.0190354},\n  eprint        = {2312.00128},\n  primaryclass  = {physics.plasm-ph},\n  title         = {Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak},\n  url           = {https://arxiv.org/abs/2312.00128},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1OcPX1eQpCcQpwZ19oOoUdzY3gcIxLCHA5R_JrCPVt2A/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": "https://docs.google.com/document/d/1EqkRHuQs1yQqMvZs_L6p9JAy2vKX5OCTubzttFBuRoQ/edit?usp=sharing"
        }
      ]
    },
    "fair": {
      "reproducible": "in progress",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 1,
        "reason": "No public implementation or containerized setup released\n"
      },
      "specification": {
        "rating": 3,
        "reason": "No standardized I/O, latency constraint, or complete framing\n"
      },
      "dataset": {
        "rating": 0,
        "reason": "Dataset not provided or described in any formal way\n"
      },
      "metrics": {
        "rating": 2,
        "reason": "Throughput and accuracy mentioned, but not defined or benchmarked\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "Prototype CNNs described; no code, baseline, or training details available\n"
      },
      "documentation": {
        "rating": 2,
        "reason": "Some insight via papers, but no working repo, setup, or replication path\n"
      }
    },
    "id": "in-situ_high-speed_computer_vision"
  },
  {
    "date": "2021-10-20",
    "version": "v1.0",
    "last_updated": "2021-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2021-10-20",
    "name": "MLPerf HPC - Cosmoflow",
    "url": "https://github.com/mlcommons/hpc",
    "doi": "10.48550/arXiv.2110.11466",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Scientific ML training and inference on HPC systems",
    "keywords": [
      "HPC",
      "training",
      "inference",
      "scientific ML"
    ],
    "summary": "MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Training",
      "Inference"
    ],
    "ai_capability_measured": [
      "Scaling efficiency",
      "training time",
      "model accuracy on HPC"
    ],
    "metrics": [
      "Training time",
      "Accuracy",
      "GPU utilization"
    ],
    "models": [
      "CosmoFlow",
      "DeepCAM",
      "OpenCatalyst"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Shared framework with MLCommons Science; reference implementations included.\n",
    "contact": {
      "name": "Steven Farrell (MLCommons)",
      "email": "unknown"
    },
    "cite": [
      "@misc{farrell2021mlperfhpcholisticbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendona and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},\n  eprint        = {2110.11466},\n  primaryclass  = {cs.LG},\n  title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  url           = {https://arxiv.org/abs/2110.11466},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "See MLCommons Science entry below"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference implementations exist but containerization and environment setup require manual effort across HPC systems.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Hardware constraints and I/O formats are not fully defined for all scenarios.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Not all data is independently versioned or comes with standardized FAIR metadata.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Central guidance is available but requires domain-specific effort to replicate results across systems.\n"
      }
    },
    "id": "mlperf_hpc_-_cosmoflow"
  },
  {
    "date": "2021-10-20",
    "version": "v1.0",
    "last_updated": "2021-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2021-10-20",
    "name": "MLPerf HPC - DeepCAM",
    "url": "https://github.com/mlcommons/hpc",
    "doi": "10.48550/arXiv.2110.11466",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Scientific ML training and inference on HPC systems",
    "keywords": [
      "HPC",
      "training",
      "inference",
      "scientific ML"
    ],
    "summary": "MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Training",
      "Inference"
    ],
    "ai_capability_measured": [
      "Scaling efficiency",
      "training time",
      "model accuracy on HPC"
    ],
    "metrics": [
      "Training time",
      "Accuracy",
      "GPU utilization"
    ],
    "models": [
      "DeepCAM"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Shared framework with MLCommons Science; reference implementations included.\n",
    "contact": {
      "name": "Steven Farrell (MLCommons)",
      "email": "unknown"
    },
    "cite": [
      "@misc{farrell2021mlperfhpcholisticbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendona and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},\n  eprint        = {2110.11466},\n  primaryclass  = {cs.LG},\n  title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  url           = {https://arxiv.org/abs/2110.11466},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "See MLCommons Science entry below"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference implementations exist but containerization and environment setup require manual effort across HPC systems.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Hardware constraints and I/O formats are not fully defined for all scenarios.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Not all data is independently versioned or comes with standardized FAIR metadata.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Central guidance is available but requires domain-specific effort to replicate results across systems.\n"
      }
    },
    "id": "mlperf_hpc_-_deepcam"
  },
  {
    "date": "2021-10-20",
    "version": "v1.0",
    "last_updated": "2021-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2021-10-20",
    "name": "MLPerf HPC - Open Catalyst Project DimeNet++ ",
    "url": "https://github.com/mlcommons/hpc",
    "doi": "10.48550/arXiv.2110.11466",
    "domain": [
      "Chemistry"
    ],
    "focus": "Scientific ML training and inference on HPC systems",
    "keywords": [
      "HPC",
      "training",
      "inference",
      "scientific ML"
    ],
    "summary": "MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Training",
      "Inference"
    ],
    "ai_capability_measured": [
      "Scaling efficiency",
      "training time",
      "model accuracy on HPC"
    ],
    "metrics": [
      "Training time",
      "Accuracy",
      "GPU utilization"
    ],
    "models": [
      "DeepCAM"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Shared framework with MLCommons Science; reference implementations included.\n",
    "contact": {
      "name": "Steven Farrell (MLCommons)",
      "email": "unknown"
    },
    "cite": [
      "@misc{farrell2021mlperfhpcholisticbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendona and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},\n  eprint        = {2110.11466},\n  primaryclass  = {cs.LG},\n  title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  url           = {https://arxiv.org/abs/2110.11466},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "See MLCommons Science entry below"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference implementations exist but containerization and environment setup require manual effort across HPC systems.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Hardware constraints and I/O formats are not fully defined for all scenarios.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Not all data is independently versioned or comes with standardized FAIR metadata.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Central guidance is available but requires domain-specific effort to replicate results across systems.\n"
      }
    },
    "id": "mlperf_hpc_-_open_catalyst_project_dimenet_"
  },
  {
    "date": "2021-10-20",
    "version": "v1.0",
    "last_updated": "2021-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2021-10-20",
    "name": "MLPerf HPC - OpenFold",
    "url": "https://github.com/mlcommons/hpc",
    "doi": "10.48550/arXiv.2110.11466",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Scientific ML training and inference on HPC systems",
    "keywords": [
      "HPC",
      "training",
      "inference",
      "scientific ML"
    ],
    "summary": "MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Training",
      "Inference"
    ],
    "ai_capability_measured": [
      "Scaling efficiency",
      "training time",
      "model accuracy on HPC"
    ],
    "metrics": [
      "Training time",
      "Accuracy",
      "GPU utilization"
    ],
    "models": [
      "DeepCAM"
    ],
    "ml_motif": [
      "Sequence Prediction/Forecasting"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Shared framework with MLCommons Science; reference implementations included.\n",
    "contact": {
      "name": "Steven Farrell (MLCommons)",
      "email": "unknown"
    },
    "cite": [
      "@misc{farrell2021mlperfhpcholisticbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendona and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},\n  eprint        = {2110.11466},\n  primaryclass  = {cs.LG},\n  title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  url           = {https://arxiv.org/abs/2110.11466},\n  year          = {2021}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "See MLCommons Science entry below"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference implementations exist but containerization and environment setup require manual effort across HPC systems.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Hardware constraints and I/O formats are not fully defined for all scenarios.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Not all data is independently versioned or comes with standardized FAIR metadata.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "None\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Central guidance is available but requires domain-specific effort to replicate results across systems.\n"
      }
    },
    "id": "mlperf_hpc_-_openfold"
  },
  {
    "date": "2023-06-01",
    "version": "v1.0",
    "last_updated": "2023-06",
    "expired": "no",
    "valid": "yes",
    "valid_date": "2023-06-01",
    "name": "MLCommons Science - CloudMask",
    "url": "https://github.com/mlcommons/science",
    "doi": "10.1007/978-3-031-23220-6_4",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "AI benchmarks for scientific applications including time-series, imaging, and simulation",
    "keywords": [
      "science AI",
      "benchmark",
      "MLCommons",
      "HPC"
    ],
    "summary": "MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Time-series analysis",
      "Image classification",
      "Simulation surrogate modeling"
    ],
    "ai_capability_measured": [
      "Inference accuracy",
      "simulation speed-up",
      "generalization"
    ],
    "metrics": [
      "MAE",
      "Accuracy",
      "Speedup vs simulation"
    ],
    "models": [
      "CNN",
      "GNN",
      "Transformer"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Joint effort under Apache-2.0 license.\n",
    "contact": {
      "name": "MLCommons Science Working Group",
      "email": "science-chairs@mlcommons.org"
    },
    "cite": [
      "@InProceedings{10.1007/978-3-031-23220-6_4,\n  author=\"Thiyagalingam, Jeyan\n  and von Laszewski, Gregor\n  and Yin, Junqi\n  and Emani, Murali\n  and Papay, Juri\n  and Barrett, Gregg\n  and Luszczek, Piotr\n  and Tsaris, Aristeidis\n  and Kirkpatrick, Christine\n  and Wang, Feiyi\n  and Gibbs, Tom\n  and Vishwanath, Venkatram\n  and Shankar, Mallikarjun\n  and Fox, Geoffrey\n  and Hey, Tony\",\n  editor=\"Anzt, Hartwig\n  and Bienz, Amanda\n  and Luszczek, Piotr\n  and Baboulin, Marc\",\n  title=\"AI Benchmarking for Science: Efforts from the MLCommons Science Working Group\",\n  booktitle=\"High Performance Computing. ISC High Performance 2022 International Workshops\",\n  year=\"2022\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\",\n  pages=\"47--64\",\n  abstract=\"With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.\",\n  isbn=\"978-3-031-23220-6\"\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "CloudMask Benchmark",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/cloudmask#data"
        },
        {
          "name": "MLCommons Data Earthquake",
          "url": "https://github.com/laszewsk/mlcommons-data-earthquake"
        },
        {
          "name": "A Database of Convergent Beam Electron Diffraction Patterns for Machine Learning of the Structural Properties of Materials",
          "url": "https://doi.ccs.ornl.gov/dataset/7aed61eb-e44c-5b14-82ea-07917d1b2d3b"
        },
        {
          "name": "CANDLE UNO",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/uno#data-description"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Actively maintained GitHub repository available at https://github.com/mlcommons/science\nwith implementations, scripts, and reproducibility support.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "All five specification aspects are covered: system constraints, task, dataset format,\nbenchmark inputs, and outputs.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Public scientific datasets are used with defined splits. At least 4 FAIR principles\nare followed.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Clearly defined metrics such as accuracy, training time, and GPU utilization are\nused. These metrics are explained and effectively capture solution performance.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A reference implementation is available, well-documented, trainable/open, and includes\nfull metric evaluation and software/hardware details.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Thorough documentation exists covering the task, background, motivation, evaluation\ncriteria, and includes a supporting paper.\n"
      }
    },
    "id": "mlcommons_science_-_cloudmask"
  },
  {
    "date": "2023-06-01",
    "version": "v1.0",
    "last_updated": "2023-06",
    "expired": "no",
    "valid": "yes",
    "valid_date": "2023-06-01",
    "name": "MLCommons Science - Earthquake",
    "url": "https://github.com/mlcommons/science",
    "doi": "10.1007/978-3-031-23220-6_4",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "AI benchmarks for scientific applications including time-series, imaging, and simulation",
    "keywords": [
      "science AI",
      "benchmark",
      "MLCommons",
      "HPC"
    ],
    "summary": "MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Time-series analysis",
      "Image classification",
      "Simulation surrogate modeling"
    ],
    "ai_capability_measured": [
      "Inference accuracy",
      "simulation speed-up",
      "generalization"
    ],
    "metrics": [
      "MAE",
      "Accuracy",
      "Speedup vs simulation"
    ],
    "models": [
      "CNN",
      "GNN",
      "Transformer"
    ],
    "ml_motif": [
      "Sequence Prediction/Forecasting"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Joint effort under Apache-2.0 license.\n",
    "contact": {
      "name": "MLCommons Science Working Group",
      "email": "science-chairs@mlcommons.org"
    },
    "cite": [
      "@InProceedings{10.1007/978-3-031-23220-6_4,\n  author=\"Thiyagalingam, Jeyan\n  and von Laszewski, Gregor\n  and Yin, Junqi\n  and Emani, Murali\n  and Papay, Juri\n  and Barrett, Gregg\n  and Luszczek, Piotr\n  and Tsaris, Aristeidis\n  and Kirkpatrick, Christine\n  and Wang, Feiyi\n  and Gibbs, Tom\n  and Vishwanath, Venkatram\n  and Shankar, Mallikarjun\n  and Fox, Geoffrey\n  and Hey, Tony\",\n  editor=\"Anzt, Hartwig\n  and Bienz, Amanda\n  and Luszczek, Piotr\n  and Baboulin, Marc\",\n  title=\"AI Benchmarking for Science: Efforts from the MLCommons Science Working Group\",\n  booktitle=\"High Performance Computing. ISC High Performance 2022 International Workshops\",\n  year=\"2022\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\",\n  pages=\"47--64\",\n  abstract=\"With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.\",\n  isbn=\"978-3-031-23220-6\"\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "CloudMask Benchmark",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/cloudmask#data"
        },
        {
          "name": "MLCommons Data Earthquake",
          "url": "https://github.com/laszewsk/mlcommons-data-earthquake"
        },
        {
          "name": "A Database of Convergent Beam Electron Diffraction Patterns for Machine Learning of the Structural Properties of Materials",
          "url": "https://doi.ccs.ornl.gov/dataset/7aed61eb-e44c-5b14-82ea-07917d1b2d3b"
        },
        {
          "name": "CANDLE UNO",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/uno#data-description"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Actively maintained GitHub repository available at https://github.com/mlcommons/science\nwith implementations, scripts, and reproducibility support.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "All five specification aspects are covered: system constraints, task, dataset format,\nbenchmark inputs, and outputs.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Public scientific datasets are used with defined splits. At least 4 FAIR principles\nare followed.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Clearly defined metrics such as accuracy, training time, and GPU utilization are\nused. These metrics are explained and effectively capture solution performance.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A reference implementation is available, well-documented, trainable/open, and includes\nfull metric evaluation and software/hardware details.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Thorough documentation exists covering the task, background, motivation, evaluation\ncriteria, and includes a supporting paper.\n"
      }
    },
    "id": "mlcommons_science_-_earthquake"
  },
  {
    "date": "2023-06-01",
    "version": "v1.0",
    "last_updated": "2023-06",
    "expired": "no",
    "valid": "yes",
    "valid_date": "2023-06-01",
    "name": "MLCommons Science - Candle UNO",
    "url": "https://github.com/mlcommons/science",
    "doi": "10.1007/978-3-031-23220-6_4",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "AI benchmarks for scientific applications including time-series, imaging, and simulation",
    "keywords": [
      "science AI",
      "benchmark",
      "MLCommons",
      "HPC"
    ],
    "summary": "MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Time-series analysis",
      "Image classification",
      "Simulation surrogate modeling"
    ],
    "ai_capability_measured": [
      "Inference accuracy",
      "simulation speed-up",
      "generalization"
    ],
    "metrics": [
      "MAE",
      "Accuracy",
      "Speedup vs simulation"
    ],
    "models": [
      "CNN",
      "GNN",
      "Transformer"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Joint effort under Apache-2.0 license.\n",
    "contact": {
      "name": "MLCommons Science Working Group",
      "email": "science-chairs@mlcommons.org"
    },
    "cite": [
      "@InProceedings{10.1007/978-3-031-23220-6_4,\n  author=\"Thiyagalingam, Jeyan\n  and von Laszewski, Gregor\n  and Yin, Junqi\n  and Emani, Murali\n  and Papay, Juri\n  and Barrett, Gregg\n  and Luszczek, Piotr\n  and Tsaris, Aristeidis\n  and Kirkpatrick, Christine\n  and Wang, Feiyi\n  and Gibbs, Tom\n  and Vishwanath, Venkatram\n  and Shankar, Mallikarjun\n  and Fox, Geoffrey\n  and Hey, Tony\",\n  editor=\"Anzt, Hartwig\n  and Bienz, Amanda\n  and Luszczek, Piotr\n  and Baboulin, Marc\",\n  title=\"AI Benchmarking for Science: Efforts from the MLCommons Science Working Group\",\n  booktitle=\"High Performance Computing. ISC High Performance 2022 International Workshops\",\n  year=\"2022\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\",\n  pages=\"47--64\",\n  abstract=\"With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.\",\n  isbn=\"978-3-031-23220-6\"\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "CloudMask Benchmark",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/cloudmask#data"
        },
        {
          "name": "MLCommons Data Earthquake",
          "url": "https://github.com/laszewsk/mlcommons-data-earthquake"
        },
        {
          "name": "A Database of Convergent Beam Electron Diffraction Patterns for Machine Learning of the Structural Properties of Materials",
          "url": "https://doi.ccs.ornl.gov/dataset/7aed61eb-e44c-5b14-82ea-07917d1b2d3b"
        },
        {
          "name": "CANDLE UNO",
          "url": "https://github.com/mlcommons/science/tree/main/benchmarks/uno#data-description"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Actively maintained GitHub repository available at https://github.com/mlcommons/science\nwith implementations, scripts, and reproducibility support.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "All five specification aspects are covered: system constraints, task, dataset format,\nbenchmark inputs, and outputs.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Public scientific datasets are used with defined splits. At least 4 FAIR principles\nare followed.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Clearly defined metrics such as accuracy, training time, and GPU utilization are\nused. These metrics are explained and effectively capture solution performance.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A reference implementation is available, well-documented, trainable/open, and includes\nfull metric evaluation and software/hardware details.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Thorough documentation exists covering the task, background, motivation, evaluation\ncriteria, and includes a supporting paper.\n"
      }
    },
    "id": "mlcommons_science_-_candle_uno"
  },
  {
    "date": "2023-06-01",
    "version": "v1.0",
    "last_updated": "2023-06",
    "expired": "no",
    "valid": "yes",
    "valid_date": "2023-06-01",
    "name": "MLCommons Science - STEMDL",
    "url": "https://github.com/mlcommons/science",
    "doi": "10.1007/978-3-031-23220-6_4",
    "domain": [
      "Materials Science"
    ],
    "focus": "AI benchmarks for scientific applications including time-series, imaging, and simulation",
    "keywords": [
      "science AI",
      "benchmark",
      "MLCommons",
      "HPC"
    ],
    "summary": "MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility.\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Time-series analysis",
      "Image classification",
      "Simulation surrogate modeling"
    ],
    "ai_capability_measured": [
      "Inference accuracy",
      "simulation speed-up",
      "generalization"
    ],
    "metrics": [
      "MAE",
      "Accuracy",
      "Speedup vs simulation"
    ],
    "models": [
      "CNN",
      "GNN",
      "Transformer"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Joint effort under Apache-2.0 license.\n",
    "contact": {
      "name": "MLCommons Science Working Group",
      "email": "science-chairs@mlcommons.org"
    },
    "cite": [
      "@InProceedings{10.1007/978-3-031-23220-6_4,\n  author=\"Thiyagalingam, Jeyan\n  and von Laszewski, Gregor\n  and Yin, Junqi\n  and Emani, Murali\n  and Papay, Juri\n  and Barrett, Gregg\n  and Luszczek, Piotr\n  and Tsaris, Aristeidis\n  and Kirkpatrick, Christine\n  and Wang, Feiyi\n  and Gibbs, Tom\n  and Vishwanath, Venkatram\n  and Shankar, Mallikarjun\n  and Fox, Geoffrey\n  and Hey, Tony\",\n  editor=\"Anzt, Hartwig\n  and Bienz, Amanda\n  and Luszczek, Piotr\n  and Baboulin, Marc\",\n  title=\"AI Benchmarking for Science: Efforts from the MLCommons Science Working Group\",\n  booktitle=\"High Performance Computing. ISC High Performance 2022 International Workshops\",\n  year=\"2022\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\",\n  pages=\"47--64\",\n  abstract=\"With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.\",\n  isbn=\"978-3-031-23220-6\"\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "A Database of Convergent Beam Electron Diffraction Patterns for Machine Learning of the Structural Properties of Materials",
          "url": "https://doi.ccs.ornl.gov/dataset/7aed61eb-e44c-5b14-82ea-07917d1b2d3b"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Actively maintained GitHub repository available at https://github.com/mlcommons/science\nwith implementations, scripts, and reproducibility support.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "All five specification aspects are covered: system constraints, task, dataset format,\nbenchmark inputs, and outputs.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Public scientific datasets are used with defined splits. At least 4 FAIR principles\nare followed.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Clearly defined metrics such as accuracy, training time, and GPU utilization are\nused. These metrics are explained and effectively capture solution performance.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A reference implementation is available, well-documented, trainable/open, and includes\nfull metric evaluation and software/hardware details.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Thorough documentation exists covering the task, background, motivation, evaluation\ncriteria, and includes a supporting paper.\n"
      }
    },
    "id": "mlcommons_science_-_stemdl"
  },
  {
    "date": "2021-07-05",
    "version": "v1.0",
    "last_updated": "2021-07",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2021-07-05",
    "name": "LHC New Physics Dataset",
    "url": "https://arxiv.org/pdf/2107.02157",
    "doi": "unknown",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time LHC event filtering for anomaly detection using proton collision data",
    "keywords": [
      "anomaly detection",
      "proton collision",
      "real-time inference",
      "event filtering",
      "unsupervised ML"
    ],
    "summary": "A dataset of proton-proton collision events emulating a 40 MHz real-time data stream from LHC detectors, pre-filtered on electron or muon presence. Designed for unsupervised new-physics detection algorithms under latency/bandwidth constraints.\n",
    "licensing": "unknown",
    "task_types": [
      "Anomaly Detection",
      "Event classification"
    ],
    "ai_capability_measured": [
      "Unsupervised signal detection under latency and bandwidth constraints"
    ],
    "metrics": [
      "ROC-AUC",
      "Detection efficiency"
    ],
    "models": [
      "Autoencoder",
      "Variational autoencoder",
      "Isolation forest"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Framework",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box.\n",
    "contact": {
      "name": "Ema Puljak",
      "email": "ema.puljak@cern.ch"
    },
    "cite": [
      "@misc{https://doi.org/10.5281/zenodo.5046389,\n  author    = {Aarrestad, Thea and Govorkova, Ekaterina and Ngadiuba, Jennifer and Puljak, Ema and Pierini, Maurizio and Wozniak, Kinga Anna},\n  copyright = {Creative Commons Attribution 4.0 International},\n  doi       = {10.5281/ZENODO.5046389},\n  publisher = {Zenodo},\n  title     = {Unsupervised New Physics detection at 40 MHz: Training Dataset},\n  url       = {https://zenodo.org/record/5046389},\n  year      = {2021}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Zenodo stores, background + 3 black-box signal sets. 1M events each",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1BnX67GfTQxHbDuUsH-MuHIl1uKxCIjrXHSoxvIaB72g/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "While not formally evaluated in the previous version, Zenodo and paper links suggest available code for baseline models\n(e.g., autoencoders, GANs), though they are scattered and not unified in a single repository.\n"
      },
      "specification": {
        "rating": 3,
        "reason": "The task and context are clearly described, but system constraints and formal inputs/outputs are not fully specified.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large-scale dataset hosted on Zenodo, publicly available, well-documented, with defined train/test structure.\nAppears to follow at least 4 FAIR principles.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Uses reasonable metrics (ROC-AUC, detection efficiency) that capture performance but lacks\nfull explanation and standard evaluation tools.\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Baselines are described across multiple papers but lack centralized, reproducible implementations\nand hardware/software setup details.\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Some description in papers and dataset metadata exists, but lacks a unified guide, README,\nor training setup in a central location.\n"
      }
    },
    "id": "lhc_new_physics_dataset"
  },
  {
    "date": "2023-07-17",
    "version": "v1.0",
    "last_updated": "2023-07",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-07-17",
    "name": "MLCommons Medical AI - Pancreas Segmentation (DFCI)",
    "url": "https://github.com/mlcommons/medical",
    "doi": "10.1038/s42256-023-00652-2",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data",
    "keywords": [
      "medical AI",
      "federated evaluation",
      "privacy-preserving",
      "fairness",
      "healthcare benchmarks"
    ],
    "summary": "The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE)\nto accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical\nmodels on diverse datasets, improving generalizability and equity while keeping data onsite .\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Federated evaluation",
      "Model validation"
    ],
    "ai_capability_measured": [
      "Clinical accuracy",
      "fairness",
      "generalizability",
      "privacy compliance"
    ],
    "metrics": [
      "ROC AUC",
      "Accuracy",
      "Fairness metrics"
    ],
    "models": [
      "MedPerf-validated CNNs",
      "GaNDLF workflows"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Platform",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Open-source platform under Apache-2.0; used across 20+ institutions and hospitals .\n",
    "contact": {
      "name": "Alex Karargyris (MLCommons Medical AI)",
      "email": "unknown"
    },
    "cite": [
      "@article{karargyris2023federated,\n  author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J. and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and Narayana Moorthy, Prakash and Chowdhury, Alexander and Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and Kanter, David and Xenochristou, Maria and Beutel, Daniel J. and Chung, Verena and Bergquist, Timothy and Eddy, James and Abid, Abubakar and Tunstall, Lewis and Sanseviero, Omar and Dimitriadis, Dimitrios and Qian, Yiming and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and Bala, Srini and Bittorf, Victor and Puchala, Sreekar Reddy and Ricciuti, Biagio and Samineni, Soujanya and Sengupta, Eshna and Chaudhari, Akshay and Coleman, Cody and Desinghu, Bala and Diamos, Gregory and Dutta, Debo and Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and Kashyap, Satyananda and Lane, Nicholas and Mallick, Indranil and Mascagni, Pietro and Mehta, Virendra and Moraes, Cassiano Ferro and Natarajan, Vivek and Nikolov, Nikola and Padoy, Nicolas and Pekhimenko, Gennady and Reddi, Vijay Janapa and Reina, G. Anthony and Ribalta, Pablo and Singh, Abhishek and Thiagarajan, Jayaraman J. and Albrecht, Jacob and Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and Shah, Prashant and Xu, Daguang and Yadav, Poonam and Talby, David and Awad, Mark M. and Howard, Jeremy P. and Rosenthal, Michael and Marchionni, Luigi and Loda, Massimo and Johnson, Jason M. and Bakas, Spyridon and Mattson, Peter and FeTS Consortium and BraTS-2020 Consortium and AI4SafeChole Consortium},\n  month = jul,\n  doi = {10.1038/s42256-023-00652-2},\n  journal = {Nature Machine Intelligence},\n  number = {7},\n  pages = {799--810},\n  title = {Federated benchmarking of medical artificial intelligence with MedPerf},\n  url = {https://doi.org/10.1038/s42256-023-00652-2},\n  volume = {5},\n  year = {2023},\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Multi-institutional clinical datasets, radiology",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "GitHub repository (https://github.com/mlcommons/medical) provides actively maintained\nopen-source tools like MedPerf and GaNDLF for federated medical AI evaluation.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "The platform defines federated tasks and model evaluation scenarios. Some clinical and\nsystem-level constraints are implied but not uniformly formalized across all use cases.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Multi-institutional datasets used in federated settings; real-world data is handled\nprivately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly\nsupport goals like generalizability and equity.\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models\nare centrally documented or easily reproducible.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Extensive documentation, papers, and community support exist. Clear examples and usage\ninstructions are provided in GitHub and publications.\n"
      }
    },
    "id": "mlcommons_medical_ai_-_pancreas_segmentation_dfci"
  },
  {
    "date": "2023-07-17",
    "version": "v1.0",
    "last_updated": "2023-07",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-07-17",
    "name": "MLCommons Medical AI - Brain Tumor Segmentation (BraTS)",
    "url": "https://github.com/mlcommons/medical",
    "doi": "10.1038/s42256-023-00652-2",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data",
    "keywords": [
      "medical AI",
      "federated evaluation",
      "privacy-preserving",
      "fairness",
      "healthcare benchmarks"
    ],
    "summary": "The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE)\nto accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical\nmodels on diverse datasets, improving generalizability and equity while keeping data onsite .\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Federated evaluation",
      "Model validation"
    ],
    "ai_capability_measured": [
      "Clinical accuracy",
      "fairness",
      "generalizability",
      "privacy compliance"
    ],
    "metrics": [
      "ROC AUC",
      "Accuracy",
      "Fairness metrics"
    ],
    "models": [
      "MedPerf-validated CNNs",
      "GaNDLF workflows"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Platform",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Open-source platform under Apache-2.0; used across 20+ institutions and hospitals .\n",
    "contact": {
      "name": "Alex Karargyris (MLCommons Medical AI)",
      "email": "unknown"
    },
    "cite": [
      "@article{karargyris2023federated,\n  author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J. and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and Narayana Moorthy, Prakash and Chowdhury, Alexander and Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and Kanter, David and Xenochristou, Maria and Beutel, Daniel J. and Chung, Verena and Bergquist, Timothy and Eddy, James and Abid, Abubakar and Tunstall, Lewis and Sanseviero, Omar and Dimitriadis, Dimitrios and Qian, Yiming and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and Bala, Srini and Bittorf, Victor and Puchala, Sreekar Reddy and Ricciuti, Biagio and Samineni, Soujanya and Sengupta, Eshna and Chaudhari, Akshay and Coleman, Cody and Desinghu, Bala and Diamos, Gregory and Dutta, Debo and Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and Kashyap, Satyananda and Lane, Nicholas and Mallick, Indranil and Mascagni, Pietro and Mehta, Virendra and Moraes, Cassiano Ferro and Natarajan, Vivek and Nikolov, Nikola and Padoy, Nicolas and Pekhimenko, Gennady and Reddi, Vijay Janapa and Reina, G. Anthony and Ribalta, Pablo and Singh, Abhishek and Thiagarajan, Jayaraman J. and Albrecht, Jacob and Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and Shah, Prashant and Xu, Daguang and Yadav, Poonam and Talby, David and Awad, Mark M. and Howard, Jeremy P. and Rosenthal, Michael and Marchionni, Luigi and Loda, Massimo and Johnson, Jason M. and Bakas, Spyridon and Mattson, Peter and FeTS Consortium and BraTS-2020 Consortium and AI4SafeChole Consortium},\n  month = jul,\n  doi = {10.1038/s42256-023-00652-2},\n  journal = {Nature Machine Intelligence},\n  number = {7},\n  pages = {799--810},\n  title = {Federated benchmarking of medical artificial intelligence with MedPerf},\n  url = {https://doi.org/10.1038/s42256-023-00652-2},\n  volume = {5},\n  year = {2023},\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Multi-institutional clinical datasets, radiology",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "GitHub repository (https://github.com/mlcommons/medical) provides actively maintained\nopen-source tools like MedPerf and GaNDLF for federated medical AI evaluation.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "The platform defines federated tasks and model evaluation scenarios. Some clinical and\nsystem-level constraints are implied but not uniformly formalized across all use cases.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Multi-institutional datasets used in federated settings; real-world data is handled\nprivately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly\nsupport goals like generalizability and equity.\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models\nare centrally documented or easily reproducible.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Extensive documentation, papers, and community support exist. Clear examples and usage\ninstructions are provided in GitHub and publications.\n"
      }
    },
    "id": "mlcommons_medical_ai_-_brain_tumor_segmentation_brats"
  },
  {
    "date": "2023-07-17",
    "version": "v1.0",
    "last_updated": "2023-07",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-07-17",
    "name": "MLCommons Medical AI -  Surgical Workflow Phase Recognition (SurgMLCube)",
    "url": "https://github.com/mlcommons/medical",
    "doi": "10.1038/s42256-023-00652-2",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data",
    "keywords": [
      "medical AI",
      "federated evaluation",
      "privacy-preserving",
      "fairness",
      "healthcare benchmarks"
    ],
    "summary": "The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE)\nto accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical\nmodels on diverse datasets, improving generalizability and equity while keeping data onsite .\n",
    "licensing": "Apache License 2.0",
    "task_types": [
      "Federated evaluation",
      "Model validation"
    ],
    "ai_capability_measured": [
      "Clinical accuracy",
      "fairness",
      "generalizability",
      "privacy compliance"
    ],
    "metrics": [
      "ROC AUC",
      "Accuracy",
      "Fairness metrics"
    ],
    "models": [
      "MedPerf-validated CNNs",
      "GaNDLF workflows"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Platform",
    "ml_task": [
      "NA"
    ],
    "solutions": "0",
    "notes": "Open-source platform under Apache-2.0; used across 20+ institutions and hospitals .\n",
    "contact": {
      "name": "Alex Karargyris (MLCommons Medical AI)",
      "email": "unknown"
    },
    "cite": [
      "@article{karargyris2023federated,\n  author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J. and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and Narayana Moorthy, Prakash and Chowdhury, Alexander and Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and Kanter, David and Xenochristou, Maria and Beutel, Daniel J. and Chung, Verena and Bergquist, Timothy and Eddy, James and Abid, Abubakar and Tunstall, Lewis and Sanseviero, Omar and Dimitriadis, Dimitrios and Qian, Yiming and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and Bala, Srini and Bittorf, Victor and Puchala, Sreekar Reddy and Ricciuti, Biagio and Samineni, Soujanya and Sengupta, Eshna and Chaudhari, Akshay and Coleman, Cody and Desinghu, Bala and Diamos, Gregory and Dutta, Debo and Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and Kashyap, Satyananda and Lane, Nicholas and Mallick, Indranil and Mascagni, Pietro and Mehta, Virendra and Moraes, Cassiano Ferro and Natarajan, Vivek and Nikolov, Nikola and Padoy, Nicolas and Pekhimenko, Gennady and Reddi, Vijay Janapa and Reina, G. Anthony and Ribalta, Pablo and Singh, Abhishek and Thiagarajan, Jayaraman J. and Albrecht, Jacob and Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and Shah, Prashant and Xu, Daguang and Yadav, Poonam and Talby, David and Awad, Mark M. and Howard, Jeremy P. and Rosenthal, Michael and Marchionni, Luigi and Loda, Massimo and Johnson, Jason M. and Bakas, Spyridon and Mattson, Peter and FeTS Consortium and BraTS-2020 Consortium and AI4SafeChole Consortium},\n  month = jul,\n  doi = {10.1038/s42256-023-00652-2},\n  journal = {Nature Machine Intelligence},\n  number = {7},\n  pages = {799--810},\n  title = {Federated benchmarking of medical artificial intelligence with MedPerf},\n  url = {https://doi.org/10.1038/s42256-023-00652-2},\n  volume = {5},\n  year = {2023},\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Multi-institutional clinical datasets, radiology",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "GitHub repository (https://github.com/mlcommons/medical) provides actively maintained\nopen-source tools like MedPerf and GaNDLF for federated medical AI evaluation.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "The platform defines federated tasks and model evaluation scenarios. Some clinical and\nsystem-level constraints are implied but not uniformly formalized across all use cases.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Multi-institutional datasets used in federated settings; real-world data is handled\nprivately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly\nsupport goals like generalizability and equity.\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models\nare centrally documented or easily reproducible.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Extensive documentation, papers, and community support exist. Clear examples and usage\ninstructions are provided in GitHub and publications.\n"
      }
    },
    "id": "mlcommons_medical_ai_-__surgical_workflow_phase_recognition_surgmlcube"
  },
  {
    "date": "2024-10-28",
    "version": "v1.0",
    "last_updated": "2024-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-10-28",
    "name": "CaloChallenge 2022",
    "url": "http://arxiv.org/abs/2410.21611",
    "doi": "10.48550/arXiv.2410.21611",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Fast generative-model-based calorimeter shower simulation evaluation",
    "keywords": [
      "calorimeter simulation",
      "generative models",
      "surrogate modeling",
      "LHC",
      "fast simulation"
    ],
    "summary": "The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative-model submissions (VAEs, GANs, Flows, Diffusion)\non four calorimeter shower datasets; benchmarking shower quality, generation speed, and model complexity .\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Surrogate modeling"
    ],
    "ai_capability_measured": [
      "Simulation fidelity",
      "speed",
      "efficiency"
    ],
    "metrics": [
      "Histogram similarity",
      "Classifier AUC",
      "Generation latency"
    ],
    "models": [
      "VAE variants",
      "GAN variants",
      "Normalizing flows",
      "Diffusion models"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Dataset",
    "ml_task": [
      "Surrogate Modeling"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset sizes.\n",
    "contact": {
      "name": "Claudius Krause (CaloChallenge Lead)",
      "email": "unknown"
    },
    "cite": [
      "@misc{krause2024calochallenge2022communitychallenge,\n  archiveprefix = {arXiv},\n  author        = {Claudius Krause and Michele Faucci Giannelli and Gregor Kasieczka and Benjamin Nachman and Dalila Salamani and David Shih and Anna Zaborowska and Oz Amram and Kerstin Borras and Matthew R. Buckley and Erik Buhmann and Thorsten Buss and Renato Paulo Da Costa Cardoso and Anthony L. Caterini and Nadezda Chernyavskaya and Federico A. G. Corchia and Jesse C. Cresswell and Sascha Diefenbacher and Etienne Dreyer and Vijay Ekambaram and Engin Eren and Florian Ernst and Luigi Favaro and Matteo Franchini and Frank Gaede and Eilam Gross and Shih-Chieh Hsu and Kristina Jaruskova and Benno Kch and Jayant Kalagnanam and Raghav Kansal and Taewoo Kim and Dmitrii Kobylianskii and Anatolii Korol and William Korcari and Dirk Krcker and Katja Krger and Marco Letizia and Shu Li and Qibin Liu and Xiulong Liu and Gabriel Loaiza-Ganem and Thandikire Madula and Peter McKeown and Isabell-A. Melzer-Pellmann and Vinicius Mikuni and Nam Nguyen and Ayodele Ore and Sofia Palacios Schweitzer and Ian Pang and Kevin Pedro and Tilman Plehn and Witold Pokorski and Huilin Qu and Piyush Raikwar and John A. Raine and Humberto Reyes-Gonzalez and Lorenzo Rinaldi and Brendan Leigh Ross and Moritz A. W. Scham and Simon Schnake and Chase Shimmin and Eli Shlizerman and Nathalie Soybelman and Mudhakar Srivatsa and Kalliopi Tsolaki and Sofia Vallecorsa and Kyongmin Yeo and Rui Zhang},\n  eprint        = {2410.21611},\n  primaryclass  = {physics.ins-det},\n  title         = {CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation},\n  url           = {https://arxiv.org/abs/2410.21611},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Four LHC calorimeter shower datasets",
          "url": "various voxel resolutions"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1JBH3WTDp2jpSt_utc1p5Dv3-MBX4xY-NVzzfXCd9xhA/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 4,
        "reason": "Community GitHub repos and model implementations are available for the 31 submissions.\nWhile not fully unified in one place, the software is accessible and reproducible.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "The taskevaluating fast generative calorimeter simulationsis clearly defined with\nbenchmarking protocols, constraints like latency and model complexity, and structured\nevaluation criteria.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Four well-structured calorimeter datasets are provided, with different voxel resolutions,\nopen access, signal/background separation, and metadata. FAIR principles are well covered.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Metrics like histogram similarity, classifier AUC, and generation latency are well defined\nand relevant for simulation quality, fidelity, and performance.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Several baselines (GANs, VAEs, flows, diffusion models) are documented and evaluated.\nSome are available via community repos, though not all are fully standardized or bundled.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Accompanied by a detailed paper and dataset description. Reproduction of pipelines may require\nadditional setup or familiarity with the model submissions.\n"
      }
    },
    "id": "calochallenge_"
  },
  {
    "date": "2022-10-13",
    "version": "v0.1.0",
    "last_updated": "2025-05",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2022-10-13",
    "name": "PDEBench",
    "url": "https://github.com/pdebench/PDEBench",
    "doi": "10.48550/arXiv.2210.07182",
    "domain": [
      "Computational Science & AI",
      "Climate & Earth Science",
      "Mathematics"
    ],
    "focus": "Benchmark suite for ML-based surrogates solving time-dependent PDEs",
    "keywords": [
      "PDEs",
      "CFD",
      "scientific ML",
      "surrogate modeling",
      "NeurIPS"
    ],
    "summary": "PDEBench offers forward/inverse PDE tasks with large ready-to-use datasets and baselines (FNO, U-Net, PINN), packaged via a unified API. It won the SimTech Best Paper Award 2023 .\n",
    "licensing": "Other",
    "task_types": [
      "Supervised Learning"
    ],
    "ai_capability_measured": [
      "Time-dependent PDE modeling; physical accuracy"
    ],
    "metrics": [
      "RMSE",
      "boundary RMSE",
      "Fourier RMSE"
    ],
    "models": [
      "FNO",
      "U-Net",
      "PINN",
      "Gradient-Based inverse methods"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Framework",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email \n",
    "contact": {
      "name": "Makoto Takamoto (makoto.takamoto@neclab.eu)",
      "email": "unknown"
    },
    "cite": [
      "@misc{takamoto2024pdebenchextensivebenchmarkscientific,\n  archiveprefix = {arXiv},\n  author        = {Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pflger and Mathias Niepert},\n  eprint        = {2210.07182},\n  primaryclass  = {cs.LG},\n  title         = {PDEBENCH: An Extensive Benchmark for Scientific Machine Learning},\n  url           = {https://arxiv.org/abs/2210.07182},\n  year          = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1MvXdFub0PxUDtB49wqli6mmSCdLErv2nLdOJUtMylOo/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "GitHub repository (https://github.com/pdebench/PDEBench) is actively maintained and includes\ntraining pipelines, data loaders, and evaluation scripts. Installation and usage are well-documented.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Clearly defined tasks for forward and inverse PDE problems, with structured input/output formats,\nsystem constraints, and task specifications.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Diverse PDE datasets (synthetic and real-world) hosted on DaRUS with DOIs. Datasets are\nwell-documented, structured, and follow FAIR practices.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Includes RMSE, boundary RMSE, and Fourier-domain RMSE. These are well-suited to PDE problems,\nthough rationale behind metric choices could be expanded in some cases.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baselines (FNO, U-Net, PINN, etc.) are available and documented, but not every model\nincludes full training and evaluation reproducibility out-of-the-box.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Strong documentation on GitHub including examples, configs, and usage instructions.\nSome model-specific details and tutorials could be further expanded.\n"
      }
    },
    "id": "pdebench"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Delta Squared-DFT",
    "url": "https://neurips.cc/virtual/2024/poster/97788",
    "doi": "10.48550/arXiv.2406.14347",
    "domain": [
      "Chemistry",
      "Materials Science"
    ],
    "focus": "Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies",
    "keywords": [
      "density functional theory",
      "Delta Squared-ML correction",
      "reaction energetics",
      "quantum chemistry"
    ],
    "summary": "Introduces the Delta Squared-ML paradigm-using ML corrections to DFT to predict reaction energies with accuracy comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and organometallic transformations.\n",
    "licensing": "unknown",
    "task_types": [
      "Regression"
    ],
    "ai_capability_measured": [
      "High-accuracy energy prediction",
      "DFT correction"
    ],
    "metrics": [
      "Mean Absolute Error (eV)",
      "Energy ranking accuracy"
    ],
    "models": [
      "Delta Squared-ML correction networks",
      "Kernel ridge regression"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Dataset + Benchmark",
    "ml_task": [
      "Regression"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility.\n",
    "contact": {
      "name": "Wei Liu",
      "email": "unknown"
    },
    "cite": [
      "@misc{khrabrov2024nabla2dftuniversalquantumchemistry,\n  title={Delta-Squared DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials}, \n  author={Kuzma Khrabrov and Anton Ber and Artem Tsypin and Konstantin Ushenin and Egor Rumiantsev and Alexander Telepov and Dmitry Protasov and Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and Sergey Nikolenko and Elena Tutubalina and Artur Kadurin},\n  year={2024},\n  eprint={2406.14347},\n  archivePrefix={arXiv},\n  primaryClass={physics.chem-ph},\n  url={https://arxiv.org/abs/2406.14347}, \n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Source code and baseline models available for ML correction to DFT; framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Benchmark focuses on reaction energy prediction with clear goals, though some task specifics could be formalized further.\n"
      },
      "dataset": {
        "rating": 4.5,
        "reason": "Multi-modal quantum chemistry datasets are standardized and accessible; repository available.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Uses standard regression metrics like MAE and energy ranking accuracy; appropriate for task.\n"
      },
      "reference_solution": {
        "rating": 3.5,
        "reason": "Includes baseline regression and kernel ridge models; implementations are reproducible.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Source code supports pipeline reuse, but formal evaluation splits may vary.\n"
      }
    },
    "id": "delta_squared-dft"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Urban Data Layer (UDL) - PM2.5 Concentration Prediction",
    "url": "https://neurips.cc/virtual/2024/poster/97837",
    "doi": "unknown",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Unified data pipeline for multi-modal urban science research",
    "keywords": [
      "data pipeline",
      "urban science",
      "multi-modal",
      "benchmark"
    ],
    "summary": "UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks .\n",
    "licensing": "unknown",
    "task_types": [
      "Prediction",
      "Classification"
    ],
    "ai_capability_measured": [
      "Multi-modal urban inference",
      "standardization"
    ],
    "metrics": [
      "Task-specific accuracy or RMSE"
    ],
    "models": [
      "Baseline regression/classification pipelines"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Framework",
    "ml_task": [
      "Prediction, classification"
    ],
    "solutions": "0",
    "notes": "Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models .\n",
    "contact": {
      "name": "Yiheng Wang",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_0db7f135,\n  author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {7296--7310},\n  publisher = {Curran Associates, Inc.},\n  title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Source code is publicly available on GitHub; baseline regression and classification\npipelines are included but framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Multiple urban science tasks like prediction and classification are well specified\nwith clear input/output and evaluation criteria.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large, multi-modal urban datasets are open-source, well-documented, and support\nreproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baseline models available but not exhaustive; community adoption and extensions expected.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "GitHub repository and conference poster provide comprehensive code and reproducibility\ninstructions.\n"
      }
    },
    "id": "urban_data_layer_udl_-_pm_concentration_prediction"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Urban Data Layer (UDL) - Built-up Area Classification",
    "url": "https://neurips.cc/virtual/2024/poster/97837",
    "doi": "unknown",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Unified data pipeline for multi-modal urban science research",
    "keywords": [
      "data pipeline",
      "urban science",
      "multi-modal",
      "benchmark"
    ],
    "summary": "UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks .\n",
    "licensing": "unknown",
    "task_types": [
      "Prediction",
      "Classification"
    ],
    "ai_capability_measured": [
      "Multi-modal urban inference",
      "standardization"
    ],
    "metrics": [
      "Task-specific accuracy or RMSE"
    ],
    "models": [
      "Baseline regression/classification pipelines"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "Prediction, classification"
    ],
    "solutions": "0",
    "notes": "Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models .\n",
    "contact": {
      "name": "Yiheng Wang",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_0db7f135,\n  author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {7296--7310},\n  publisher = {Curran Associates, Inc.},\n  title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Source code is publicly available on GitHub; baseline regression and classification\npipelines are included but framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Multiple urban science tasks like prediction and classification are well specified\nwith clear input/output and evaluation criteria.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large, multi-modal urban datasets are open-source, well-documented, and support\nreproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baseline models available but not exhaustive; community adoption and extensions expected.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "GitHub repository and conference poster provide comprehensive code and reproducibility\ninstructions.\n"
      }
    },
    "id": "urban_data_layer_udl_-_built-up_area_classification"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Urban Data Layer (UDL) - Administrative Boundaries Identification",
    "url": "https://neurips.cc/virtual/2024/poster/97837",
    "doi": "unknown",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Unified data pipeline for multi-modal urban science research",
    "keywords": [
      "data pipeline",
      "urban science",
      "multi-modal",
      "benchmark"
    ],
    "summary": "UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks .\n",
    "licensing": "unknown",
    "task_types": [
      "Prediction",
      "Classification"
    ],
    "ai_capability_measured": [
      "Multi-modal urban inference",
      "standardization"
    ],
    "metrics": [
      "Task-specific accuracy or RMSE"
    ],
    "models": [
      "Baseline regression/classification pipelines"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Framework",
    "ml_task": [
      "Prediction, classification"
    ],
    "solutions": "0",
    "notes": "Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models .\n",
    "contact": {
      "name": "Yiheng Wang",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_0db7f135,\n  author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {7296--7310},\n  publisher = {Curran Associates, Inc.},\n  title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Source code is publicly available on GitHub; baseline regression and classification\npipelines are included but framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Multiple urban science tasks like prediction and classification are well specified\nwith clear input/output and evaluation criteria.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large, multi-modal urban datasets are open-source, well-documented, and support\nreproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baseline models available but not exhaustive; community adoption and extensions expected.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "GitHub repository and conference poster provide comprehensive code and reproducibility\ninstructions.\n"
      }
    },
    "id": "urban_data_layer_udl_-_administrative_boundaries_identification"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Urban Data Layer (UDL) - El Nino Anomaly Detection",
    "url": "https://neurips.cc/virtual/2024/poster/97837",
    "doi": "unknown",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Unified data pipeline for multi-modal urban science research",
    "keywords": [
      "data pipeline",
      "urban science",
      "multi-modal",
      "benchmark"
    ],
    "summary": "UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks .\n",
    "licensing": "unknown",
    "task_types": [
      "Prediction",
      "Classification"
    ],
    "ai_capability_measured": [
      "Multi-modal urban inference",
      "standardization"
    ],
    "metrics": [
      "Task-specific accuracy or RMSE"
    ],
    "models": [
      "Baseline regression/classification pipelines"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Framework",
    "ml_task": [
      "Prediction, classification"
    ],
    "solutions": "0",
    "notes": "Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models .\n",
    "contact": {
      "name": "Yiheng Wang",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_0db7f135,\n  author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {7296--7310},\n  publisher = {Curran Associates, Inc.},\n  title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Source code is publicly available on GitHub; baseline regression and classification\npipelines are included but framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Multiple urban science tasks like prediction and classification are well specified\nwith clear input/output and evaluation criteria.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large, multi-modal urban datasets are open-source, well-documented, and support\nreproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baseline models available but not exhaustive; community adoption and extensions expected.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "GitHub repository and conference poster provide comprehensive code and reproducibility\ninstructions.\n"
      }
    },
    "id": "urban_data_layer_udl_-_el_nino_anomaly_detection"
  },
  {
    "date": "2024-11-13",
    "version": "v1.0",
    "last_updated": "2024-11",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-11-13",
    "name": "LLMs for Crop Science",
    "url": "https://openreview.net/forum?id=hMj6jZ6JWU#discussion",
    "doi": "N/A",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts",
    "keywords": [
      "crop science",
      "prompt engineering",
      "domain adaptation",
      "question answering"
    ],
    "summary": "Establishes a benchmark of over 5000 expert-annotated QA pairs and prompts in Chinese and English, covering crop traits, growth stages, and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and retrieval-augmented prompts.\n",
    "licensing": "CC-BY-NC-4.0",
    "task_types": [
      "Question Answering",
      "Inference"
    ],
    "ai_capability_measured": [
      "Scientific knowledge",
      "crop reasoning"
    ],
    "metrics": [
      "Accuracy",
      "F1 score"
    ],
    "models": [
      "GPT-3.5",
      "GPT-4",
      "Claude-3-opus",
      "Qwen-max",
      "LLama3-8B",
      "InternLM2-7B",
      "Qwen1.5-7B"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Dataset",
    "ml_task": [
      "QA, inference"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation.\n",
    "contact": {
      "name": "Deepak Patel",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{zhang2024empowering,\ntitle={Empowering and Assessing the Utility of Large Language Models in Crop Science},\nauthor={Hang Zhang and Jiawei Sun and Renqi Chen and Wei Liu and Zhonghang Yuan and Xinzhe Zheng and Zhefan Wang and Zhiyuan Yang and Hang Yan and Han-Sen Zhong and Xiqing Wang and Wanli Ouyang and Fan Yang and Nanqing Dong},\nbooktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\nyear={2024},\nurl={https://openreview.net/forum?id=hMj6jZ6JWU}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "CROP Dataset (Train Split)",
          "url": "https://huggingface.co/datasets/AI4Agr/CROP-dataset"
        },
        {
          "name": "CROP Benchmark (Test Split)",
          "url": "https://huggingface.co/datasets/AI4Agr/CROP-benchmark"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Empowering and Assessing the Utility of Large Language Models in Crop Science - Experiments",
          "url": "https://renqichen.github.io/The_Crop/"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Code for evaluation and training of multiple models is available and well documented. Environment details are provided.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Tasks are clearly defined (QA, inference) with structured input/output formats, though no system constraints are provided.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset adheres to all FAIR principles, is well-documented, and publicly available on Hugging Face. Train/Test splits are provided across two Huggingface datasets.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Accuracy is mentioned in the README and webpage as an evaluation metric,\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "A reference solution is available and well documented. Training code is provided for multiple open weight models.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "The benchmark is well documented with a detailed paper, README, and webpage. Instructions for reproducing results are clear.\n"
      }
    },
    "id": "llms_for_crop_science"
  },
  {
    "date": "2024-10-15",
    "version": "v1.0",
    "last_updated": "2024-10",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-10-15",
    "name": "DUNE",
    "url": "https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf",
    "doi": "10.48550/arXiv.2103.13910",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Real-time ML for DUNE DAQ time-series data",
    "keywords": [
      "DUNE",
      "time-series",
      "real-time",
      "trigger"
    ],
    "summary": "Applying real-time ML methods to time-series data from DUNE detectors, exploring trigger-level anomaly detection and event selection with low latency constraints.\n",
    "licensing": "Via Fermilab",
    "task_types": [
      "Trigger selection",
      "Time-series anomaly detection"
    ],
    "ai_capability_measured": [
      "Low-latency event detection"
    ],
    "metrics": [
      "Detection efficiency",
      "Latency"
    ],
    "models": [
      "CNN",
      "LSTM (planned)"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Benchmark (in progress)",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Prototype models demonstrated on SONIC platform\n",
    "contact": {
      "name": "Andrew J. Morgan",
      "email": "unknown"
    },
    "cite": [
      "@misc{abud2021deep,\n  title={Deep Underground Neutrino Experiment (DUNE) Near Detector Conceptual Design Report}, \n  author={A. Abed Abud and B. Abi and R. Acciarri and M. A. Acero and G. Adamov and D. Adams and M. Adinolfi and A. Aduszkiewicz and Z. Ahmad and J. Ahmed and T. Alion and S. Alonso Monsalve and M. Alrashed and C. Alt and A. Alton and P. Amedo and J. Anderson and C. Andreopoulos and M. P. Andrews and F. Andrianala and S. Andringa and N. Anfimov and A. Ankowski and M. Antonova and S. Antusch and A. Aranda-Fernandez and A. Ariga and L. O. Arnold and M. A. Arroyave and J. Asaadi and A. Aurisano and V. Aushev and D. Autiero and M. Ayala-Torres and F. Azfar and H. Back and J. J. Back and C. Backhouse and P. Baesso and I. Bagaturia and L. Bagby and S. Balasubramanian and P. Baldi and B. Baller and B. Bambah and F. Barao and G. Barenboim and G. J. Barker and W. Barkhouse and C. Barnes and G. Barr and J. Barranco Monarca and N. Barros and J. L. Barrow and A. Basharina-Freshville and A. Bashyal and V. Basque and E. Belchior and J. B. R. Battat and F. Battisti and F. Bay and J. L. Bazo Alba and J. F. Beacom and E. Bechetoille and B. Behera and L. Bellantoni and G. Bellettini and V. Bellini and O. Beltramello and D. Belver and N. Benekos and F. Bento Neves and S. Berkman and P. Bernardini and R. M. Berner and H. Berns and S. Bertolucci and M. Betancourt and A. Betancur Rodr{\\'i}guez and M. Bhattacharjee and S. Bhuller and B. Bhuyan and S. Biagi and J. Bian and M. Biassoni and K. Biery and B. Bilki and M. Bishai and A. Bitadze and A. Blake and F. D. M. Blaszczyk and G. C. Blazey and E. Blucher and J. Boissevain and S. Bolognesi and T. Bolton and L. Bomben and M. Bonesini and M. Bongrand and F. Bonini and A. Booth and C. Booth and S. Bordoni and A. Borkum and T. Boschi and N. Bostan and P. Bour and C. Bourgeois and S. B. Boyd and D. Boyden and J. Bracinik and D. Braga and D. Brailsford and A. Brandt and J. Bremer and C. Brew and E. Brianne and S. J. Brice and C. Brizzolari and C. Bromberg and G. Brooijmans and J. Brooke and A. Bross and G. Brunetti and M. Brunetti and N. Buchanan and H. Budd and D. Caiulo and P. Calafiura and J. Calcutt and M. Calin and S. Calvez and E. Calvo and A. Caminata and M. Campanelli and K. Cankocak and D. Caratelli and G. Carini and B. Carlus and P. Carniti and I. Caro Terrazas and H. Carranza and T. Carroll and J. F. Casta\\~{n}o Forero and A. Castillo and C. Castromonte and E. Catano-Mur and C. Cattadori and F. Cavalier and F. Cavanna and S. Centro and G. Cerati and A. Cervelli and A. Cervera Villanueva and M. Chalifour and A. Chappell and E. Chardonnet and N. Charitonidis and A. Chatterjee and S. Chattopadhyay and H. Chen and M. Chen and Y. Chen and Z. Chen and D. Cherdack and C. Chi and S. Childress and A. Chiriacescu and G. Chisnall and K. Cho and S. Choate and D. Chokheli and S. Choubey and A. Christensen and D. Christian and G. Christodoulou and A. Chukanov and E. Church and P. Clarke and T. E. Coan and A. G. Cocco and J. A. B. Coelho and E. Conley and R. Conley and J. M. Conrad and M. Convery and S. Copello and L. Corwin and L. Cremaldi and L. Cremonesi and J. I. Crespo-Anad\\'{o}n and E. Cristaldo and R. Cross and A. Cudd and C. Cuesta and Y. Cui and D. Cussans and M. Dabrowski and O. Dalager and H. da Motta and L. Da Silva Peres and C. David and Q. David and G. S. Davies and S. Davini and J. Dawson and K. De and R. M. De Almeida and P. Debbins and I. De Bonis and M. P. Decowski and A. de Gouv{\\^e}a and P. C. De Holanda and I. L. De Icaza Astiz and A. Deisting and P. De Jong and A. Delbart and D. Delepine and M. Delgado and A. Dell'Acqua and P. De Lurgio and J. R. T. de Mello Neto and D. M. DeMuth and S. Dennis and C. Densham and G. W. Deptuch and A. De Roeck and V. De Romeri and G. De Souza and R. Dharmapalan and F. Diaz and J. S. D\\'{i}az and S. Di Domizio and L. Di Giulio and P. Ding and L. Di Noto and C. Distefano and R. Diurba and M. Diwan and Z. Djurcic and N. Dokania and S. Dolan and M. J. Dolinski and L. Domine and D. Douglas and D. Douillet and G. Drake and F. Drielsma and D. Duchesneau and K. Duffy and P. Dunne and T. Durkin and H. Duyang and O. Dvornikov and D. A. Dwyer and A. S. Dyshkant and M. Eads and A. Earle and D. Edmunds and J. Eisch and L. Emberger and S. Emery and A. Ereditato and C. O. Escobar and G. Eurin and J. J. Evans and E. Ewart and A. C. Ezeribe and K. Fahey and A. Falcone and C. Farnese and Y. Farzan and J. Felix and M. Fernandes Carneiro da Silva and E. Fernandez-Martinez and P. Fernandez Menendez and F. Ferraro and L. Fields and F. Filthaut and A. Fiorentini and R. S. Fitzpatrick and W. Flanagan and B. Fleming and R. Flight and D. V. Forero and J. Fowler and W. Fox and J. Franc and K. Francis and D. Franco and J. Freeman and J. Freestone and J. Fried and A. Friedland and S. Fuess and I. Furic and A. P. Furmanski and A. Gago and H. Gallagher and A. Gallas and A. Gallego-Ros and N. Gallice and V. Galymov and E. Gamberini and T. Gamble and R. Gandhi and R. Gandrajula and F. Gao and S. Gao and D. Garcia-Gamez and M. \\'{A} Garc\\'{i}a-Peris and S. Gardiner and D. Gastler and G. Ge and B. Gelli and A. Gendotti and S. Gent and Z. Ghorbani-Moghaddam and D. Gibin and I. Gil-Botella and S. Gilligan and C. Girerd and A. K. Giri and D. Gnani and O. Gogota and M. Gold and S. Gollapinni and K. Gollwitzer and R. A. Gomes and L. V. Gomez Bermeo and L. S. Gomez Fajardo and F. Gonnella and J. A. Gonzalez-Cuevas and D. Gonzalez-Diaz and M. Gonzalez-Lopez and M. C. Goodman and O. Goodwin and S. Goswami and C. Gotti and E. Goudzovski and C. Grace and M. Graham and R. Gran and E. Granados and P. Granger and A. Grant and C. Grant and D. Gratieri and P. Green and L. Greenler and J. Greer and W. C. Griffith and M. Groh and J. Grudzinski and K. Grzelak and W. Gu and V. Guarino and R. Guenette and E. Guerard and A. Guglielmi and B. Guo and K. K. Guthikonda and R. Gutierrez and P. Guzowski and M. M. Guzzo and S. Gwon and A. Habig and H. Hadavand and R. Haenni and A. Hahn and J. Haiston and P. Hamacher-Baumann and T. Hamernik and P. Hamilton and J. Han and D. A. Harris and J. Hartnell and J. Harton and T. Hasegawa and C. Hasnip and R. Hatcher and K. W. Hatfield and A. Hatzikoutelis and C. Hayes and E. Hazen and A. Heavey and K. M. Heeger and J. Heise and K. Hennessy and S. Henry and M. A. Hernandez Morquecho and K. Herner and L. Hertel and V Hewes and A. Higuera and T. Hill and S. J. Hillier and A. Himmel and J. Hoff and C. Hohl and A. Holin and E. Hoppe and G. A. Horton-Smith and M. Hostert and A. Hourlier and B. Howard and R. Howell and J. Huang and J. Huang and J. Hugon and G. Iles and N. Ilic and A. M. Iliescu and R. Illingworth and A. Ioannisian and L. Isenhower and R. Itay and A. Izmaylov and S. Jackson and V. Jain and E. James and B. Jargowsky and F. Jediny and D. Jena and Y. S. Jeong and C. Jes\\'{u}s-Valls and X. Ji and L. Jiang and S. Jim\\'{e}nez and A. Jipa and R. Johnson and B. Jones and S. B. Jones and M. Judah and C. K. Jung and T. Junk and Y. Jwa and M. Kabirnezhad and A. Kaboth and I. Kadenko and I. Kakorin and F. Kamiya and N. Kaneshige and G. Karagiorgi and G. Karaman and A. Karcher and M. Karolak and Y. Karyotakis and S. Kasai and S. P. Kasetti and L. Kashur and N. Kazaryan and E. Kearns and P. Keener and K. J. Kelly and E. Kemp and O. Kemularia and W. Ketchum and S. H. Kettell and M. Khabibullin and A. Khotjantsev and A. Khvedelidze and D. Kim and B. King and B. Kirby and M. Kirby and J. Klein and K. Koehler and L. W. Koerner and S. Kohn and P. P. Koller and L. Kolupaeva and M. Kordosky and T. Kosc and U. Kose and V. A. Kosteleck\\'{y} and K. Kothekar and F. Krennrich and I. Kreslo and Y. Kudenko and V. A. Kudryavtsev and S. Kulagin and J. Kumar and P. Kumar and P. Kunze and N. Kurita and C. Kuruppu and V. Kus and T. Kutter and A. Lambert and B. Land and K. Lande and C. E. Lane and K. Lang and T. Langford and J. Larkin and P. Lasorak and D. Last and C. Lastoria and A. Laundrie and A. Lawrence and I. Lazanu and R. LaZur and T. Le and S. Leardini and J. Learned and P. LeBrun and T. LeCompte and G. Lehmann Miotto and R. Lehnert and M. A. Leigui de Oliveira and M. Leitner and L. Li and S. W. Li and T. Li and Y. Li and H. Liao and C. S. Lin and Q. Lin and S. Lin and A. Lister and B. R. Littlejohn and J. Liu and S. Lockwitz and T. Loew and M. Lokajicek and I. Lomidze and K. Long and K. Loo and D. Lorca and T. Lord and J. M. LoSecco and W. C. Louis and X. -G. Lu and K. B. Luk and X. Luo and N. Lurkin and T. Lux and V. P. Luzio and D. MacFarlane and A. A. Machado and P. Machado and C. T. Macias and J. R. Macier and A. Maddalena and A. Madera and P. Madigan and S. Magill and K. Mahn and A. Maio and A. Major and J. A. Maloney and G. Mandrioli and R. C. Mandujano and J. Maneira and L. Manenti and S. Manly and A. Mann and K. Manolopoulos and M. Manrique Plata and V. N. Manyam and L. Manzanillas and M. Marchan and A. Marchionni and W. Marciano and D. Marfatia and C. Mariani and J. Maricic and R. Marie and F. Marinho and A. D. Marino and D. Marsden and M. Marshak and C. M. Marshall and J. Marshall and J. Marteau and J. Martin-Albo and N. Martinez and D. A. Martinez Caicedo and S. Martynenko and K. Mason and A. Mastbaum and M. Masud and S. Matsuno and J. Matthews and C. Mauger and N. Mauri and K. Mavrokoridis and I. Mawby and R. Mazza and A. Mazzacane and E. Mazzucato and T. McAskill and E. McCluskey and N. McConkey and K. S. McFarland and C. McGrew and A. McNab and A. Mefodiev and P. Mehta and P. Melas and O. Mena and S. Menary and H. Mendez and D. P. M{\\'e}ndez and A. Menegolli and G. Meng and M. D. Messier and W. Metcalf and T. Mettler and M. Mewes and H. Meyer and T. Miao and G. Michna and T. Miedema and J. Migenda and V. Mikola and R. Milincic and W. Miller and J. Mills and C. Milne and O. Mineev and O. G. Miranda and S. Miryala and C. S. Mishra and S. R. Mishra and A. Mislivec and D. Mladenov and I. Mocioiu and K. Moffat and N. Moggi and R. Mohanta and T. A. Mohayai and N. Mokhov and J. Molina and L. Molina Bueno and A. Montanari and C. Montanari and D. Montanari and L. M. Montano Zetina and J. Moon and M. Mooney and A. F. Moor and D. Moreno and C. Morris and C. Mossey and E. Motuk and C. A. Moura and J. Mousseau and W. Mu and L. Mualem and J. Mueller and M. Muether and S. Mufson and F. Muheim and A. Muir and M. Mulhearn and D. Munford and H. Muramatsu and S. Murphy and J. Musser and J. Nachtman and S. Nagu and M. Nalbandyan and R. Nandakumar and D. Naples and S. Narita and D. Navas-Nicol\\'{a}s and A. Navrer-Agasson and N. Nayak and M. Nebot-Guinot and K. Negishi and J. K. Nelson and J. Nesbit and M. Nessi and D. Newbold and M. Newcomer and D. Newhart and H. Newton and R. Nichol and F. Nicolas-Arnaldos and E. Niner and K. Nishimura and A. Norman and A. Norrick and R. Northrop and P. Novella and J. A. Nowak and M. Oberling and J. P. Ochoa-Ricoux and A. Olivares Del Campo and A. Olivier and A. Olshevskiy and Y. Onel and Y. Onishchuk and J. Ott and L. Pagani and S. Pakvasa and G. Palacio and O. Palamara and S. Palestini and J. M. Paley and M. Pallavicini and C. Palomares and J. L. Palomino-Gallo and E. Pantic and V. Paolone and V. Papadimitriou and R. Papaleo and A. Papanestis and S. Paramesvaran and S. Parke and Z. Parsa and M. Parvu and S. Pascoli and L. Pasqualini and J. Pasternak and J. Pater and C. Patrick and L. Patrizii and R. B. Patterson and S. J. Patton and T. Patzak and A. Paudel and B. Paulos and L. Paulucci and Z. Pavlovic and G. Pawloski and D. Payne and V. Pec and S. J. M. Peeters and E. Pennacchio and A. Penzo and O. L. G. Peres and J. Perry and D. Pershey and G. Pessina and G. Petrillo and C. Petta and R. Petti and F. Piastra and L. Pickering and F. Pietropaolo and R. Plunkett and R. Poling and X. Pons and N. Poonthottathil and S. Pordes and J. Porter and M. Potekhin and R. Potenza and B. V. K. S. Potukuchi and J. Pozimski and M. Pozzato and S. Prakash and T. Prakash and S. Prince and D. Pugnere and X. Qian and M. C. Queiroga Bazetto and J. L. Raaf and V. Radeka and J. Rademacker and B. Radics and A. Rafique and E. Raguzin and M. Rai and M. Rajaoalisoa and I. Rakhno and A. Rakotonandrasana and L. Rakotondravohitra and Y. A. Ramachers and R. Rameika and M. A. Ramirez Delgado and B. Ramson and A. Rappoldi and G. Raselli and P. Ratoff and S. Raut and R. F. Razakamiandra and J. S. Real and B. Rebel and M. Reggiani-Guzzo and T. Rehak and J. Reichenbacher and S. D. Reitzner and H. Rejeb Sfar and A. Renshaw and S. Rescia and F. Resnati and A. Reynolds and C. Riccio and G. Riccobene and L. C. J. Rice and J. Ricol and A. Rigamonti and Y. Rigaut and D. Rivera and L. Rochester and M. Roda and P. Rodrigues and M. J. Rodriguez Alonso and E. Rodriguez Bonilla and J. Rodriguez Rondon and S. Rosauro-Alcaraz and M. Rosenberg and P. Rosier and B. Roskovec and M. Rossella and J. Rout and P. Roy and S. Roy and A. Rubbia and C. Rubbia and F. C. Rubio and B. Russell and D. Ruterbories and R. Saakyan and S. Sacerdoti and T. Safford and R. Sahay and N. Sahu and P. Sala and N. Samios and O. Samoylov and M. C. Sanchez and D. A. Sanders and D. Sankey and S. Santana and M. Santos-Maldonado and N. Saoulidou and P. Sapienza and C. Sarasty and I. Sarcevic and G. Savage and V. Savinov and A. Scaramelli and A. Scarff and A. Scarpelli and T. Schaffer and H. Schellman and P. Schlabach and D. Schmitz and K. Scholberg and A. Schukraft and E. Segreto and J. Sensenig and I. Seong and A. Sergi and D. Sgalaberna and M. H. Shaevitz and S. Shafaq and M. Shamma and R. Sharankova and H. R. Sharma and R. Sharma and R. Kumar and T. Shaw and C. Shepherd-Themistocleous and S. Shin and D. Shooltz and R. Shrock and L. Simard and F. Simon and N. Simos and J. Sinclair and G. Sinev and J. Singh and J. Singh and V. Singh and R. Sipos and F. W. Sippach and G. Sirri and A. Sitraka and K. Siyeon and K. Skarpaas VIII and A. Smith and E. Smith and P. Smith and J. Smolik and M. Smy and E. L. Snider and P. Snopok and M. Soares Nunes and H. Sobel and M. Soderberg and C. J. Solano Salinas and S. S\\\"{o}ldner-Rembold and N. Solomey and V. Solovov and W. E. Sondheim and M. Sorel and J. Soto-Oton and A. Sousa and K. Soustruznik and F. Spagliardi and M. Spanu and J. Spitz and N. J. C. Spooner and K. Spurgeon and R. Staley and M. Stancari and L. Stanco and R. Stanley and R. Stein and H. M. Steiner and J. Stewart and B. Stillwell and J. Stock and F. Stocker and T. Stokes and M. Strait and T. Strauss and S. Striganov and A. Stuart and J. G. Suarez and H. Sullivan and D. Summers and A. Surdo and V. Susic and L. Suter and C. M. Sutera and R. Svoboda and B. Szczerbinska and A. M. Szelc and R. Talaga and H. A. Tanaka and B. Tapia Oregui and A. Tapper and S. Tariq and E. Tatar and R. Tayloe and A. M. Teklu and M. Tenti and K. Terao and C. A. Ternes and F. Terranova and G. Testera and A. Thea and J. L. Thompson and C. Thorn and S. C. Timm and J. Todd and A. Tonazzo and D. Torbunov and M. Torti and M. Tortola and F. Tortorici and D. Totani and M. Toups and C. Touramanis and J. Trevor and S. Trilov and W. H. Trzaska and Y. T. Tsai and Z. Tsamalaidze and K. V. Tsang and N. Tsverava and S. Tufanli and C. Tull and E. Tyley and M. Tzanov and M. A. Uchida and J. Urheim and T. Usher and S. Uzunyan and M. R. Vagins and P. Vahle and G. A. Valdiviesso and E. Valencia and Z. Vallari and J. W. F. Valle and S. Vallecorsa and R. Van Berg and R. G. Van de Water and F. Varanini and D. Vargas and G. Varner and J. Vasel and S. Vasina and G. Vasseur and N. Vaughan and K. Vaziri and S. Ventura and A. Verdugo and S. Vergani and M. A. Vermeulen and M. Verzocchi and M. Vicenzi and H. Vieira de Souza and C. Vignoli and C. Vilela and B. Viren and T. Vrba and T. Wachala and A. V. Waldron and M. Wallbank and H. Wang and J. Wang and M. H. L. S. Wang and Y. Wang and Y. Wang and K. Warburton and D. Warner and M. Wascko and D. Waters and A. Watson and P. Weatherly and A. Weber and M. Weber and H. Wei and A. Weinstein and D. Wenman and M. Wetstein and A. White and L. H. Whitehead and D. Whittington and M. J. Wilking and C. Wilkinson and Z. Williams and F. Wilson and R. J. Wilson and J. Wolcott and T. Wongjirad and A. Wood and K. Wood and E. Worcester and M. Worcester and C. Wret and W. Wu and W. Wu and Y. Xiao and E. Yandel and G. Yang and K. Yang and S. Yang and T. Yang and A. Yankelevich and N. Yershov and K. Yonehara and T. Young and B. Yu and H. Yu and J. Yu and W. Yuan and R. Zaki and J. Zalesak and L. Zambelli and B. Zamorano and A. Zani and L. Zazueta and G. Zeit and G. P. Zeller and J. Zennamo and K. Zeug and C. Zhang and M. Zhao and E. Zhivun and G. Zhu and P. Zilberman and E. D. Zimmerman and M. Zito and S. Zucchelli and J. Zuklin and V. Zutshi and R. Zwaska},\n  year={2021},\n  eprint={2103.13910},\n  archivePrefix={arXiv},\n  primaryClass={physics.ins-det},\n  url={https://arxiv.org/abs/2103.13910}, \n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "DUNE SONIC data",
          "url": ""
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1_xI6kpeb3zSCMY_rzKV9s-MCMi7kHAdsLLV0eHxG9kM"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "no",
      "benchmark_ready": "No"
    },
    "ratings": {
      "software": {
        "rating": 1,
        "reason": "Code not available; no containerization or setup provided\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Constraints like latency thresholds are described qualitatively but not numerically defined\n"
      },
      "dataset": {
        "rating": 3,
        "reason": "Dataset lacks a public URL; FAIR metadata and versioning are missing\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Metrics are relevant but no benchmark baseline or detailed evaluation guidance is provided\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Autoencoder prototype exists but is not reproducible; RL model still in development\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Documentation exists only in slides/GDocs; no implementation guide or structured release\n"
      }
    },
    "id": "dune"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "MassSpecGym - De novo molecule generation",
    "url": "https://neurips.cc/virtual/2024/poster/97823",
    "doi": "unknown",
    "domain": [
      "Chemistry"
    ],
    "focus": "Benchmark suite for discovery and identification of molecules via MS/MS",
    "keywords": [
      "mass spectrometry",
      "molecular structure",
      "de novo generation",
      "retrieval",
      "dataset"
    ],
    "summary": "MassSpecGym curates the largest public MS/MS dataset with three standardized tasks-de novo structure \ngeneration, molecule retrieval, and spectrum simulation-using challenging generalization splits to \npropel ML-driven molecule discovery.\n",
    "licensing": "unknown",
    "task_types": [
      "De novo generation",
      "Retrieval",
      "Simulation"
    ],
    "ai_capability_measured": [
      "Molecular identification and generation from spectral data"
    ],
    "metrics": [
      "Structure accuracy",
      "Retrieval precision",
      "Simulation MSE"
    ],
    "models": [
      "Graph-based generative models",
      "Retrieval baselines"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Dataset, Benchmark",
    "ml_task": [
      "Generation, retrieval, simulation"
    ],
    "solutions": "0",
    "notes": "Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks.\n",
    "contact": {
      "name": "Roman Bushuiev",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c6c31413,\n  author = {Bushuiev, Roman and Bushuiev, Anton and de Jonge, Niek F. and \n            Young, Adamo and Kretschmer, Fleming and Samusevich, Raman and \n            Heirman, Janne and Wang, Fei and Zhang, Luke and D\\\"{u}hrkop, Kai and \n            Ludwig, Marcus and Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and \n            Schmid, Robin and Greiner, Russell and Wang, Bo and Wishart, David S. and \n            Liu, Li-Ping and Rousu, Juho and Bittremieux, Wout and Rost, Hannes and \n            Mak, Tytus D. and Hassoun, Soha and Huber, Florian and van der Hooft, Justin J.J. and \n            Stravs, Michael A. and B\\\"{o}cker, Sebastian and Sivic, Josef and Pluskal, Tom\\'{a}\\v{s}},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {110010--110027},\n  publisher = {Curran Associates, Inc.},\n  title = {MassSpecGym: A benchmark for the discovery and identification of molecules},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Open-source GitHub repository available; baseline models and training code partially\nprovided but overall framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Clearly defined tasks including molecule generation, retrieval, and spectrum simulation,\nscoped for MS/MS molecular identification.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Largest public MS/MS dataset with extensive annotations; minor point deducted for\nlack of explicit train/validation/test splits.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE\nused consistently.\n"
      },
      "reference_solution": {
        "rating": 3.5,
        "reason": "CNN-based baselines are referenced, but pretrained weights and comprehensive training\npipelines are not fully documented.\n"
      },
      "documentation": {
        "rating": 1,
        "reason": "Paper and poster describe benchmark goals and design, but documentation and user\nguides are minimal and repo status uncertain.\n"
      }
    },
    "id": "massspecgym_-_de_novo_molecule_generation"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "MassSpecGym - Molecule Retrieval",
    "url": "https://neurips.cc/virtual/2024/poster/97823",
    "doi": "unknown",
    "domain": [
      "Chemistry"
    ],
    "focus": "Benchmark suite for discovery and identification of molecules via MS/MS",
    "keywords": [
      "mass spectrometry",
      "molecular structure",
      "de novo generation",
      "retrieval",
      "dataset"
    ],
    "summary": "MassSpecGym curates the largest public MS/MS dataset with three standardized tasks-de novo structure \ngeneration, molecule retrieval, and spectrum simulation-using challenging generalization splits to \npropel ML-driven molecule discovery.\n",
    "licensing": "unknown",
    "task_types": [
      "De novo generation",
      "Retrieval",
      "Simulation"
    ],
    "ai_capability_measured": [
      "Molecular identification and generation from spectral data"
    ],
    "metrics": [
      "Structure accuracy",
      "Retrieval precision",
      "Simulation MSE"
    ],
    "models": [
      "Graph-based generative models",
      "Retrieval baselines"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Dataset, Benchmark",
    "ml_task": [
      "Generation, retrieval, simulation"
    ],
    "solutions": "0",
    "notes": "Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks.\n",
    "contact": {
      "name": "Roman Bushuiev",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c6c31413,\n  author = {Bushuiev, Roman and Bushuiev, Anton and de Jonge, Niek F. and \n            Young, Adamo and Kretschmer, Fleming and Samusevich, Raman and \n            Heirman, Janne and Wang, Fei and Zhang, Luke and D\\\"{u}hrkop, Kai and \n            Ludwig, Marcus and Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and \n            Schmid, Robin and Greiner, Russell and Wang, Bo and Wishart, David S. and \n            Liu, Li-Ping and Rousu, Juho and Bittremieux, Wout and Rost, Hannes and \n            Mak, Tytus D. and Hassoun, Soha and Huber, Florian and van der Hooft, Justin J.J. and \n            Stravs, Michael A. and B\\\"{o}cker, Sebastian and Sivic, Josef and Pluskal, Tom\\'{a}\\v{s}},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {110010--110027},\n  publisher = {Curran Associates, Inc.},\n  title = {MassSpecGym: A benchmark for the discovery and identification of molecules},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Open-source GitHub repository available; baseline models and training code partially\nprovided but overall framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Clearly defined tasks including molecule generation, retrieval, and spectrum simulation,\nscoped for MS/MS molecular identification.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Largest public MS/MS dataset with extensive annotations; minor point deducted for\nlack of explicit train/validation/test splits.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE\nused consistently.\n"
      },
      "reference_solution": {
        "rating": 3.5,
        "reason": "CNN-based baselines are referenced, but pretrained weights and comprehensive training\npipelines are not fully documented.\n"
      },
      "documentation": {
        "rating": 1,
        "reason": "Paper and poster describe benchmark goals and design, but documentation and user\nguides are minimal and repo status uncertain.\n"
      }
    },
    "id": "massspecgym_-_molecule_retrieval"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "MassSpecGym - Spectrum Simulation",
    "url": "https://neurips.cc/virtual/2024/poster/97823",
    "doi": "unknown",
    "domain": [
      "Chemistry"
    ],
    "focus": "Benchmark suite for discovery and identification of molecules via MS/MS",
    "keywords": [
      "mass spectrometry",
      "molecular structure",
      "de novo generation",
      "retrieval",
      "dataset"
    ],
    "summary": "MassSpecGym curates the largest public MS/MS dataset with three standardized tasks-de novo structure \ngeneration, molecule retrieval, and spectrum simulation-using challenging generalization splits to \npropel ML-driven molecule discovery.\n",
    "licensing": "unknown",
    "task_types": [
      "De novo generation",
      "Retrieval",
      "Simulation"
    ],
    "ai_capability_measured": [
      "Molecular identification and generation from spectral data"
    ],
    "metrics": [
      "Structure accuracy",
      "Retrieval precision",
      "Simulation MSE"
    ],
    "models": [
      "Graph-based generative models",
      "Retrieval baselines"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Dataset, Benchmark",
    "ml_task": [
      "Generation, retrieval, simulation"
    ],
    "solutions": "0",
    "notes": "Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks.\n",
    "contact": {
      "name": "Roman Bushuiev",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c6c31413,\n  author = {Bushuiev, Roman and Bushuiev, Anton and de Jonge, Niek F. and \n            Young, Adamo and Kretschmer, Fleming and Samusevich, Raman and \n            Heirman, Janne and Wang, Fei and Zhang, Luke and D\\\"{u}hrkop, Kai and \n            Ludwig, Marcus and Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and \n            Schmid, Robin and Greiner, Russell and Wang, Bo and Wishart, David S. and \n            Liu, Li-Ping and Rousu, Juho and Bittremieux, Wout and Rost, Hannes and \n            Mak, Tytus D. and Hassoun, Soha and Huber, Florian and van der Hooft, Justin J.J. and \n            Stravs, Michael A. and B\\\"{o}cker, Sebastian and Sivic, Josef and Pluskal, Tom\\'{a}\\v{s}},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {110010--110027},\n  publisher = {Curran Associates, Inc.},\n  title = {MassSpecGym: A benchmark for the discovery and identification of molecules},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Open-source GitHub repository available; baseline models and training code partially\nprovided but overall framework maturity is moderate.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Clearly defined tasks including molecule generation, retrieval, and spectrum simulation,\nscoped for MS/MS molecular identification.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Largest public MS/MS dataset with extensive annotations; minor point deducted for\nlack of explicit train/validation/test splits.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE\nused consistently.\n"
      },
      "reference_solution": {
        "rating": 3.5,
        "reason": "CNN-based baselines are referenced, but pretrained weights and comprehensive training\npipelines are not fully documented.\n"
      },
      "documentation": {
        "rating": 1,
        "reason": "Paper and poster describe benchmark goals and design, but documentation and user\nguides are minimal and repo status uncertain.\n"
      }
    },
    "id": "massspecgym_-_spectrum_simulation"
  },
  {
    "date": "2025-03-03",
    "version": "v1.0",
    "last_updated": "2025-03",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-03-03",
    "name": "HDR ML Anomaly Challenge - Gravitational Waves",
    "url": "https://www.codabench.org/competitions/2626/",
    "doi": "10.48550/arXiv.2503.02112",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets",
    "keywords": [
      "anomaly detection",
      "gravitational waves",
      "astrophysics",
      "time-series"
    ],
    "summary": "A benchmark for detecting anomalous transient gravitational-wave signals, including \"unknown-unknowns,\" using preprocessed LIGO time-series at 4096 Hz. Competitors submit inference models on Codabench for continuous 50 ms segments from dual interferometers. \n",
    "licensing": "NA",
    "task_types": [
      "Anomaly Detection"
    ],
    "ai_capability_measured": [
      "Novel event detection in physical signals"
    ],
    "metrics": [
      "ROC-AUC",
      "Precision/Recall"
    ],
    "models": [
      "Deep latent CNNs",
      "Autoencoders"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Dataset",
    "ml_task": [
      "Anomaly Detection"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench.\n",
    "contact": {
      "name": "HDR A3D3 Team",
      "email": "unknown"
    },
    "cite": [
      "@misc{campolongo2025buildingmachinelearningchallenges,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Sal Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://www.codabench.org/competitions/2626/"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 4,
        "reason": "Benchmark platform provided on Codabench with starter kits and submission infrastructure.\nCode and baseline models are publicly accessible but not extensively maintained beyond the challenge.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Well-defined anomaly detection task on gravitational-wave time series with clear input/output\nexpectations and challenge constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Uses preprocessed LIGO/Virgo time series data at 4096 Hz, publicly available and standard in astrophysics.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "ROC-AUC, precision, and recall metrics are clearly specified and appropriate for anomaly detection.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Baseline deep latent CNNs and autoencoders are provided and reproducible, but not extensively documented.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Documentation includes challenge instructions, starter kit details, and baseline descriptions,\nbut could benefit from more thorough tutorials and code walkthroughs.\n"
      }
    },
    "id": "hdr_ml_anomaly_challenge_-_gravitational_waves"
  },
  {
    "date": "2025-03-03",
    "version": "v1.0",
    "last_updated": "2025-03",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-03-03",
    "name": "HDR ML Anomaly Challenge - Butterfly",
    "url": "https://www.codabench.org/competitions/3764/",
    "doi": "10.48550/arXiv.2503.02112",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset",
    "keywords": [
      "anomaly detection",
      "computer vision",
      "genomics",
      "butterfly hybrids"
    ],
    "summary": "Image-based challenge for detecting butterfly hybrids in microscopy-driven species data. Participants evaluate models on Codabench using image segmentation/classification. \n",
    "licensing": "NA",
    "task_types": [
      "Anomaly Detection"
    ],
    "ai_capability_measured": [
      "Hybrid detection in biological systems"
    ],
    "metrics": [
      "Classification accuracy",
      "F1 score"
    ],
    "models": [
      "CNN-based detectors"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Dataset",
    "ml_task": [
      "Anomaly Detection"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Hybrid detection benchmarks hosted on Codabench\n",
    "contact": {
      "name": "Imageomics/HDR Team",
      "email": "unknown"
    },
    "cite": [
      "@misc{campolongo2025buildingmachinelearningchallenges2,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Sal Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://www.codabench.org/competitions/3764/"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Codabench platform provides submission infrastructure but no fully maintained\ncode repository or reproducible baseline implementations.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task is clearly described with domain-specific anomaly detection objectives and\nrelevant physics motivation.\n"
      },
      "dataset": {
        "rating": 3,
        "reason": "Dataset consists of real detector data with synthetic anomaly injections; access\nis restricted and requires NDA, limiting openness and FAIR compliance.\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "Standard metrics (ROC, F1, precision) are used; evaluation protocols are clear\nbut not deeply elaborated.\n"
      },
      "reference_solution": {
        "rating": 2,
        "reason": "Baselines are partially described but lack public code or reproducible execution\nscripts.\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Challenge website provides basic descriptions and evaluation metrics but lacks\ncomprehensive tutorials or example workflows.\n"
      }
    },
    "id": "hdr_ml_anomaly_challenge_-_butterfly"
  },
  {
    "date": "2025-03-03",
    "version": "v1.0",
    "last_updated": "2025-03",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-03-03",
    "name": "HDR ML Anomaly Challenge - Sea Level Rise",
    "url": "https://www.codabench.org/competitions/3223/",
    "doi": "10.48550/arXiv.2503.02112",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery",
    "keywords": [
      "anomaly detection",
      "climate science",
      "sea-level rise",
      "time-series",
      "remote sensing"
    ],
    "summary": "A challenge combining North Atlantic sea-level time-series and satellite imagery to detect flooding anomalies. Models submitted via Codabench. \n",
    "licensing": "NA",
    "task_types": [
      "Anomaly Detection"
    ],
    "ai_capability_measured": [
      "Detection of environmental anomalies"
    ],
    "metrics": [
      "ROC-AUC",
      "Precision/Recall"
    ],
    "models": [
      "CNNs, RNNs, Transformers"
    ],
    "ml_motif": [
      "Anomaly Detection"
    ],
    "type": "Dataset",
    "ml_task": [
      "Anomaly Detection"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Sponsored by NSF HDR; integrates sensor and satellite data. \n",
    "contact": {
      "name": "HDR A3D3 Team",
      "email": "unknown"
    },
    "cite": [
      "@misc{campolongo2025buildingmachinelearningchallenges3,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and \n                   Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and \n                   Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and \n                   Aneesh Subramanian and Philip Harris and Advaith Anand and \n                   David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and \n                   Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and \n                   Mohammad Ahmadi Gharehtoragh and Sal Alonso Monsalve and Marta Babicz and \n                   Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and \n                   Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and \n                   David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and \n                   Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and \n                   Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and \n                   Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and \n                   Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and \n                   Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and \n                   Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and \n                   Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and \n                   Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and \n                   Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and \n                   Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and \n                   Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and \n                   Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and \n                   Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and \n                   Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and \n                   Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and \n                   Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://www.codabench.org/competitions/3223/"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 2,
        "reason": "Benchmark platform exists on Codabench, but no baseline code or maintained repository\nfor reference solutions provided yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Well-defined anomaly detection task combining satellite imagery and time-series data,\nwith clear physical and domain-specific framing.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Uses preprocessed, public, and well-structured sensor and satellite data for the\nNorth Atlantic sea-level rise region.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Standard metrics such as ROC-AUC, precision, and recall are specified and suitable\nfor the anomaly detection tasks.\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "No starter models or baseline implementations linked or provided publicly.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Challenge page, starter kits, and related papers offer strong guidance for participants.\n"
      }
    },
    "id": "hdr_ml_anomaly_challenge_-_sea_level_rise"
  },
  {
    "date": "2025-01-24",
    "version": "v1.0",
    "last_updated": "2025-02",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2025-01-24",
    "name": "Single Qubit Readout on QICK System",
    "url": "https://github.com/fastmachinelearning/ml-quantum-readout",
    "doi": "10.48550/arXiv.2501.14663",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Real-time single-qubit state classification using FPGA firmware",
    "keywords": [
      "qubit readout",
      "hls4ml",
      "FPGA",
      "QICK"
    ],
    "summary": "Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination. :contentReference[oaicite:0]{index=0}\n",
    "licensing": "NA",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "Single-shot fidelity",
      "inference latency"
    ],
    "metrics": [
      "Accuracy",
      "Latency"
    ],
    "models": [
      "hls4ml quantized NN"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization. \n",
    "contact": {
      "name": "Javier Campos, Giuseppe Di Guglielmo",
      "email": "unknown"
    },
    "cite": [
      "@misc{diguglielmo2025endtoendworkflowmachinelearningbased,\n  archiveprefix = {arXiv},\n  author        = {Giuseppe Di Guglielmo and Botao Du and Javier Campos and Alexandra Boltasseva and Akash V. Dixit and Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and Ruichao Ma and Gabriel N. Perdue and Nhan Tran and Omer Yesilyurt and Daniel Bowring},\n  eprint        = {2501.14663},\n  primaryclass  = {quant-ph},\n  title         = {End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml},\n  url           = {https://arxiv.org/abs/2501.14663},\n  year          = {2025}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Zenodo: ml-quantum-readout dataset",
          "url": "zenodo.org/records/14427490"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "(none)"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Code and FPGA firmware available on GitHub; integration with hls4ml demonstrated.\nSome deployment details and examples are provided but overall software maturity is moderate.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "Task clearly defined: real-time single-qubit state classification with latency and\nfidelity constraints. Labeling and ground truth definitions could be more explicit.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Dataset hosted on Zenodo with structured data; however, detailed documentation on\nimage acquisition and labeling pipeline is limited.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Standard classification metrics (accuracy, latency) are used and directly relevant\nto the quantum readout task.\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "No baseline or starter models with runnable code are linked publicly.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Codabench task page and GitHub repo provide descriptions and usage instructions,\nbut detailed API or deployment tutorials are limited.\n"
      }
    },
    "id": "single_qubit_readout_on_qick_system"
  },
  {
    "date": "2023-11-20",
    "version": "v1.0",
    "last_updated": "2023-11",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2023-11-20",
    "name": "GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark",
    "url": "https://arxiv.org/abs/2311.12022",
    "doi": "10.48550/arXiv.2311.12022",
    "domain": [
      "Biology & Medicine",
      "High Energy Physics",
      "Chemistry"
    ],
    "focus": "Graduate-level, expert-validated multiple-choice questions hard even with web access",
    "keywords": [
      "Google-proof",
      "multiple-choice",
      "expert reasoning",
      "science QA"
    ],
    "summary": "Contains 448 challenging questions written by domain experts, with expert accuracy at 65% (74% discounting clear errors) and non-experts reaching just 34%. GPT-4 baseline scores ~39%-designed for scalable oversight evaluation. \n",
    "licensing": "NA",
    "task_types": [
      "Multiple choice"
    ],
    "ai_capability_measured": [
      "Scientific reasoning",
      "knowledge probing"
    ],
    "metrics": [
      "Accuracy"
    ],
    "models": [
      "GPT-4 baseline"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Multiple choice"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Google-proof, supports oversight research.\n",
    "contact": {
      "name": "David Rein (NYU)",
      "email": "unknown"
    },
    "cite": [
      "@misc{rein2023gpqagraduatelevelgoogleproofqa2,\n  archiveprefix = {arXiv},\n  author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},\n  eprint        = {2311.12022},\n  primaryclass  = {cs.AI},\n  title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},\n  url           = {https://arxiv.org/abs/2311.12022},\n  year          = {2023}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "GPQA dataset",
          "url": "zip/HuggingFace"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "(none)"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Dataset and benchmark materials are publicly available via HuggingFace and GitHub,\nbut no integrated runnable code or software framework is provided.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning.\nInput/output formats and evaluation criteria are well described.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA.\n"
      },
      "reference_solution": {
        "rating": 1,
        "reason": "No baseline implementations or starter code are linked or provided for reproduction.\n"
      },
      "documentation": {
        "rating": 3,
        "reason": "Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines.\n"
      }
    },
    "id": "gpqa_a_graduate-level_google-proof_question_and_answer_benchmark"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "SeafloorAI",
    "url": "https://neurips.cc/virtual/2024/poster/97432",
    "doi": "10.48550/arXiv.2411.00172",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Large-scale vision-language dataset for seafloor mapping and geological classification",
    "keywords": [
      "sonar imagery",
      "vision-language",
      "seafloor mapping",
      "segmentation",
      "QA"
    ],
    "summary": "A first-of-its-kind dataset covering 17,300 sq.km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus ~7M QA pairs-designed for both vision and language-based ML models in marine science\n",
    "licensing": "unknown",
    "task_types": [
      "Image segmentation",
      "Vision-language QA"
    ],
    "ai_capability_measured": [
      "Geospatial understanding",
      "multimodal reasoning"
    ],
    "metrics": [
      "Segmentation pixel accuracy",
      "QA accuracy"
    ],
    "models": [
      "SegFormer",
      "ViLT-style multimodal models"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Dataset",
    "ml_task": [
      "Segmentation, QA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Data processing code publicly available, covering five geological layers; curated with marine scientists\n",
    "contact": {
      "name": "Kien X. Nguyen",
      "email": "unknown"
    },
    "cite": [
      "@misc{nguyen2024seafloor,\n  archiveprefix = {arXiv},\n  author = {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},\n  eprint = {2411.00172},\n  primaryclass = {cs.CV},\n  title = {SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},\n  url = {https://arxiv.org/abs/2411.00172},\n  year=2024\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Sonar imagery + annotations",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "unknown"
        },
        {
          "name": "ChatGPT LLM",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Data processing code is publicly available, but no full benchmark framework or\nrunnable model implementations are provided yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Tasks (image segmentation and vision-language QA) are clearly defined with\ngeospatial and multimodal objectives well specified.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large-scale, well-annotated sonar imagery dataset with segmentation masks\nand natural language descriptions; curated with domain experts.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified\nand appropriate for the tasks.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but\nreproducible code or pretrained weights are not fully available yet.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Dataset description and data processing instructions are provided,\nbut tutorials and benchmark usage guides are limited.\n"
      }
    },
    "id": "seafloorai"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "SeafloorGenAI",
    "url": "https://neurips.cc/virtual/2024/poster/97432",
    "doi": "10.48550/arXiv.2411.00172",
    "domain": [
      "Climate & Earth Science"
    ],
    "focus": "Large-scale vision-language dataset for seafloor mapping and geological classification",
    "keywords": [
      "sonar imagery",
      "vision-language",
      "seafloor mapping",
      "segmentation",
      "QA"
    ],
    "summary": "A first-of-its-kind dataset covering 17,300 sq.km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus ~7M QA pairs-designed for both vision and language-based ML models in marine science\n",
    "licensing": "unknown",
    "task_types": [
      "Image segmentation",
      "Vision-language QA"
    ],
    "ai_capability_measured": [
      "Geospatial understanding",
      "multimodal reasoning"
    ],
    "metrics": [
      "Segmentation pixel accuracy",
      "QA accuracy"
    ],
    "models": [
      "SegFormer",
      "ViLT-style multimodal models"
    ],
    "ml_motif": [
      "Reasoning & Generalization"
    ],
    "type": "Dataset",
    "ml_task": [
      "Segmentation, QA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Data processing code publicly available, covering five geological layers; curated with marine scientists\n",
    "contact": {
      "name": "Kien X. Nguyen",
      "email": "unknown"
    },
    "cite": [
      "@misc{nguyen2024seafloor,\n  archiveprefix = {arXiv},\n  author = {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},\n  eprint = {2411.00172},\n  primaryclass = {cs.CV},\n  title = {SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},\n  url = {https://arxiv.org/abs/2411.00172},\n  year=2024\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "Sonar imagery + annotations",
          "url": "unknown"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "unknown"
        },
        {
          "name": "ChatGPT LLM",
          "url": "unknown"
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Data processing code is publicly available, but no full benchmark framework or\nrunnable model implementations are provided yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Tasks (image segmentation and vision-language QA) are clearly defined with\ngeospatial and multimodal objectives well specified.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Large-scale, well-annotated sonar imagery dataset with segmentation masks\nand natural language descriptions; curated with domain experts.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified\nand appropriate for the tasks.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but\nreproducible code or pretrained weights are not fully available yet.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Dataset description and data processing instructions are provided,\nbut tutorials and benchmark usage guides are limited.\n"
      }
    },
    "id": "seafloorgenai"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "SuperCon3D - Property Prediction",
    "url": "https://neurips.cc/virtual/2024/poster/97553",
    "doi": "unknown",
    "domain": [
      "Materials Science"
    ],
    "focus": "Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures",
    "keywords": [
      "superconductivity",
      "crystal structures",
      "equivariant GNN",
      "generative models"
    ],
    "summary": "SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates .\n",
    "licensing": "unknown",
    "task_types": [
      "Regression (Tc prediction)",
      "Generative modeling"
    ],
    "ai_capability_measured": [
      "Structure-to-property prediction",
      "structure generation"
    ],
    "metrics": [
      "MAE (Tc)",
      "Validity of generated structures"
    ],
    "models": [
      "SODNet",
      "DiffCSP-SC"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Dataset + Models",
    "ml_task": [
      "Regression, Generation"
    ],
    "solutions": "0",
    "notes": "Demonstrates advantage of combining ordered and disordered structural data in model design .\n",
    "contact": {
      "name": "Zhong Zuo",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c4e3b55e,\n  author = {Chen, Pin and Peng, Luoxuan and Jiao, Rui and Mo, Qing and Wang, Zhen and Huang, Wenbing and Liu, Yang and Lu, Yutong},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {108902--108928},\n  publisher = {Curran Associates, Inc.},\n  title = {Learning Superconductivity from Ordered and Disordered Material Structures},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Baseline models (SODNet, DiffCSP-SC) are described in the paper; however,\nfully reproducible code and pretrained models are not publicly available yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Tasks for regression (Tc prediction) and generative modeling with clear input/output\nstructures and domain constraints are well defined.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset contains 3D crystal structures and associated properties; well-curated but\nnot fully released publicly at this time.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Metrics such as MAE for Tc prediction and validity checks for generated structures\nare appropriate and clearly described.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Paper provides model architecture details and some training insights, but no\ncomplete open-source reference implementations yet.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and GitHub provide good metadata and data processing descriptions; tutorials\nand user guides could be expanded.\n"
      }
    },
    "id": "supercond_-_property_prediction"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "SuperCon3D -  Inverse Crystal Structure Generation",
    "url": "https://neurips.cc/virtual/2024/poster/97553",
    "doi": "unknown",
    "domain": [
      "Materials Science"
    ],
    "focus": "Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures",
    "keywords": [
      "superconductivity",
      "crystal structures",
      "equivariant GNN",
      "generative models"
    ],
    "summary": "SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates .\n",
    "licensing": "unknown",
    "task_types": [
      "Regression (Tc prediction)",
      "Generative modeling"
    ],
    "ai_capability_measured": [
      "Structure-to-property prediction",
      "structure generation"
    ],
    "metrics": [
      "MAE (Tc)",
      "Validity of generated structures"
    ],
    "models": [
      "SODNet",
      "DiffCSP-SC"
    ],
    "ml_motif": [
      "Generative"
    ],
    "type": "Dataset + Models",
    "ml_task": [
      "Regression, Generation"
    ],
    "solutions": "0",
    "notes": "Demonstrates advantage of combining ordered and disordered structural data in model design .\n",
    "contact": {
      "name": "Zhong Zuo",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c4e3b55e,\n  author = {Chen, Pin and Peng, Luoxuan and Jiao, Rui and Mo, Qing and Wang, Zhen and Huang, Wenbing and Liu, Yang and Lu, Yutong},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {108902--108928},\n  publisher = {Curran Associates, Inc.},\n  title = {Learning Superconductivity from Ordered and Disordered Material Structures},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Baseline models (SODNet, DiffCSP-SC) are described in the paper; however,\nfully reproducible code and pretrained models are not publicly available yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Tasks for regression (Tc prediction) and generative modeling with clear input/output\nstructures and domain constraints are well defined.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Dataset contains 3D crystal structures and associated properties; well-curated but\nnot fully released publicly at this time.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Metrics such as MAE for Tc prediction and validity checks for generated structures\nare appropriate and clearly described.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Paper provides model architecture details and some training insights, but no\ncomplete open-source reference implementations yet.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and GitHub provide good metadata and data processing descriptions; tutorials\nand user guides could be expanded.\n"
      }
    },
    "id": "supercond_-__inverse_crystal_structure_generation"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "GeSS - Track Pileup",
    "url": "https://neurips.cc/virtual/2024/poster/97816",
    "doi": "unknown",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Benchmark suite evaluating geometric deep learning models under real-world distribution shifts",
    "keywords": [
      "geometric deep learning",
      "distribution shift",
      "OOD robustness",
      "scientific applications"
    ],
    "summary": "GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access .\n",
    "licensing": "unknown",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "OOD performance in scientific settings"
    ],
    "metrics": [
      "Accuracy",
      "RMSE",
      "OOD robustness delta"
    ],
    "models": [
      "GCN",
      "EGNN",
      "DimeNet++"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Classification, Regression"
    ],
    "solutions": "0",
    "notes": "Includes no-OOD, unlabeled-OOD, and few-label scenarios .\n",
    "contact": {
      "name": "Deyu Zou",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_a8063075,\n  author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {92499--92528},\n  publisher = {Curran Associates, Inc.},\n  title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference code expected post-conference; current public software availability limited.\nBenchmark infrastructure partially described but not fully released yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Benchmark clearly defines OOD robustness scenarios with classification and regression\ntasks in scientific domains, though no explicit hardware constraints are given.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Curated datasets of 3D crystal structures and material properties are included and\npublicly available for reproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses well-established metrics such as MAE and structural validity for materials modeling,\nplus accuracy and OOD robustness deltas.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected\nto be released soon.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and poster provide solid explanation of benchmarks and scientific motivation;\nmore extensive user documentation forthcoming.\n"
      }
    },
    "id": "gess_-_track_pileup"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "GeSS - Track Signal",
    "url": "https://neurips.cc/virtual/2024/poster/97816",
    "doi": "unknown",
    "domain": [
      "High Energy Physics"
    ],
    "focus": "Benchmark suite evaluating geometric deep learning models under real-world distribution shifts",
    "keywords": [
      "geometric deep learning",
      "distribution shift",
      "OOD robustness",
      "scientific applications"
    ],
    "summary": "GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access .\n",
    "licensing": "unknown",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "OOD performance in scientific settings"
    ],
    "metrics": [
      "Accuracy",
      "RMSE",
      "OOD robustness delta"
    ],
    "models": [
      "GCN",
      "EGNN",
      "DimeNet++"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Classification, Regression"
    ],
    "solutions": "0",
    "notes": "Includes no-OOD, unlabeled-OOD, and few-label scenarios .\n",
    "contact": {
      "name": "Deyu Zou",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_a8063075,\n  author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {92499--92528},\n  publisher = {Curran Associates, Inc.},\n  title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference code expected post-conference; current public software availability limited.\nBenchmark infrastructure partially described but not fully released yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Benchmark clearly defines OOD robustness scenarios with classification and regression\ntasks in scientific domains, though no explicit hardware constraints are given.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Curated datasets of 3D crystal structures and material properties are included and\npublicly available for reproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses well-established metrics such as MAE and structural validity for materials modeling,\nplus accuracy and OOD robustness deltas.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected\nto be released soon.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and poster provide solid explanation of benchmarks and scientific motivation;\nmore extensive user documentation forthcoming.\n"
      }
    },
    "id": "gess_-_track_signal"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "GeSS - DrugOOD",
    "url": "https://neurips.cc/virtual/2024/poster/97816",
    "doi": "unknown",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Benchmark suite evaluating geometric deep learning models under real-world distribution shifts",
    "keywords": [
      "geometric deep learning",
      "distribution shift",
      "OOD robustness",
      "scientific applications"
    ],
    "summary": "GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access .\n",
    "licensing": "unknown",
    "task_types": [
      "Classification"
    ],
    "ai_capability_measured": [
      "OOD performance in scientific settings"
    ],
    "metrics": [
      "Accuracy",
      "RMSE",
      "OOD robustness delta"
    ],
    "models": [
      "GCN",
      "EGNN",
      "DimeNet++"
    ],
    "ml_motif": [
      "Classification"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Classification, Regression"
    ],
    "solutions": "0",
    "notes": "Includes no-OOD, unlabeled-OOD, and few-label scenarios .\n",
    "contact": {
      "name": "Deyu Zou",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_a8063075,\n  author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {92499--92528},\n  publisher = {Curran Associates, Inc.},\n  title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference code expected post-conference; current public software availability limited.\nBenchmark infrastructure partially described but not fully released yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Benchmark clearly defines OOD robustness scenarios with classification and regression\ntasks in scientific domains, though no explicit hardware constraints are given.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Curated datasets of 3D crystal structures and material properties are included and\npublicly available for reproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses well-established metrics such as MAE and structural validity for materials modeling,\nplus accuracy and OOD robustness deltas.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected\nto be released soon.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and poster provide solid explanation of benchmarks and scientific motivation;\nmore extensive user documentation forthcoming.\n"
      }
    },
    "id": "gess_-_drugood"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "GeSS - QMOF",
    "url": "https://neurips.cc/virtual/2024/poster/97816",
    "doi": "unknown",
    "domain": [
      "Materials Science"
    ],
    "focus": "Benchmark suite evaluating geometric deep learning models under real-world distribution shifts",
    "keywords": [
      "geometric deep learning",
      "distribution shift",
      "OOD robustness",
      "scientific applications"
    ],
    "summary": "GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access .\n",
    "licensing": "unknown",
    "task_types": [
      "Classification",
      "Regression"
    ],
    "ai_capability_measured": [
      "OOD performance in scientific settings"
    ],
    "metrics": [
      "Accuracy",
      "RMSE",
      "OOD robustness delta"
    ],
    "models": [
      "GCN",
      "EGNN",
      "DimeNet++"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Classification, Regression"
    ],
    "solutions": "0",
    "notes": "Includes no-OOD, unlabeled-OOD, and few-label scenarios .\n",
    "contact": {
      "name": "Deyu Zou",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_a8063075,\n  author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {92499--92528},\n  publisher = {Curran Associates, Inc.},\n  title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Reference code expected post-conference; current public software availability limited.\nBenchmark infrastructure partially described but not fully released yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Benchmark clearly defines OOD robustness scenarios with classification and regression\ntasks in scientific domains, though no explicit hardware constraints are given.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Curated datasets of 3D crystal structures and material properties are included and\npublicly available for reproducible research.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Uses well-established metrics such as MAE and structural validity for materials modeling,\nplus accuracy and OOD robustness deltas.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected\nto be released soon.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "Paper and poster provide solid explanation of benchmarks and scientific motivation;\nmore extensive user documentation forthcoming.\n"
      }
    },
    "id": "gess_-_qmof"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "Vocal Call Locator (VCL)",
    "url": "https://neurips.cc/virtual/2024/poster/97470",
    "doi": "unknown",
    "domain": [
      "Biology & Medicine"
    ],
    "focus": "Benchmarking sound-source localization of rodent vocalizations from multi-channel audio",
    "keywords": [
      "source localization",
      "bioacoustics",
      "time-series",
      "SSL"
    ],
    "summary": "The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms in bioacoustics .\n",
    "licensing": "unknown",
    "task_types": [
      "Sound source localization"
    ],
    "ai_capability_measured": [
      "Source localization accuracy in bioacoustic settings"
    ],
    "metrics": [
      "Localization error (cm)",
      "Recall/Precision"
    ],
    "models": [
      "CNN-based SSL models"
    ],
    "ml_motif": [
      "Regression"
    ],
    "type": "Dataset",
    "ml_task": [
      "Anomaly Detection / localization"
    ],
    "solutions": "0",
    "notes": "Dataset spans real, simulated, and mixed audio; supports benchmarking across data types .\n",
    "contact": {
      "name": "Ralph Peterson",
      "email": "unknown"
    },
    "cite": [
      "@inproceedings{neurips2024_c00d37d6,\n  author = {Peterson, Ralph E and Tanelus, Aramis and Ick, Christopher and Mimica, Bartul and Francis, Niegil and Ivan, Violet J and Choudhri, Aman and Falkner, Annegret L and Murthy, Mala and Schneider, David M and Sanes, Dan H and Williams, Alex H},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {106370--106382},\n  publisher = {Curran Associates, Inc.},\n  title = {Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 3,
        "reason": "Some baseline CNN models for sound source localization are reported,\nbut no publicly available or fully integrated runnable codebase yet.\n"
      },
      "specification": {
        "rating": 5,
        "reason": "Well-defined localization tasks with multiple scenarios and real-world\nenvironment conditions; input/output formats clearly described.\n"
      },
      "dataset": {
        "rating": 4,
        "reason": "Large-scale audio dataset covering real and simulated data with\nstandardized splits, though exact data formats are not fully detailed.\n"
      },
      "metrics": {
        "rating": 5,
        "reason": "Includes localization error, precision, recall, and other relevant metrics\nfor robust evaluation.\n"
      },
      "reference_solution": {
        "rating": 5,
        "reason": "Multiple baselines evaluated over diverse models and architectures,\nsupporting reproducibility of benchmark comparisons.\n"
      },
      "documentation": {
        "rating": 1,
        "reason": "Methodology and paper are thorough, but setup instructions and runnable\ncode are not publicly provided, limiting user onboarding.\n"
      }
    },
    "id": "vocal_call_locator_vcl"
  },
  {
    "date": "2024-12-13",
    "version": "v1.0",
    "last_updated": "2024-12",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-13",
    "name": "SPIQA (LLM)",
    "url": "https://neurips.cc/virtual/2024/poster/97575",
    "doi": "10.48550/arXiv.2407.09413",
    "domain": [
      "Computational Science & AI"
    ],
    "focus": "Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance)",
    "keywords": [
      "multimodal QA",
      "scientific figures",
      "image+text",
      "chain-of-thought prompting"
    ],
    "summary": "A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions. Highlights performance differences between chain-of-thought and end-to-end adapter models.\n",
    "licensing": "unknown",
    "task_types": [
      "Multimodal QA"
    ],
    "ai_capability_measured": [
      "Visual reasoning",
      "scientific figure understanding"
    ],
    "metrics": [
      "Accuracy",
      "F1 score"
    ],
    "models": [
      "LLaVA",
      "MiniGPT-4",
      "Owl-LLM adapter variants"
    ],
    "ml_motif": [
      "Multimodal Reasoning"
    ],
    "type": "Benchmark",
    "ml_task": [
      "Multimodal QA"
    ],
    "solutions": "Solution details are described in the referenced paper or repository.",
    "notes": "Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs.\n",
    "contact": {
      "name": "Xiaoyan Zhong",
      "email": "unknown"
    },
    "cite": [
      "@misc{pramanick2025spiqadatasetmultimodalquestion,\n  title={SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers}, \n  author={Shraman Pramanick and Rama Chellappa and Subhashini Venugopalan},\n  year={2025},\n  eprint={2407.09413},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2407.09413}, \n}\n"
    ],
    "datasets": {
      "links": []
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": ""
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "Well-documented codebase available on Github\n"
      },
      "specification": {
        "rating": 3.5,
        "reason": "Task of QA over scientific figures is sufficient but not fully formalized in input/output terms. No hawrdware constraints.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "Full dataset available on Hugging Face with train/test/valid splits.\n"
      },
      "metrics": {
        "rating": 4,
        "reason": "Reports accuracy and F1; fair but no visual reasoning-specific metric.\n"
      },
      "reference_solution": {
        "rating": 4,
        "reason": "10 LLM adapter baselines; results included without constraints.\n"
      },
      "documentation": {
        "rating": 5,
        "reason": "Full paper available\n"
      }
    },
    "id": "spiqa_llm"
  },
  {
    "date": "2024-12-03",
    "version": "v1.0",
    "last_updated": "2025-06",
    "expired": "unknown",
    "valid": "yes",
    "valid_date": "2024-12-03",
    "name": "The Well",
    "url": "https://polymathic-ai.org/the_well/",
    "doi": "unknown",
    "domain": [
      "Biology & Medicine",
      "Computational Science & AI",
      "High Energy Physics"
    ],
    "focus": "Foundation model + surrogate dataset spanning 16 physical simulation domains",
    "keywords": [
      "surrogate modeling",
      "foundation model",
      "physics simulations",
      "spatiotemporal dynamics"
    ],
    "summary": "A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains-from biology to astrophysical magnetohydrodynamic simulations-with unified API and metadata. Ideal for training surrogate and foundation models on scientific data. \n",
    "licensing": "BSD 3-Clause License",
    "task_types": [
      "Supervised Learning"
    ],
    "ai_capability_measured": [
      "Surrogate modeling",
      "physics-based prediction"
    ],
    "metrics": [
      "Dataset size",
      "Domain breadth"
    ],
    "models": [
      "FNO baselines",
      "U-Net baselines"
    ],
    "ml_motif": [
      "Sequence Prediction/Forecasting"
    ],
    "type": "Dataset",
    "ml_task": [
      "Supervised Learning"
    ],
    "solutions": "1",
    "notes": "Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. \"Benchmarks\" are nominally baseline models that were trained on different parts of the dataset, all of them being time series prediction tasks.\n",
    "contact": {
      "name": "Ruben Ohana",
      "email": "rohana@flatironinstitute.org"
    },
    "cite": [
      "@inproceedings{neurips2024_4f9a5acd,\n  author = {Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina J. and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesley and Dalziel, Stuart B. and Fielding, Drummond B. and Fortunato, Daniel and Goldberg, Jared A. and Hirashima, Keiya and Jiang, Yan-Fei and Kerswell, Rich R. and Maddu, Suryanarayana and Miller, Jonah and Mukhopadhyay, Payel and Nixon, Stefan S. and Shen, Jeff and Watteaux, Romain and Blancard, Bruno R\\'{e}galdo-Saint and Rozet, Fran\\c{c}ois and Parker, Liam H. and Cranmer, Miles and Ho, Shirley},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {44989--45037},\n  publisher = {Curran Associates, Inc.},\n  title = {The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}\n"
    ],
    "datasets": {
      "links": [
        {
          "name": "16 simulation datasets",
          "url": "HDF5) via PyPI/GitHub"
        }
      ]
    },
    "results": {
      "links": [
        {
          "name": "Gemini LLM Deep Research",
          "url": "https://docs.google.com/document/d/1axQvD_aY9O71A2nxWaoFvsfE8HvZ4TmwDoZ4cQQwW58/edit?usp=sharing"
        },
        {
          "name": "ChatGPT LLM",
          "url": ""
        }
      ]
    },
    "fair": {
      "reproducible": "Yes",
      "benchmark_ready": "Yes"
    },
    "ratings": {
      "software": {
        "rating": 5,
        "reason": "BSD-licensed software and unified API are available via GitHub and PyPI.\nSupports loading and manipulating large HDF5 datasets across 16 domains.\n"
      },
      "specification": {
        "rating": 4,
        "reason": "The benchmark includes clearly defined surrogate modeling tasks, data structure, and metadata.\nHowever, constraints and formal task specs vary slightly across domains.\n"
      },
      "dataset": {
        "rating": 5,
        "reason": "15 TB of ML-ready HDF5 datasets across 16 physics domains. Public, well-structured,\nrichly annotated, and designed with FAIR principles in mind.\n"
      },
      "metrics": {
        "rating": 3,
        "reason": "Domain breadth and dataset size are emphasized. Standardized quantitative metrics for\nmodel evaluation (e.g., RMSE, accuracy) are not uniformly applied across all domains.\n"
      },
      "reference_solution": {
        "rating": 3,
        "reason": "Includes FNO and U-Net baselines, but does not yet provide fully trained, reproducible\nmodels or scripts across all datasets.\n"
      },
      "documentation": {
        "rating": 4,
        "reason": "The GitHub repo and NeurIPS paper provide detailed guidance on dataset use,\nstructure, and training setup. Tutorials and walkthroughs could be expanded further.\n"
      }
    },
    "id": "the_well"
  }
]