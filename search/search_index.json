{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLCommons Science Working Group AI Benchmarks Collection","text":"<p>This site curates a collection of AI benchmarks. The main artifact is the PDF report:</p> <ul> <li>Report (PDF): benchmarks.pdf</li> </ul> <p>If you cite this work, please reference the PDF (not the Markdown pages). A BibTeX entry is provided below.</p> <pre><code>@misc{benchmark-collection,\n  title        = {MLCommons Science Working Group AI Benchmarks Collection},\n  author       = {Gregor von Laszewski and\n                  Reece Shiraishi and\n                  Anjay Krishnan and\n                  Nhan Tran and\n                  Benjamin Hawks and\n                  Marco Colombo and\n                  Geoffrey C. Fox},\n  year         = {2025},\n  month        = jul,\n  howpublished = {GitHub},\n  url          = {https://mlcommons-science.github.io/benchmark/benchmarks.pdf}\n}\n</code></pre> <p>For online browsing, we provide two views:</p> <ul> <li>Benchmarks (cards): filterable and sortable index with detail pages.</li> <li>Benchmarks (table): compact Markdown table with links to the same detail pages.</li> </ul> <p>Note: The Markdown pages are generated for web browsing and should not be cited. Please cite the PDF report above.</p> <p>All pages are generated automatically, please don\u2019t edit them directly. To propose changes, update the source files here:</p> <ul> <li>https://github.com/mlcommons-science/benchmark/tree/main/source</li> </ul> <p>For program improvements, please contact Gregor von Laszewski at <code>laszewski at gmail.com</code>.</p> <p>The project README is available at:</p> <ul> <li>https://github.com/mlcommons-science/benchmark</li> </ul>"},{"location":"md/benchmarks/","title":"Benchmarks","text":"Date Name Domain Focus Keywords Task Types Metrics Models Citation Software Rating Software Reason Specification Rating Specification Reason Dataset Rating Dataset Reason Metrics Rating Metrics Reason Reference Solution Rating Reference Solution Reason Documentation Rating Documentation Reason Average Ratings 2020-09-07 MMLU (Massive Multitask Language Understanding) Multidomain Academic knowledge and reasoning across 57 subjects multitask, multiple-choice, zero-shot, few-shot, knowledge probing Multiple choice Accuracy GPT-4o, Gemini 1.5 Pro, o1, DeepSeek-R1 <sup>1</sup> 0 No instructions to download or run data given on the site 4 No system constraints 5 Meets all FAIR principles and properly versioned. 5 Fully defined, represents a solution's performance. 2 Reference models are available (i.e. GPT-3), but are not trainable or publicly documented 5 Well-explained in a provided paper. 3.5 2023-11-20 GPQA Diamond Science Graduate-level scientific reasoning Google-proof, graduate-level, science QA, chemistry, physics Multiple choice, Multi-step QA Accuracy o1, DeepSeek-R1 <sup>2</sup> 5 Python version and requirements specified on Github site 2 No system constraints or I/O specified 5 Easily able to access dataset. Comes with predefined splits as mentioned in the paper 5 Each question has a correct answer, representing the tested model's performance. 1 Common models such as GPT-3.5 were compared. They are not open and don't provide requirements 5 All information is listed in the associated paper 3.833 2018-03-14 ARC-Challenge (Advanced Reasoning Challenge) Science Grade-school science with reasoning emphasis grade-school, science QA, challenge set, reasoning Multiple choice Accuracy GPT-4, Claude <sup>3</sup> 0 No link to code or documentation 2 Task is clear, but no constraints or format is mentioned 4 Data accessible, offers instructions on how to download the data via CLI tools. No splits. 5 (by default) All questions in the dataset are multiple choice, all have a correct answer 1 There are over 300 models listed, but very few, if any, show performance on the dataset or list constraints 5 Explains all necessary information inside a paper 2.833 2025-01-24 Humanity's Last Exam Multidomain Broad cross-domain academic reasoning cross-domain, academic exam, multiple-choice, multidisciplinary Multiple choice Accuracy unkown <sup>4</sup> 4 Code for testing models posted on the github. Unknown how to run a custom model. 2 Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified 2 Data accessible through Hugging Face, but requires giving contact information to access 5 (by default) All questions in the dataset are multiple choice, all have a correct answer 2 Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result 5 Paper available with necessary information 3.333 2024-11-07 FrontierMath Mathematics Challenging advanced mathematical reasoning symbolic reasoning, number theory, algebraic geometry, category theory Problem solving Accuracy unkown <sup>5</sup> 0 No link to code provided 3 Well-specified process for asking questions and receiving answers. No software or hardware constraints 0 Paper and website had no link to any dataset. It may still exist somewhere 5 (by default) All questions in the dataset have a correct answer 2 Displays result of leading models on the benchmark, but none are trainable or list constraints 0 No specified way to reproduce the reference solution 1.667 2024-07-18 SciCode Scientific Programming Scientific code generation and problem solving code synthesis, scientific computing, programming benchmark Coding Solve rate (%) Claude3.5-Sonnet <sup>6</sup> 5 Code to run exists on github repo 4.5 Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints. 0 Paper and website had no link to any dataset. It may still exist somewhere 2 Metrics stated, but method of grading is not specified 1 Models presented with scores, but none are open or list constraints 4 Paper containing all needed info except for evlauation criteria 2.75 2025-03-13 AIME (American Invitational Mathematics Examination) Mathematics Pre-college advanced problem solving algebra, combinatorics, number theory, geometry Problem solving Accuracy unkown <sup>7</sup> 0 No code available 0 Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints 4 Easily accessible data with problems and solutions, but no splits 5 (by default) Answer is correct or it's not 0 Not given. Human performance stats exist, but no mentions of AI performance 0 Not given 1.5 2025-02-15 MATH-500 Mathematics Math reasoning generalization calculus, algebra, number theory, geometry Problem solving Accuracy unkown <sup>8</sup> 0 No code provided 0 No method of presentation and evaluation is not stated. No constraints 5 Problems and solutions are easily downloaded. Could not find a way to download the data 2 Problem spec states that all of the AI reasoning steps are subject to grading, but no specified way to evaluate the steps 0 Not given 0 Not given. Implicit instructions to download dataset. 1.167 2024-04-02 CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction) Multidomain Science Long-context scientific reasoning long-context, information extraction, multimodal Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension Accuracy unkown <sup>9</sup> 4 Code is available, but not well documented 1 Explains types of problems in detail, but does not state exactly how to administer them. 4 Dataset is available via Github, but hard to find 5 Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem. 1 Exists, but is not open 5 Associated paper explains all criteria 3.333 2023-01-26 FEABench (Finite Element Analysis Benchmark) Computational Engineering FEA simulation accuracy and performance finite element, simulation, PDE Simulation, Performance evaluation Solve time, Error norm FEniCS, deal.II <sup>10</sup> 4 Code is available, but poorly documented 1.5 Output is defined and task clarity is questionable 4 Available, but not split into sets 5 Fully defined metrics 4 Three open-source models were used. No system constraints. 5 In associated paper 3.917 2024-07-12 SPIQA (Scientific Paper Image Question Answering) Computer Science Multimodal QA on scientific figures multimodal QA, figure understanding, table comprehension, chain-of-thought Question answering, Multimodal QA, Chain-of-Thought evaluation Accuracy, F1 score Chain-of-Thought models, Multimodal QA systems <sup>11</sup> 0 Not provided 5 Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope. 4.5 Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization. 5 Uses quantitative metrics (Accuracy, F1) aligned with the task 2 Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all. 5 All information provided in paper 3.583 2020-09-28 MedQA Medical Question Answering Medical board exam QA USMLE, diagnostic QA, medical knowledge, multilingual Multiple choice Accuracy Neural reader, Retrieval-based QA systems <sup>12</sup> 5 All code available on the github 3 Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified. 4 Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria. 5 Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models. 0 No reference solution mentioned. 4 Paper is available. Evaluation criteria are not mentioned. 3.5 2025-05-13 BaisBench (Biological AI Scientist Benchmark) Computational Biology Omics-driven AI research tasks single-cell annotation, biological QA, autonomous discovery Cell type annotation, Multiple choice Annotation accuracy, QA accuracy LLM-based AI scientist agents <sup>13</sup> 5 Instructions for environment setup available 4 Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified. 5 Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards. 5 Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals. 0 Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet. 5 Dataset and paper accessible; IPYNB files for setup are available on the github repo. 4.0 2023-01-26 MOLGEN Computational Chemistry Molecular generation and optimization SELFIES, GAN, property optimization Distribution learning, Goal-oriented generation Validity%, Novelty%, QED, Docking score MolGen <sup>14</sup> 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0.0 2020-05-02 Open Graph Benchmark (OGB) - Biology Graph ML Biological graph property prediction node prediction, link prediction, graph classification Node property prediction, Link property prediction, Graph property prediction Accuracy, ROC-AUC GCN, GraphSAGE, GAT <sup>15</sup> 5 All necessary information is provided on the Github 4 Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined. No constraints. 5 Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included. 5 Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks. 3 Multiple baselines implemented and documented (GCN, GAT, GraphSAGE). No contraints. 5 All necessary information is included in a paper. 4.5 2011-10-01 Materials Project Materials Science DFT-based property prediction DFT, materials genome, high-throughput Property prediction MAE, R^2 Automatminer, Crystal Graph Neural Networks <sup>16</sup> 0 No instructions available 1.5 The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases. 3 API key required to access data. No predefined splits. 5 Uses numerical metrics like MAE and R^2 2 Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed. 0 No explanations or paper provided 1.917 2020-10-20 OCP (Open Catalyst Project) Chemistry; Materials Science Catalyst adsorption energy prediction DFT relaxations, adsorption energy, graph neural networks Energy prediction, Force prediction MAE (energy), MAE (force) CGCNN, SchNet, DimeNet++, GemNet-OC <sup>17</sup>, <sup>18</sup>, <sup>19</sup>, <sup>20</sup> 5 Data provided in Github links 5 Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy. 5 Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable. 5 MAE (energy and force) are standard and reproducible. 4 Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed. 1 Paper exists, but content is behind a paywall. 4.167 2023-06-20 JARVIS-Leaderboard Materials Science; Benchmarking Comparative evaluation of materials design methods leaderboards, materials methods, simulation Method benchmarking, Leaderboard ranking MAE, RMSE, Accuracy unkown <sup>21</sup> 1 Setup script provided, but no code provided 1 Only dataset format is defined. 4 Data is public and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks. No splits. 5 Metrics stated for each benchmark. 4 Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); no constraints specified. 1 Only the task is specified. 2.667 2022-02-22 Quantum Computing Benchmarks (QML) Quantum Computing Quantum algorithm performance evaluation quantum circuits, state preparation, error correction Circuit benchmarking, State classification Fidelity, Success probability IBM Q, IonQ, AQT@LBNL <sup>22</sup> 4 Run instructions exist, but are not easy to follow 3 No system constraints. Task clarity and dataset format are not clearly specified. 4 Datasets are accessible, but not split. 3 Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured. 0 Not provided 1 Only the task is defined. 2.5 2024-10-01 CFDBench (Fluid Dynamics) Fluid Dynamics; Scientific ML Neural operator surrogate modeling neural operators, CFD, FNO, DeepONet Surrogate modeling L2 error, MAE FNO, DeepONet, U-Net <sup>23</sup> 5 The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation 0 Not listed 0 Not given 5 Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives. 5 Baseline models like FNO and DeepONet are implemented, hardware specified. 5 Associated paper gives all necessary information. 3.333 2023-04-23 SatImgNet Remote Sensing Satellite imagery classification land-use, zero-shot, multi-task Image classification Accuracy CLIP, BLIP, ALBEF <sup>24</sup> 0 No scripts or environment information provided 4 Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation. 5 Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks. 5 Accuracy of classification is an appropriate metric 4 Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified 5 Paper provides all required information 3.833 2023-07-19 ClimateLearn Climate Science; Forecasting ML for weather and climate modeling medium-range forecasting, ERA5, data-driven Forecasting RMSE, Anomaly correlation CNN baselines, ResNet variants <sup>25</sup> 5 Quickstart notebook makes for easy usage 5 Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints. 5 Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant. 5 ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary. 0 The benchmark is geared for CNN architectures, but no specific model was mentioned. 5 Explained in the benchmark's paper. 4.167 2022-06-09 BIG-Bench (Beyond the Imitation Game Benchmark) NLP; AI Evaluation Diverse reasoning and generalization tasks few-shot, multi-task, bias analysis Few-shot evaluation, Multi-task evaluation Accuracy, Task-specific metrics GPT-3, Dense Transformers, Sparse Transformers <sup>26</sup> 4.5 Quick start notebook provided, but instructions on how to run it are lacking. 4.5 Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized. 5 Public, versioned, and well-documented; FAIR overall 5 Many tasks use standard quantitative metrics (accuracy, BLEU, F1). Others involve subjective ratings (e.g., Likert), which reduces cross-task comparability. 2 Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey. 5 Explained in the associated paper. 4.333 2019-11-20 CommonSenseQA NLP; Commonsense Commonsense question answering ConceptNet, multiple-choice, adversarial Multiple choice Accuracy BERT-large, RoBERTa, GPT-3 <sup>27</sup> 5 All code given on Github site 4 Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified. 5 Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries. 5 Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation. 4 Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not run with hardware constraints 5 Given in paper. 4.667 2019-07-24 Winogrande NLP; Commonsense Winograd Schema-style pronoun resolution adversarial, pronoun resolution Pronoun resolution Accuracy, AUC RoBERTa, BERT, GPT-2 <sup>28</sup> 0 No template code provided 5 Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included. 5 Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata. 5 Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations. 4 Baseline results available, requiring users to submit their methods along with their submissions. Constraints are not required in submissions. 5 Dataset page and paper provide sufficient detail 4.0 2024-05-01 Jet Classification Particle Physics Real-time classification of particle jets using HL-LHC simulation features classification, real-time ML, jet tagging, QKeras Classification Accuracy, AUC Keras DNN, QKeras quantized DNN <sup>29</sup> 3 Not containerized; Setup automation/documentation could be improved 4 System constraints missing 5 None 5 None 4 HW/SW requirements missing; Reference not bundled as official starter kit 4 Full reproducibility requires manual setup 4.167 2024-05-01 Irregular Sensor Data Compression Particle Physics Real-time compression of sparse sensor data with autoencoders compression, autoencoder, sparse data, irregular sampling Compression MSE, Compression ratio Autoencoder, Quantized autoencoder <sup>30</sup> 3 Not containerized; Full automation and documentation could be improved 4 Exact latency or resource constraints not numerically specified 5 All criteria met 5 All criteria met 4 Not fully documented or automated for reproducibility 4 Setup for deployment (e.g., FPGA pipeline) requires familiarity with tooling 4.167 2024-05-01 Beam Control Accelerators and Magnets Reinforcement learning control of accelerator beam position RL, beam stabilization, control systems, simulation Control Stability, Control loss DDPG, PPO (planned) <sup>31</sup>, <sup>32</sup> 1 Code not documented; Incomplete setup and not containerized 4 Latency/resource constraints not fully quantified 3 Not findable (no DOI/indexing); Not interoperable (format/schema unspecified) 5 All criteria met 2 HW/SW requirements missing; Metrics not evaluated with reference; Baseline not trainable/open 3 Setup instructions and pretrained model details are missing 3.0 2024-07-08 Ultrafast jet classification at the HL-LHC Particle Physics FPGA-optimized real-time jet origin classification at the HL-LHC jet classification, FPGA, quantization-aware training, Deep Sets, Interaction Networks Classification Accuracy, Latency, Resource utilization MLP, Deep Sets, Interaction Network <sup>33</sup> 3 Not containerized; Setup and automation incomplete 4 Hardware constraints are referenced but not fully detailed or standardized 4 FAIR metadata limited; no clear mention of dataset format or splits 3 Metrics exist (accuracy, latency, utilization), but formal definitions and evaluation guidance are limited 2 Reference implementations not fully reproducible; no evaluation pipeline or training setup provided 3 No linked GitHub repo or setup instructions; paper provides partial guidance only 3.167 2024-10-15 Quench detection Accelerators and Magnets Real-time detection of superconducting magnet quenches using ML quench detection, autoencoder, anomaly detection, real-time Anomaly detection, Quench localization ROC-AUC, Detection latency Autoencoder, RL agents (in development) <sup>34</sup> 1 Code not provided; no evidence of documentation or containerization 4 Real-time detection task is clearly described, but exact constraints, inputs/outputs, and evaluation protocol are only partially specified 2 Dataset URL is missing; FAIR principles largely unmet 3 ROC-AUC and latency are mentioned, but metric definitions and formal evaluation setup are missing 1 No baseline or reproducible model implementation available 2 Only a conference slide deck is available; lacks detailed instructions or repository for reproduction 2.167 2024-10-15 DUNE Particle Physics Real-time ML for DUNE DAQ time-series data DUNE, time-series, real-time, trigger Trigger selection, Time-series anomaly detection Detection efficiency, Latency CNN, LSTM (planned) <sup>35</sup> 1 Code not available; no containerization or setup provided 4 Constraints like latency thresholds are described qualitatively but not numerically defined 3 Dataset lacks a public URL; FAIR metadata and versioning are missing 4 Metrics are relevant but no benchmark baseline or detailed evaluation guidance is provided 2 Autoencoder prototype exists but is not reproducible; RL model still in development 3 Documentation exists only in slides/GDocs; no implementation guide or structured release 2.833 2025-01-08 Intelligent experiments through real-time AI Instrumentation and Detectors; Nuclear Physics; Particle Physics Real-time FPGA-based triggering and detector control for sPHENIX and future EIC FPGA, Graph Neural Network, hls4ml, real-time inference, detector control Trigger classification, Detector control, Real-time inference Accuracy (charm and beauty detection), Latency (micros), Resource utilization (LUT/FF/BRAM/DSP) Bipartite Graph Network with Set Transformers (BGN-ST), GarNet (edge-classifier) <sup>36</sup> 3 No containerized or open-source setup provided 4 Architectural/system specifications are incomplete 2 Dataset is internal and not publicly available or FAIR-compliant 3 Metrics relevant but not supported by evaluation scripts or baselines 3 No public or reproducible implementation released 3 No public GitHub or complete pipeline documentation 3.0 2025-01-09 Neural Architecture Codesign for Fast Physics Applications Physics; Materials Science; Particle Physics Automated neural architecture search and hardware-efficient model codesign for fast physics applications neural architecture search, FPGA deployment, quantization, pruning, hls4ml Classification, Peak finding Accuracy, Latency, Resource utilization NAC-based BraggNN, NAC-optimized Deep Sets (jet) <sup>37</sup> 3 Toolchain (hls4ml, nac-opt) described but not yet containerized or fully packaged 5 Fully specified task with constraints and target deployment; includes hardware context 2 Simulated datasets referenced but not publicly available or FAIR-compliant 5 Clear, quantitative metrics aligned with task goals and hardware evaluation 4 Models tested on hardware with source code references; full training pipeline not yet released 4 Detailed paper and tools described; open repo planned but not yet complete 3.833 2024-06-24 Smart Pixels for LHC Particle Physics; Instrumentation and Detectors On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors smart pixel, on-sensor inference, data reduction, trigger Image Classification, Data filtering Data rejection rate, Power per pixel 2-layer pixel NN <sup>38</sup> 2 No packaged code or setup scripts available; replication depends on hardware description and paper 5 None 2 No dataset links; not publicly hosted or FAIR-compliant 5 None 3 In-pixel 2-layer NN described and evaluated, but reproducibility and source files are not released 3 Paper contains detailed descriptions, but no repo or external guide for reproducing results 3.333 2023-10-03 HEDM (BraggNN) Material Science Fast Bragg peak analysis using deep learning in diffraction microscopy BraggNN, diffraction, peak finding, HEDM Peak detection Localization accuracy, Inference time BraggNN <sup>39</sup> 2 No standalone code repository or setup instructions provided 5 None 2 No dataset links or FAIR metadata; unclear public access 4 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts 3 BraggNN model is described and evaluated, but no direct implementation or inference scripts available 3 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline 3.167 2023-12-03 4D-STEM Material Science Real-time ML for scanning transmission electron microscopy 4D-STEM, electron microscopy, real-time, image processing Image Classification, Streamed data inference Classification accuracy, Throughput CNN models (prototype) <sup>40</sup> 2 No standalone code repository or setup instructions provided 5 None 2 No dataset links or FAIR metadata; unclear public access 4 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts 3 BraggNN model is described and evaluated, but no direct implementation or inference scripts available 3 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline 3.167 2023-12-05 In-Situ High-Speed Computer Vision Fusion/Plasma Real-time image classification for in-situ plasma diagnostics plasma, in-situ vision, real-time ML Image Classification Accuracy, FPS CNN <sup>41</sup> 1 No public implementation or containerized setup released 3 No standardized I/O, latency constraint, or complete framing 0 Dataset not provided or described in any formal way 2 Throughput and accuracy mentioned, but not defined or benchmarked 1 Prototype CNNs described; no code, baseline, or training details available 2 Some insight via papers, but no working repo, setup, or replication path 1.5 2020-01-01 BenchCouncil AIBench General End-to-end AI benchmarking across micro, component, and application levels benchmarking, AI systems, application-level evaluation Training, Inference, End-to-end AI workloads Throughput, Latency, Accuracy ResNet, BERT, GANs, Recommendation systems <sup>42</sup> 3 No containerized or automated implementation provided for full benchmark suite 4 Task coverage is broad and well-scoped, but system constraints and expected outputs are not uniformly defined 3 Multiple datasets are mentioned, but not consistently FAIR-documented, versioned, or linked 4 Metrics are appropriate, but standardization and reproducibility across tasks vary 3 Reference models (e.g., ResNet, BERT) described; no turnkey implementation or results repository for all levels 3 Paper is comprehensive, but minimal user-facing documentation or structured reproduction guide 3.333 2020-01-01 BenchCouncil BigDataBench General Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads big data, AI benchmarking, data analytics Data preprocessing, Inference, End-to-end data pipelines Data throughput, Latency, Accuracy CNN, LSTM, SVM, XGBoost <sup>43</sup> 3 No automated setup across all tasks; some components require manual integration. 4 Specific I/O formats and hardware constraints are not uniformly detailed across all tasks. 4 Some datasets lack consistent versioning or rich metadata annotations. 5 None 4 Not all benchmark components have fully reproducible baselines; deployment across platforms is fragmented. 4 Setup requires manual steps; some task-specific instructions lack clarity. 4.0 2021-10-20 MLPerf HPC Cosmology, Climate, Protein Structure, Catalysis Scientific ML training and inference on HPC systems HPC, training, inference, scientific ML Training, Inference Training time, Accuracy, GPU utilization CosmoFlow, DeepCAM, OpenCatalyst <sup>44</sup> 3 Reference implementations exist but containerization and environment setup require manual effort across HPC systems. 4 Hardware constraints and I/O formats are not fully defined for all scenarios. 5 Not all data is independently versioned or comes with standardized FAIR metadata. 5 None 4 Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled. 4 Central guidance is available but requires domain-specific effort to replicate results across systems. 4.167 2023-06-01 MLCommons Science Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD AI benchmarks for scientific applications including time-series, imaging, and simulation science AI, benchmark, MLCommons, HPC Time-series analysis, Image classification, Simulation surrogate modeling MAE, Accuracy, Speedup vs simulation CNN, GNN, Transformer <sup>45</sup> 5 Actively maintained GitHub repository available at https://github.com/mlcommons/science with implementations, scripts, and reproducibility support. 5 All five specification aspects are covered: system constraints, task, dataset format, benchmark inputs, and outputs. 5 Public scientific datasets are used with defined splits. At least 4 FAIR principles are followed. 5 Clearly defined metrics such as accuracy, training time, and GPU utilization are used. These metrics are explained and effectively capture solution performance. 5 A reference implementation is available, well-documented, trainable/open, and includes full metric evaluation and software/hardware details. 5 Thorough documentation exists covering the task, background, motivation, evaluation criteria, and includes a supporting paper. 5.0 2021-07-05 LHC New Physics Dataset Particle Physics; Real-time Triggering Real-time LHC event filtering for anomaly detection using proton collision data anomaly detection, proton collision, real-time inference, event filtering, unsupervised ML Anomaly detection, Event classification ROC-AUC, Detection efficiency Autoencoder, Variational autoencoder, Isolation forest <sup>46</sup> 3 While not formally evaluated in the previous version, Zenodo and paper links suggest available code for baseline models (e.g., autoencoders, GANs), though they are scattered and not unified in a single repository. 3 The task and context are clearly described, but system constraints and formal inputs/outputs are not fully specified. 5 Large-scale dataset hosted on Zenodo, publicly available, well-documented, with defined train/test structure. Appears to follow at least 4 FAIR principles. 4 Uses reasonable metrics (ROC-AUC, detection efficiency) that capture performance but lacks full explanation and standard evaluation tools. 2 Baselines are described across multiple papers but lack centralized, reproducible implementations and hardware/software setup details. 3 Some description in papers and dataset metadata exists, but lacks a unified guide, README, or training setup in a central location. 3.333 2023-07-17 MLCommons Medical AI Healthcare; Medical AI Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data medical AI, federated evaluation, privacy-preserving, fairness, healthcare benchmarks Federated evaluation, Model validation ROC AUC, Accuracy, Fairness metrics MedPerf-validated CNNs, GaNDLF workflows <sup>47</sup> 5 GitHub repository (https://github.com/mlcommons/medical) provides actively maintained open-source tools like MedPerf and GaNDLF for federated medical AI evaluation. 4 The platform defines federated tasks and model evaluation scenarios. Some clinical and system-level constraints are implied but not uniformly formalized across all use cases. 4 Multi-institutional datasets used in federated settings; real-world data is handled privately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit. 5 Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly support goals like generalizability and equity. 3 GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models are centrally documented or easily reproducible. 5 Extensive documentation, papers, and community support exist. Clear examples and usage instructions are provided in GitHub and publications. 4.333 2024-10-28 CaloChallenge 2022 LHC Calorimeter; Particle Physics Fast generative-model-based calorimeter shower simulation evaluation calorimeter simulation, generative models, surrogate modeling, LHC, fast simulation Surrogate modeling Histogram similarity, Classifier AUC, Generation latency VAE variants, GAN variants, Normalizing flows, Diffusion models <sup>48</sup> 4 Community GitHub repos and model implementations are available for the 31 submissions. While not fully unified in one place, the software is accessible and reproducible. 5 The task\u2014evaluating fast generative calorimeter simulations\u2014is clearly defined with benchmarking protocols, constraints like latency and model complexity, and structured evaluation criteria. 5 Four well-structured calorimeter datasets are provided, with different voxel resolutions, open access, signal/background separation, and metadata. FAIR principles are well covered. 5 Metrics like histogram similarity, classifier AUC, and generation latency are well defined and relevant for simulation quality, fidelity, and performance. 4 Several baselines (GANs, VAEs, flows, diffusion models) are documented and evaluated. Some are available via community repos, though not all are fully standardized or bundled. 4 Accompanied by a detailed paper and dataset description. Reproduction of pipelines may require additional setup or familiarity with the model submissions. 4.5 ongoing Papers With Code (SOTA Platform) General ML; All domains Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers leaderboard, benchmarking, reproducibility, open-source Multiple (Classification, Detection, NLP, etc.) Task-specific (Accuracy, F1, BLEU, etc.) All published models with code <sup>49</sup> 5 Actively maintained open-source platform (https://paperswithcode.com) under Apache 2.0 license; includes automatic integration with GitHub, datasets, and models for reproducibility. 4 Task and benchmark structures are well organized and standardized, but due to its broad coverage, input/output formats vary significantly between tasks and are not always tightly controlled. 3 Relies on external datasets submitted by the community. While links are available, FAIR compliance is not guaranteed or systematically enforced across all benchmarks. 5 Tracks state-of-the-art using task-specific metrics like Accuracy, F1, BLEU, etc., with consistent aggregation and historical SOTA tracking. 3 Provides links to implementations of many SOTA models, but no single unified reference baseline is required or maintained per benchmark. 4 Strong front-end documentation and metadata on benchmarks, tasks, and models; however, some benchmark-specific instructions are sparse or dependent on external paper links. 4.0 2022-01-01 Codabench General ML; Multiple Open-source platform for organizing reproducible AI benchmarks and competitions benchmark platform, code submission, competitions, meta-benchmark Multiple Submission count, Leaderboard ranking, Task-specific metrics Arbitrary code submissions <sup>50</sup> 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1.0 2021-09-27 Sabath (SBI-FAIR) Systems; Metadata FAIR metadata framework for ML-driven surrogate workflows in HPC systems meta-benchmark, metadata, HPC, surrogate modeling Systems benchmarking Metadata completeness, FAIR compliance NA <sup>51</sup> 4 Actively maintained GitHub repository (https://github.com/icl-utk-edu/slip/tree/sabath) with BSD-licensed tooling for FAIR metadata capture; integrates with existing surrogate modeling benchmarks. 4 FAIR metadata structure and logging goals are clearly described. Input/output definitions are implied through integrations (e.g., MiniWeatherML), though not always formalized. 4 Datasets used in surrogate benchmarks are publicly available, well-structured, and FAIR-aligned, but not independently hosted by Sabath itself. 4 Emphasizes metadata completeness and FAIR compliance. Metrics are clear and well-matched to its metadata-focused benchmarking context. 3 Includes integration with multiple surrogate benchmarks and models, though not all are fully documented or packaged as standardized reference solutions. 3 Basic instructions and code are provided on GitHub, but more detailed walkthroughs, use-case examples, or tutorials are limited. 3.667 2022-10-13 PDEBench CFD; Weather Modeling Benchmark suite for ML-based surrogates solving time-dependent PDEs PDEs, CFD, scientific ML, surrogate modeling, NeurIPS Supervised Learning RMSE, boundary RMSE, Fourier RMSE FNO, U-Net, PINN, Gradient-Based inverse methods <sup>52</sup> 5 GitHub repository (https://github.com/pdebench/PDEBench) is actively maintained and includes training pipelines, data loaders, and evaluation scripts. Installation and usage are well-documented. 5 Clearly defined tasks for forward and inverse PDE problems, with structured input/output formats, system constraints, and task specifications. 5 Diverse PDE datasets (synthetic and real-world) hosted on DaRUS with DOIs. Datasets are well-documented, structured, and follow FAIR practices. 4 Includes RMSE, boundary RMSE, and Fourier-domain RMSE. These are well-suited to PDE problems, though rationale behind metric choices could be expanded in some cases. 4 Baselines (FNO, U-Net, PINN, etc.) are available and documented, but not every model includes full training and evaluation reproducibility out-of-the-box. 4 Strong documentation on GitHub including examples, configs, and usage instructions. Some model-specific details and tutorials could be further expanded. 4.5 2024-12-03 The Well biological systems, fluid dynamics, acoustic scattering, astrophysical MHD Foundation model + surrogate dataset spanning 16 physical simulation domains surrogate modeling, foundation model, physics simulations, spatiotemporal dynamics Supervised Learning Dataset size, Domain breadth FNO baselines, U-Net baselines <sup>53</sup> 5 BSD-licensed software and unified API are available via GitHub and PyPI. Supports loading and manipulating large HDF5 datasets across 16 domains. 4 The benchmark includes clearly defined surrogate modeling tasks, data structure, and metadata. However, constraints and formal task specs vary slightly across domains. 5 15 TB of ML-ready HDF5 datasets across 16 physics domains. Public, well-structured, richly annotated, and designed with FAIR principles in mind. 3 Domain breadth and dataset size are emphasized. Standardized quantitative metrics for model evaluation (e.g., RMSE, accuracy) are not uniformly applied across all domains. 3 Includes FNO and U-Net baselines, but does not yet provide fully trained, reproducible models or scripts across all datasets. 4 The GitHub repo and NeurIPS paper provide detailed guidance on dataset use, structure, and training setup. Tutorials and walkthroughs could be expanded further. 4.0 2024-10-31 LLM-Inference-Bench LLM; HPC/inference Hardware performance benchmarking of LLMs on AI accelerators LLM, inference benchmarking, GPU, accelerator, throughput Inference Benchmarking Token throughput (tok/s), Latency, Framework-hardware mix performance LLaMA-2-7B, LLaMA-2-70B, Mistral-7B, Qwen-7B <sup>54</sup> 5 Public GitHub repository (https://github.com/argonne-lcf/LLM-Inference-Bench) under BSD-3 license. Includes scripts, configurations, and dashboards for running and visualizing LLM inference benchmarks across multiple accelerator platforms. 5 Benchmark scope, models, accelerator targets, and supported frameworks are clearly specified. Input configurations and output metrics are standardized across hardware types. 2 No novel dataset is introduced; benchmark relies on pre-trained LLMs and synthetic inference inputs. Dataset structure and FAIR considerations are minimal. 5 Hardware-specific metrics (token throughput, latency, utilization) are well-defined, consistently measured, and aggregated in dashboards. 3 Inference configurations and baseline performance results are provided, but there are no full reference training pipelines or model implementations. 4 GitHub repo provides clear usage instructions, setup guides, and interactive dashboard tooling. Some areas like benchmarking extensions or advanced tuning are less detailed. 4.0 2023-12-12 SGLang Framework LLM Vision Fast serving framework for LLMs and vision-language models LLM serving, vision-language, RadixAttention, performance, JSON decoding Model serving framework Tokens/sec, Time-to-first-token, Throughput gain vs baseline LLaVA, DeepSeek, Llama <sup>55</sup> 5 Actively maintained and production-deployed (e.g., xAI, NVIDIA); source code available under Apache 2.0. Includes efficient backends (RadixAttention, quantization, batching) and full serving infrastructure. 4 The framework clearly defines performance targets, serving logic, and model integration. Input/output expectations are consistent, but not all benchmarks are standardized. 2 Does not introduce new datasets; instead, it evaluates performance using existing model benchmarks. Only configuration files are included. 5 Serving-related metrics such as tokens/sec, time-to-first-token, and throughput gain vs. baselines are well-defined and consistently applied. 3 Provides benchmark configs and example integrations (e.g., with LLaVA, DeepSeek), but not all models or scripts are runnable out-of-the-box. 4 Strong GitHub documentation, install guides, and benchmarks. Some advanced topics (e.g., scaling, hardware tuning) could use deeper walkthroughs. 3.833 2023-09-12 vLLM Inference and Serving Engine LLM; HPC/inference High-throughput, memory-efficient inference and serving engine for LLMs LLM inference, PagedAttention, CUDA graph, streaming API, quantization Inference Benchmarking Tokens/sec, Time to First Token (TTFT), Memory footprint LLaMA, Mixtral, FlashAttention-based models <sup>56</sup> 5 Actively maintained open-source project under Apache 2.0. GitHub repo includes full serving engine, benchmarking scripts, CUDA integration, and deployment examples. 5 Inference benchmarks are well-defined with clear input/output formats and platform-specific constraints. Covers multiple models, hardware backends, and batching configurations. 3 No traditional dataset is included. Instead, it uses structured configs and logs suitable for inference benchmarking. FAIR principles are only partially applicable. 5 Comprehensive performance metrics like tokens/sec, time-to-first-token (TTFT), and memory footprint are consistently applied and benchmarked across frameworks. 4 Provides runnable scripts and configs for several models (LLaMA, Mixtral, etc.) across platforms. Baselines are reproducible, though not all models are fully wrapped or hosted. 4 Well-structured GitHub documentation with setup instructions, config examples, benchmarking comparisons, and performance tuning guides. 4.333 2022-06-22 vLLM Performance Dashboard LLM; HPC/inference Interactive dashboard showing inference performance of vLLM Dashboard, Throughput visualization, Latency analysis, Metric tracking Performance visualization Tokens/sec, TTFT, Memory usage LLaMA-2, Mistral, Qwen <sup>57</sup> 4 Interactive dashboard built with ObservableHQ and linked to vLLM benchmarks. Source code is not fully open, but backend integration with vLLM is well-maintained. 4 While primarily a visualization tool, it includes benchmark configurations, metric definitions, and supports comparison across models and hardware. 2 No datasets are bundled; the dashboard visualizes metrics derived from model inference logs or external endpoints, not a formal dataset. 4 Tracks tokens/sec, TTFT, memory usage, and platform comparisons. Metrics are clear but focused on visualization rather than statistical robustness. 3 Dashboards include reproducible views of benchmarked models, but do not ship with runnable model code. Relies on external serving infrastructure. 4 Public dashboard with instructions and tooltips; documentation is clear, though access is restricted (login required) and backend setup is opaque to users. 3.5 2022-04-01 Nixtla NeuralForecast Time-series forecasting; General ML High-performance neural forecasting library with &gt;30 models time-series, neural forecasting, NBEATS, NHITS, TFT, probabilistic forecasting, usability Time-series forecasting RMSE, MAPE, CRPS NBEATS, NHITS, TFT, DeepAR <sup>58</sup> 5 Actively maintained open-source library under Apache 2.0. Offers a clean API, extensive model zoo (&gt;30 models), integration with Ray, Optuna, and supports scalable training and inference workflows. 5 Forecasting task is well-defined with clear input/output structures. Framework supports probabilistic and deterministic forecasting, with unified interfaces and support for batch evaluation. 3 NeuralForecast does not include its own datasets but supports standard datasets (e.g., M4, M5, ETT). FAIR compliance depends on user-supplied data. 5 RMSE, MAPE, CRPS, and other domain-relevant metrics are well supported and integrated into the evaluation loop. 4 Includes runnable model baselines and training scripts for all supported models. Some models have pretrained weights, but not all are fully benchmarked out-of-the-box. 5 Rich documentation with examples, API references, tutorials, notebooks, and CLI support. PyPI, GitHub, and official blog posts offer clear guidance for usage and extension. 4.5 2023-06-01 Nixtla Neural Forecast NHITS Time-series; General ML Official NHITS implementation for long-horizon time series forecasting NHITS, long-horizon forecasting, neural interpolation, time-series Time-series forecasting RMSE, MAPE NHITS <sup>59</sup> 5 Implemented within the open-source NeuralForecast library under Apache 2.0. Includes training, evaluation, and hyperparameter tuning pipelines. Actively maintained. 5 The NHITS forecasting task is clearly defined with structured input/output formats. Model design targets long-horizon accuracy and compute efficiency. 3 Uses standard benchmark datasets like M4, but does not bundle them directly. FAIR compliance depends on external dataset sources and user setup. 5 Evaluated using RMSE, MAPE, and other standard forecasting metrics, integrated into training and evaluation APIs. 4 Official NHITS implementation is fully reproducible with training/eval configs, though pretrained weights are not always provided. 4 Well-documented on GitHub and in AAAI paper, with code examples, training guidance, and usage tutorials. More model-specific docs could improve clarity further. 4.333 2023-10-03 Nixtla Neural Forecast TimeLLM Time-series; General ML Reprogramming LLMs for time series forecasting Time-LLM, language model, time-series, reprogramming Time-series forecasting RMSE, MAPE Time-LLM <sup>60</sup> 4 Fully open-source under Apache 2.0, integrated into the NeuralForecast library. Includes Time-LLM implementation with example usage and training scripts. 3 High-level framing of forecasting as language modeling is clear, but detailed input/output specifications, constraints, and task formalization are minimal. 3 Evaluated on standard datasets like M4 and ETT, but dataset splits and versioning are not bundled or explicitly FAIR-compliant. 4 Standard forecasting metrics such as RMSE, MAPE, and SMAPE are reported. Evaluation is consistent, though deeper metric justification is limited. 3 Time-LLM implementation is open and reproducible, but limited baselines or comparative implementations are included directly. 3 GitHub README provides installation and quick usage examples, but lacks detailed API docs, training walkthroughs, or extended tutorials. 3.333 2023-10-05 Nixtla Neural Forecast TimeGPT Time-series; General ML Time-series foundation model \"TimeGPT\" for forecasting and anomaly detection TimeGPT, foundation model, time-series, generative model Time-series forecasting, Anomaly detection RMSE, Anomaly detection metrics TimeGPT <sup>61</sup> 4 Fully open-source Apache 2.0 implementation integrated in NeuralForecast, supporting training and evaluation via API. Production-grade deployment available via Nixtla API and Azure. 3 Concept and forecasting goals are described, but formal input/output definitions and task constraints are not rigorously specified. 3 Evaluated on existing open datasets, but consolidated data release, splits, and FAIR metadata are not provided. 4 Uses standard forecasting metrics such as RMSE, MASE, SMAPE, and anomaly detection metrics consistently across evaluations. 3 TimeGPT implementation is available, but baseline comparisons and additional reference models are limited. 3 Basic README with installation and usage examples; more detailed API docs and tutorials would improve usability. 3.333 2025-03-03 HDR ML Anomaly Challenge (Gravitational Waves) Astrophysics; Time-series Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets anomaly detection, gravitational waves, astrophysics, time-series Anomaly detection ROC-AUC, Precision/Recall Deep latent CNNs, Autoencoders <sup>62</sup> 4 Benchmark platform provided on Codabench with starter kits and submission infrastructure. Code and baseline models are publicly accessible but not extensively maintained beyond the challenge. 4 Well-defined anomaly detection task on gravitational-wave time series with clear input/output expectations and challenge constraints. 5 Uses preprocessed LIGO/Virgo time series data at 4096 Hz, publicly available and standard in astrophysics. 4 ROC-AUC, precision, and recall metrics are clearly specified and appropriate for anomaly detection. 4 Baseline deep latent CNNs and autoencoders are provided and reproducible, but not extensively documented. 4 Documentation includes challenge instructions, starter kit details, and baseline descriptions, but could benefit from more thorough tutorials and code walkthroughs. 4.167 2025-03-03 HDR ML Anomaly Challenge (Butterfly) Genomics; Image/CV Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset anomaly detection, computer vision, genomics, butterfly hybrids Anomaly detection Classification accuracy, F1 score CNN-based detectors <sup>63</sup> 3 Codabench platform provides submission infrastructure but no fully maintained code repository or reproducible baseline implementations. 4 Task is clearly described with domain-specific anomaly detection objectives and relevant physics motivation. 3 Dataset consists of real detector data with synthetic anomaly injections; access is restricted and requires NDA, limiting openness and FAIR compliance. 3 Standard metrics (ROC, F1, precision) are used; evaluation protocols are clear but not deeply elaborated. 2 Baselines are partially described but lack public code or reproducible execution scripts. 3 Challenge website provides basic descriptions and evaluation metrics but lacks comprehensive tutorials or example workflows. 3.0 2025-03-03 HDR ML Anomaly Challenge (Sea Level Rise) Climate Science; Time-series, Image/CV Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery anomaly detection, climate science, sea-level rise, time-series, remote sensing Anomaly detection ROC-AUC, Precision/Recall CNNs, RNNs, Transformers <sup>64</sup> 2 Benchmark platform exists on Codabench, but no baseline code or maintained repository for reference solutions provided yet. 5 Well-defined anomaly detection task combining satellite imagery and time-series data, with clear physical and domain-specific framing. 5 Uses preprocessed, public, and well-structured sensor and satellite data for the North Atlantic sea-level rise region. 5 Standard metrics such as ROC-AUC, precision, and recall are specified and suitable for the anomaly detection tasks. 1 No starter models or baseline implementations linked or provided publicly. 5 Challenge page, starter kits, and related papers offer strong guidance for participants. 3.833 2025-01-24 Single Qubit Readout on QICK System Quantum Computing Real-time single-qubit state classification using FPGA firmware qubit readout, hls4ml, FPGA, QICK Classification Accuracy, Latency hls4ml quantized NN <sup>65</sup> 3 Code and FPGA firmware available on GitHub; integration with hls4ml demonstrated. Some deployment details and examples are provided but overall software maturity is moderate. 4 Task clearly defined: real-time single-qubit state classification with latency and fidelity constraints. Labeling and ground truth definitions could be more explicit. 4 Dataset hosted on Zenodo with structured data; however, detailed documentation on image acquisition and labeling pipeline is limited. 5 Standard classification metrics (accuracy, latency) are used and directly relevant to the quantum readout task. 1 No baseline or starter models with runnable code are linked publicly. 4 Codabench task page and GitHub repo provide descriptions and usage instructions, but detailed API or deployment tutorials are limited. 3.5 2023-11-20 GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark Science (Biology, Physics, Chemistry) Graduate-level, expert-validated multiple-choice questions hard even with web access Google-proof, multiple-choice, expert reasoning, science QA Multiple choice Accuracy GPT-4 baseline <sup>66</sup> 3 Dataset and benchmark materials are publicly available via HuggingFace and GitHub, but no integrated runnable code or software framework is provided. 5 Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning. Input/output formats and evaluation criteria are well described. 5 The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits. 5 Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA. 1 No baseline implementations or starter code are linked or provided for reproduction. 3 Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines. 3.667 2024-12-13 SeafloorAI Marine Science; Vision-Language Large-scale vision-language dataset for seafloor mapping and geological classification sonar imagery, vision-language, seafloor mapping, segmentation, QA Image segmentation, Vision-language QA Segmentation pixel accuracy, QA accuracy SegFormer, ViLT-style multimodal models <sup>67</sup> 3 Data processing code is publicly available, but no full benchmark framework or runnable model implementations are provided yet. 5 Tasks (image segmentation and vision-language QA) are clearly defined with geospatial and multimodal objectives well specified. 5 Large-scale, well-annotated sonar imagery dataset with segmentation masks and natural language descriptions; curated with domain experts. 5 Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified and appropriate for the tasks. 4 Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but reproducible code or pretrained weights are not fully available yet. 4 Dataset description and data processing instructions are provided, but tutorials and benchmark usage guides are limited. 4.333 2024-12-13 SuperCon3D Materials Science; Superconductivity Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures superconductivity, crystal structures, equivariant GNN, generative models Regression (Tc prediction), Generative modeling MAE (Tc), Validity of generated structures SODNet, DiffCSP-SC <sup>68</sup> 3 Baseline models (SODNet, DiffCSP-SC) are described in the paper; however, fully reproducible code and pretrained models are not publicly available yet. 5 Tasks for regression (Tc prediction) and generative modeling with clear input/output structures and domain constraints are well defined. 5 Dataset contains 3D crystal structures and associated properties; well-curated but not fully released publicly at this time. 4 Metrics such as MAE for Tc prediction and validity checks for generated structures are appropriate and clearly described. 4 Paper provides model architecture details and some training insights, but no complete open-source reference implementations yet. 4 Paper and GitHub provide good metadata and data processing descriptions; tutorials and user guides could be expanded. 4.167 2024-12-13 GeSS Scientific ML; Geometric Deep Learning Benchmark suite evaluating geometric deep learning models under real-world distribution shifts geometric deep learning, distribution shift, OOD robustness, scientific applications Classification, Regression Accuracy, RMSE, OOD robustness delta GCN, EGNN, DimeNet++ <sup>69</sup> 3 Reference code expected post-conference; current public software availability limited. Benchmark infrastructure partially described but not fully released yet. 5 Benchmark clearly defines OOD robustness scenarios with classification and regression tasks in scientific domains, though no explicit hardware constraints are given. 5 Curated datasets of 3D crystal structures and material properties are included and publicly available for reproducible research. 5 Uses well-established metrics such as MAE and structural validity for materials modeling, plus accuracy and OOD robustness deltas. 4 Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected to be released soon. 4 Paper and poster provide solid explanation of benchmarks and scientific motivation; more extensive user documentation forthcoming. 4.333 2024-12-13 Vocal Call Locator (VCL) Neuroscience; Bioacoustics Benchmarking sound-source localization of rodent vocalizations from multi-channel audio source localization, bioacoustics, time-series, SSL Sound source localization Localization error (cm), Recall/Precision CNN-based SSL models <sup>70</sup> 3 Some baseline CNN models for sound source localization are reported, but no publicly available or fully integrated runnable codebase yet. 5 Well-defined localization tasks with multiple scenarios and real-world environment conditions; input/output formats clearly described. 4 Large-scale audio dataset covering real and simulated data with standardized splits, though exact data formats are not fully detailed. 5 Includes localization error, precision, recall, and other relevant metrics for robust evaluation. 5 Multiple baselines evaluated over diverse models and architectures, supporting reproducibility of benchmark comparisons. 1 Methodology and paper are thorough, but setup instructions and runnable code are not publicly provided, limiting user onboarding. 3.833 2024-12-13 MassSpecGym Cheminformatics; Molecular Discovery Benchmark suite for discovery and identification of molecules via MS/MS mass spectrometry, molecular structure, de novo generation, retrieval, dataset De novo generation, Retrieval, Simulation Structure accuracy, Retrieval precision, Simulation MSE Graph-based generative models, Retrieval baselines <sup>71</sup> 3 Open-source GitHub repository available; baseline models and training code partially provided but overall framework maturity is moderate. 5 Clearly defined tasks including molecule generation, retrieval, and spectrum simulation, scoped for MS/MS molecular identification. 5 Largest public MS/MS dataset with extensive annotations; minor point deducted for lack of explicit train/validation/test splits. 5 Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE used consistently. 3.5 CNN-based baselines are referenced, but pretrained weights and comprehensive training pipelines are not fully documented. 1 Paper and poster describe benchmark goals and design, but documentation and user guides are minimal and repo status uncertain. 3.75 2024-12-13 Urban Data Layer (UDL) Urban Computing; Data Engineering Unified data pipeline for multi-modal urban science research data pipeline, urban science, multi-modal, benchmark Prediction, Classification Task-specific accuracy or RMSE Baseline regression/classification pipelines <sup>72</sup> 3 Source code is publicly available on GitHub; baseline regression and classification pipelines are included but framework maturity is moderate. 5 Multiple urban science tasks like prediction and classification are well specified with clear input/output and evaluation criteria. 5 Large, multi-modal urban datasets are open-source, well-documented, and support reproducible research. 5 Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification. 4 Baseline models available but not exhaustive; community adoption and extensions expected. 5 GitHub repository and conference poster provide comprehensive code and reproducibility instructions. 4.5 2024-12-13 Delta Squared-DFT Computational Chemistry; Materials Science Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies density functional theory, Delta Squared-ML correction, reaction energetics, quantum chemistry Regression Mean Absolute Error (eV), Energy ranking accuracy Delta Squared-ML correction networks, Kernel ridge regression <sup>73</sup> 3 Source code and baseline models available for ML correction to DFT; framework maturity is moderate. 4 Benchmark focuses on reaction energy prediction with clear goals, though some task specifics could be formalized further. 4.5 Multi-modal quantum chemistry datasets are standardized and accessible; repository available. 4 Uses standard regression metrics like MAE and energy ranking accuracy; appropriate for task. 3.5 Includes baseline regression and kernel ridge models; implementations are reproducible. 4 Source code supports pipeline reuse, but formal evaluation splits may vary. 3.833 2024-12-13 LLMs for Crop Science Agricultural Science; NLP Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts crop science, prompt engineering, domain adaptation, question answering Question Answering, Inference Accuracy, F1 score GPT-4, LLaMA-2-13B, T5-XXL <sup>74</sup> 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0.0 2024-12-13 SPIQA (LLM) Multimodal Scientific QA; Computer Vision Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance) multimodal QA, scientific figures, image+text, chain-of-thought prompting Multimodal QA Accuracy, F1 score LLaVA, MiniGPT-4, Owl-LLM adapter variants <sup>75</sup> 5 Well-documented codebase available on Github 3.5 Task of QA over scientific figures is sufficient but not fully formalized in input/output terms. No hawrdware constraints. 5 Full dataset available on Hugging Face with train/test/valid splits. 4 Reports accuracy and F1; fair but no visual reasoning-specific metric. 4 10 LLM adapter baselines; results included without constraints. 5 Full paper available 4.417 <ol> <li> <p>Dan Hendrycks, Collin Burns, and Saurav Kadavath. Measuring massive multitask language understanding. 2021. URL: https://arxiv.org/abs/2009.03300.\u00a0\u21a9</p> </li> <li> <p>David Rein, Betty Li Hou, and Asa Cooper Stickland. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022.\u00a0\u21a9</p> </li> <li> <p>Peter Clark, Isaac Cowhey, and Oren Etzioni. Think you have solved question answering? try arc, the ai2 reasoning challenge. In EMNLP 2018, 237 248. 2018. URL: https://allenai.org/data/arc.\u00a0\u21a9</p> </li> <li> <p>Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, S\u00f8ren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, G\u00f6zdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Br\u00fcssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Mart\u00ed Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, V\u00e1clav Rozho\u0148, Vincent Ginis, Christian Stump, Niv Cohen, Rafa\u0142 Po\u015bwiata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givr\u00e9, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar \u00c4ngquist, Yanxu Chen, Harrison K Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, J\u00e9r\u00e9my Andr\u00e9oletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Kh\u00e1nh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Bir\u00f3 B\u00e1lint, Eve J. Y. Lo, Jiaqi Wang, Maria In\u00eas S. Nunes, Jeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciob\u00e2c\u0103, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekstr\u00f6m, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Pe\u00f1aflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yi\u011fit Yalin, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Bosc\u00e1, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle H\u00e4ggstr\u00f6m, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hern\u00e1ndez-C\u00e1mara, Emanuele Rodol\u00e0, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro Jos\u00e9 Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Ra\u00fal Adri\u00e1n Huerta Rodr\u00edguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benj\u00e1min Borb\u00e1s, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran \u0110uc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub \u0141ucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels M\u00fcndler, S\u00f6ren M\u00f6ller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, M\u00e1ty\u00e1s Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P V, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubi\u0107, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vil\u00e9m Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Micka\u00ebl Noy\u00e9, Micha\u0142 Pere\u0142kiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, D\u00e1niel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Gin\u00e9s, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han L\u00f9, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Bria\u0144ski, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanovi\u0107, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Ga\u00ebl Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, K\u00e1roly Zsolnai-Feh\u00e9r, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gon\u00e7alves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Y\u00fccel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity's last exam. 2025. URL: https://arxiv.org/abs/2501.14249, arXiv:2501.14249.\u00a0\u21a9</p> </li> <li> <p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli J\u00e4rviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: a benchmark for evaluating advanced mathematical reasoning in ai. 2024. URL: https://arxiv.org/abs/2411.04872, arXiv:2411.04872.\u00a0\u21a9</p> </li> <li> <p>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: a research coding benchmark curated by scientists. 2024. URL: https://arxiv.org/abs/2407.13168, arXiv:2407.13168.\u00a0\u21a9</p> </li> <li> <p>TBD. Aime. March 2025. [Online accessed 2025-06-24]. URL: https://www.vals.ai/benchmarks/aime-2025-03-13.\u00a0\u21a9</p> </li> <li> <p>HuggingFaceH4. Math-500. 2025. URL: https://huggingface.co/datasets/HuggingFaceH4/MATH-500.\u00a0\u21a9</p> </li> <li> <p>Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, and Subhashini Venugopalan. Curie: evaluating llms on multitask scientific long context understanding and reasoning. 2025. URL: https://arxiv.org/abs/2503.13517, arXiv:2503.13517.\u00a0\u21a9</p> </li> <li> <p>Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, and Peter Norgaard. Feabench: evaluating language models on multiphysics reasoning ability. 2025. URL: https://arxiv.org/abs/2504.06260, arXiv:2504.06260.\u00a0\u21a9</p> </li> <li> <p>Xiaoyan Zhong, Yijian Gao, and Suchin Gururangan. Spiqa: scientific paper image question answering. 2024. URL: https://arxiv.org/abs/2407.09413.\u00a0\u21a9</p> </li> <li> <p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. 2020. URL: https://arxiv.org/abs/2009.13081, arXiv:2009.13081.\u00a0\u21a9</p> </li> <li> <p>Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu, Lei Wei, and Xuegong Zhang. Benchmarking ai scientists in omics data-driven biological research. 2025. URL: https://arxiv.org/abs/2505.08341, arXiv:2505.08341.\u00a0\u21a9</p> </li> <li> <p>Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen. Domain-agnostic molecular generation with chemical feedback. 2024. URL: https://arxiv.org/abs/2301.11259, arXiv:2301.11259.\u00a0\u21a9</p> </li> <li> <p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: datasets for machine learning on graphs. 2021. URL: https://arxiv.org/abs/2005.00687, arXiv:2005.00687.\u00a0\u21a9</p> </li> <li> <p>Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin A. Persson. The materials project: a materials genome approach. APL Materials, 2013. URL: https://materialsproject.org/, doi:10.1063/1.4812323.\u00a0\u21a9</p> </li> <li> <p>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. The open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059\u20136072, 2021. URL: https://pubs.acs.org/doi/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.\u00a0\u21a9</p> </li> <li> <p>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066\u20133084, 2023. URL: https://pubs.acs.org/doi/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.\u00a0\u21a9</p> </li> <li> <p>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059 6072, 2021. URL: https://doi.org/10.1021/acscatal.0c04525, arXiv:https://doi.org/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.\u00a0\u21a9</p> </li> <li> <p>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066 3084, February 2023. URL: http://dx.doi.org/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.\u00a0\u21a9</p> </li> <li> <p>Kamal Choudhary, Daniel Wines, Kangming Li, Kevin F. Garrity, Vishu Gupta, Aldo H. Romero, Jaron T. Krogel, Kayahan Saritas, Addis Fuhr, Panchapakesan Ganesh, Paul R. C. Kent, Keqiang Yan, Yuchao Lin, Shuiwang Ji, Ben Blaiszik, Patrick Reiser, Pascal Friederich, Ankit Agrawal, Pratyush Tiwary, Eric Beyerle, Peter Minch, Trevor D. Rhone, Ichiro Takeuchi, Robert B. Wexler, Arun Mannodi-Kanakkithodi, Elif Ertekin, Avanish Mishra, Nithin Mathew, Mitchell Wood, Andrew D. Rohskopf, Jason Hattrick-Simpers, Shih-Han Wang, Luke E. K. Achenie, Hongliang Xin, Maureen Williams, Adam J. Biacchi, and Francesca Tavazza. JARVIS-Leaderboard: a large scale benchmark of materials design methods. npj Computational Materials, 10(1):93, 2024. URL: https://doi.org/10.1038/s41524-024-01259-w, doi:10.1038/s41524-024-01259-w.\u00a0\u21a9</p> </li> <li> <p>Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofr\u00edo, Johannes Klepsch, and Andre Luckow. Application-oriented benchmarking of quantum generative learning using quark. In 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), 475 484. IEEE, September 2023. URL: http://dx.doi.org/10.1109/QCE57702.2023.00061, doi:10.1109/qce57702.2023.00061.\u00a0\u21a9</p> </li> <li> <p>Yining Luo, Yingfa Chen, and Zhen Zhang. Cfdbench: a large-scale benchmark for machine learning methods in fluid dynamics. 2024. URL: https://arxiv.org/abs/2310.05963.\u00a0\u21a9</p> </li> <li> <p>Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: a multi-task metadataset for classifying satellite imagery using vision-language models. 2023. URL: https://huggingface.co/datasets/saral-ai/satimagnet.\u00a0\u21a9</p> </li> <li> <p>Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, and Aditya Grover. Climatelearn: benchmarking machine learning for weather and climate modeling. 2023. URL: https://arxiv.org/abs/2307.01909, arXiv:2307.01909.\u00a0\u21a9</p> </li> <li> <p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u015f, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed Gonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco\u0144, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col\u00f3n, Luke Metz, L\u00fctfi Kerem \u015eenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ram\u00edrez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142 Sw\u0119drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. 2023. URL: https://arxiv.org/abs/2206.04615, arXiv:2206.04615.\u00a0\u21a9</p> </li> <li> <p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: a question answering challenge targeting commonsense knowledge. 2019. URL: https://arxiv.org/abs/1811.00937, arXiv:1811.00937.\u00a0\u21a9</p> </li> <li> <p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. 2019. URL: https://arxiv.org/abs/1907.10641, arXiv:1907.10641.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Diana Kafkes and Jason St. John. Boostr: a dataset for accelerator control systems. 2021. URL: https://arxiv.org/abs/2101.08359, arXiv:2101.08359.\u00a0\u21a9</p> </li> <li> <p>Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, and Thea K. Aarrestad. Ultrafast jet classification on fpgas for the hl-lhc. 2024. URL: https://arxiv.org/abs/2402.01876, arXiv:2402.01876, doi:https://doi.org/10.1088/2632-2153/ad5f10.\u00a0\u21a9</p> </li> <li> <p>Maira Khan, Steve Krave, Vittorio Marinozzi, Jennifer Ngadiuba, Stoyan Stoynev, and Nhan Tran. Benchmarking and interpreting real time quench detection algorithms. In Fast Machine Learning for Science Conference 2024. Purdue University, IN, October 2024. indico.cern.ch. URL: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf.\u00a0\u21a9</p> </li> <li> <p>A. Abed Abud, B. Abi, R. Acciarri, M. A. Acero, G. Adamov, D. Adams, M. Adinolfi, A. Aduszkiewicz, Z. Ahmad, J. Ahmed, T. Alion, S. Alonso Monsalve, M. Alrashed, C. Alt, A. Alton, P. Amedo, J. Anderson, C. Andreopoulos, M. P. Andrews, F. Andrianala, S. Andringa, N. Anfimov, A. Ankowski, M. Antonova, S. Antusch, A. Aranda-Fernandez, A. Ariga, L. O. Arnold, M. A. Arroyave, J. Asaadi, A. Aurisano, V. Aushev, D. Autiero, M. Ayala-Torres, F. Azfar, H. Back, J. J. Back, C. Backhouse, P. Baesso, I. Bagaturia, L. Bagby, S. Balasubramanian, P. Baldi, B. Baller, B. Bambah, F. Barao, G. Barenboim, G. J. Barker, W. Barkhouse, C. Barnes, G. Barr, J. Barranco Monarca, N. Barros, J. L. Barrow, A. Basharina-Freshville, A. Bashyal, V. Basque, E. Belchior, J. B. R. Battat, F. Battisti, F. Bay, J. L. Bazo Alba, J. F. Beacom, E. Bechetoille, B. Behera, L. Bellantoni, G. Bellettini, V. Bellini, O. Beltramello, D. Belver, N. Benekos, F. Bento Neves, S. Berkman, P. Bernardini, R. M. Berner, H. Berns, S. Bertolucci, M. Betancourt, A. Betancur Rodr\u00edguez, M. Bhattacharjee, S. Bhuller, B. Bhuyan, S. Biagi, J. Bian, M. Biassoni, K. Biery, B. Bilki, M. Bishai, A. Bitadze, A. Blake, F. D. M. Blaszczyk, G. C. Blazey, E. Blucher, J. Boissevain, S. Bolognesi, T. Bolton, L. Bomben, M. Bonesini, M. Bongrand, F. Bonini, A. Booth, C. Booth, S. Bordoni, A. Borkum, T. Boschi, N. Bostan, P. Bour, C. Bourgeois, S. B. Boyd, D. Boyden, J. Bracinik, D. Braga, D. Brailsford, A. Brandt, J. Bremer, C. Brew, E. Brianne, S. J. Brice, C. Brizzolari, C. Bromberg, G. Brooijmans, J. Brooke, A. Bross, G. Brunetti, M. Brunetti, N. Buchanan, H. Budd, D. Caiulo, P. Calafiura, J. Calcutt, M. Calin, S. Calvez, E. Calvo, A. Caminata, M. Campanelli, K. Cankocak, D. Caratelli, G. Carini, B. Carlus, P. Carniti, I. Caro Terrazas, H. Carranza, T. Carroll, J. F. Casta\u00f1o Forero, A. Castillo, C. Castromonte, E. Catano-Mur, C. Cattadori, F. Cavalier, F. Cavanna, S. Centro, G. Cerati, A. Cervelli, A. Cervera Villanueva, M. Chalifour, A. Chappell, E. Chardonnet, N. Charitonidis, A. Chatterjee, S. Chattopadhyay, H. Chen, M. Chen, Y. Chen, Z. Chen, D. Cherdack, C. Chi, S. Childress, A. Chiriacescu, G. Chisnall, K. Cho, S. Choate, D. Chokheli, S. Choubey, A. Christensen, D. Christian, G. Christodoulou, A. Chukanov, E. Church, P. Clarke, T. E. Coan, A. G. Cocco, J. A. B. Coelho, E. Conley, R. Conley, J. M. Conrad, M. Convery, S. Copello, L. Corwin, L. Cremaldi, L. Cremonesi, J. I. Crespo-Anad\u00f3n, E. Cristaldo, R. Cross, A. Cudd, C. Cuesta, Y. Cui, D. Cussans, M. Dabrowski, O. Dalager, H. da Motta, L. Da Silva Peres, C. David, Q. David, G. S. Davies, S. Davini, J. Dawson, K. De, R. M. De Almeida, P. Debbins, I. De Bonis, M. P. Decowski, A. de Gouv\u00eaa, P. C. De Holanda, I. L. De Icaza Astiz, A. Deisting, P. De Jong, A. Delbart, D. Delepine, M. Delgado, A. Dell'Acqua, P. De Lurgio, J. R. T. de Mello Neto, D. M. DeMuth, S. Dennis, C. Densham, G. W. Deptuch, A. De Roeck, V. De Romeri, G. De Souza, R. Dharmapalan, F. Diaz, J. S. D\u00edaz, S. Di Domizio, L. Di Giulio, P. Ding, L. Di Noto, C. Distefano, R. Diurba, M. Diwan, Z. Djurcic, N. Dokania, S. Dolan, M. J. Dolinski, L. Domine, D. Douglas, D. Douillet, G. Drake, F. Drielsma, D. Duchesneau, K. Duffy, P. Dunne, T. Durkin, H. Duyang, O. Dvornikov, D. A. Dwyer, A. S. Dyshkant, M. Eads, A. Earle, D. Edmunds, J. Eisch, L. Emberger, S. Emery, A. Ereditato, C. O. Escobar, G. Eurin, J. J. Evans, E. Ewart, A. C. Ezeribe, K. Fahey, A. Falcone, C. Farnese, Y. Farzan, J. Felix, M. Fernandes Carneiro da Silva, E. Fernandez-Martinez, P. Fernandez Menendez, F. Ferraro, L. Fields, F. Filthaut, A. Fiorentini, R. S. Fitzpatrick, W. Flanagan, B. Fleming, R. Flight, D. V. Forero, J. Fowler, W. Fox, J. Franc, K. Francis, D. Franco, J. Freeman, J. Freestone, J. Fried, A. Friedland, S. Fuess, I. Furic, A. P. Furmanski, A. Gago, H. Gallagher, A. Gallas, A. Gallego-Ros, N. Gallice, V. Galymov, E. Gamberini, T. Gamble, R. Gandhi, R. Gandrajula, F. Gao, S. Gao, D. Garcia-Gamez, M. \u00c1 Garc\u00eda-Peris, S. Gardiner, D. Gastler, G. Ge, B. Gelli, A. Gendotti, S. Gent, Z. Ghorbani-Moghaddam, D. Gibin, I. Gil-Botella, S. Gilligan, C. Girerd, A. K. Giri, D. Gnani, O. Gogota, M. Gold, S. Gollapinni, K. Gollwitzer, R. A. Gomes, L. V. Gomez Bermeo, L. S. Gomez Fajardo, F. Gonnella, J. A. Gonzalez-Cuevas, D. Gonzalez-Diaz, M. Gonzalez-Lopez, M. C. Goodman, O. Goodwin, S. Goswami, C. Gotti, E. Goudzovski, C. Grace, M. Graham, R. Gran, E. Granados, P. Granger, A. Grant, C. Grant, D. Gratieri, P. Green, L. Greenler, J. Greer, W. C. Griffith, M. Groh, J. Grudzinski, K. Grzelak, W. Gu, V. Guarino, R. Guenette, E. Guerard, A. Guglielmi, B. Guo, K. K. Guthikonda, R. Gutierrez, P. Guzowski, M. M. Guzzo, S. Gwon, A. Habig, H. Hadavand, R. Haenni, A. Hahn, J. Haiston, P. Hamacher-Baumann, T. Hamernik, P. Hamilton, J. Han, D. A. Harris, J. Hartnell, J. Harton, T. Hasegawa, C. Hasnip, R. Hatcher, K. W. Hatfield, A. Hatzikoutelis, C. Hayes, E. Hazen, A. Heavey, K. M. Heeger, J. Heise, K. Hennessy, S. Henry, M. A. Hernandez Morquecho, K. Herner, L. Hertel, V Hewes, A. Higuera, T. Hill, S. J. Hillier, A. Himmel, J. Hoff, C. Hohl, A. Holin, E. Hoppe, G. A. Horton-Smith, M. Hostert, A. Hourlier, B. Howard, R. Howell, J. Huang, J. Huang, J. Hugon, G. Iles, N. Ilic, A. M. Iliescu, R. Illingworth, A. Ioannisian, L. Isenhower, R. Itay, A. Izmaylov, S. Jackson, V. Jain, E. James, B. Jargowsky, F. Jediny, D. Jena, Y. S. Jeong, C. Jes\u00fas-Valls, X. Ji, L. Jiang, S. Jim\u00e9nez, A. Jipa, R. Johnson, B. Jones, S. B. Jones, M. Judah, C. K. Jung, T. Junk, Y. Jwa, M. Kabirnezhad, A. Kaboth, I. Kadenko, I. Kakorin, F. Kamiya, N. Kaneshige, G. Karagiorgi, G. Karaman, A. Karcher, M. Karolak, Y. Karyotakis, S. Kasai, S. P. Kasetti, L. Kashur, N. Kazaryan, E. Kearns, P. Keener, K. J. Kelly, E. Kemp, O. Kemularia, W. Ketchum, S. H. Kettell, M. Khabibullin, A. Khotjantsev, A. Khvedelidze, D. Kim, B. King, B. Kirby, M. Kirby, J. Klein, K. Koehler, L. W. Koerner, S. Kohn, P. P. Koller, L. Kolupaeva, M. Kordosky, T. Kosc, U. Kose, V. A. Kosteleck\u00fd, K. Kothekar, F. Krennrich, I. Kreslo, Y. Kudenko, V. A. Kudryavtsev, S. Kulagin, J. Kumar, P. Kumar, P. Kunze, N. Kurita, C. Kuruppu, V. Kus, T. Kutter, A. Lambert, B. Land, K. Lande, C. E. Lane, K. Lang, T. Langford, J. Larkin, P. Lasorak, D. Last, C. Lastoria, A. Laundrie, A. Lawrence, I. Lazanu, R. LaZur, T. Le, S. Leardini, J. Learned, P. LeBrun, T. LeCompte, G. Lehmann Miotto, R. Lehnert, M. A. Leigui de Oliveira, M. Leitner, L. Li, S. W. Li, T. Li, Y. Li, H. Liao, C. S. Lin, Q. Lin, S. Lin, A. Lister, B. R. Littlejohn, J. Liu, S. Lockwitz, T. Loew, M. Lokajicek, I. Lomidze, K. Long, K. Loo, D. Lorca, T. Lord, J. M. LoSecco, W. C. Louis, X. -G. Lu, K. B. Luk, X. Luo, N. Lurkin, T. Lux, V. P. Luzio, D. MacFarlane, A. A. Machado, P. Machado, C. T. Macias, J. R. Macier, A. Maddalena, A. Madera, P. Madigan, S. Magill, K. Mahn, A. Maio, A. Major, J. A. Maloney, G. Mandrioli, R. C. Mandujano, J. Maneira, L. Manenti, S. Manly, A. Mann, K. Manolopoulos, M. Manrique Plata, V. N. Manyam, L. Manzanillas, M. Marchan, A. Marchionni, W. Marciano, D. Marfatia, C. Mariani, J. Maricic, R. Marie, F. Marinho, A. D. Marino, D. Marsden, M. Marshak, C. M. Marshall, J. Marshall, J. Marteau, J. Martin-Albo, N. Martinez, D. A. Martinez Caicedo, S. Martynenko, K. Mason, A. Mastbaum, M. Masud, S. Matsuno, J. Matthews, C. Mauger, N. Mauri, K. Mavrokoridis, I. Mawby, R. Mazza, A. Mazzacane, E. Mazzucato, T. McAskill, E. McCluskey, N. McConkey, K. S. McFarland, C. McGrew, A. McNab, A. Mefodiev, P. Mehta, P. Melas, O. Mena, S. Menary, H. Mendez, D. P. M\u00e9ndez, A. Menegolli, G. Meng, M. D. Messier, W. Metcalf, T. Mettler, M. Mewes, H. Meyer, T. Miao, G. Michna, T. Miedema, J. Migenda, V. Mikola, R. Milincic, W. Miller, J. Mills, C. Milne, O. Mineev, O. G. Miranda, S. Miryala, C. S. Mishra, S. R. Mishra, A. Mislivec, D. Mladenov, I. Mocioiu, K. Moffat, N. Moggi, R. Mohanta, T. A. Mohayai, N. Mokhov, J. Molina, L. Molina Bueno, A. Montanari, C. Montanari, D. Montanari, L. M. Montano Zetina, J. Moon, M. Mooney, A. F. Moor, D. Moreno, C. Morris, C. Mossey, E. Motuk, C. A. Moura, J. Mousseau, W. Mu, L. Mualem, J. Mueller, M. Muether, S. Mufson, F. Muheim, A. Muir, M. Mulhearn, D. Munford, H. Muramatsu, S. Murphy, J. Musser, J. Nachtman, S. Nagu, M. Nalbandyan, R. Nandakumar, D. Naples, S. Narita, D. Navas-Nicol\u00e1s, A. Navrer-Agasson, N. Nayak, M. Nebot-Guinot, K. Negishi, J. K. Nelson, J. Nesbit, M. Nessi, D. Newbold, M. Newcomer, D. Newhart, H. Newton, R. Nichol, F. Nicolas-Arnaldos, E. Niner, K. Nishimura, A. Norman, A. Norrick, R. Northrop, P. Novella, J. A. Nowak, M. Oberling, J. P. Ochoa-Ricoux, A. Olivares Del Campo, A. Olivier, A. Olshevskiy, Y. Onel, Y. Onishchuk, J. Ott, L. Pagani, S. Pakvasa, G. Palacio, O. Palamara, S. Palestini, J. M. Paley, M. Pallavicini, C. Palomares, J. L. Palomino-Gallo, E. Pantic, V. Paolone, V. Papadimitriou, R. Papaleo, A. Papanestis, S. Paramesvaran, S. Parke, Z. Parsa, M. Parvu, S. Pascoli, L. Pasqualini, J. Pasternak, J. Pater, C. Patrick, L. Patrizii, R. B. Patterson, S. J. Patton, T. Patzak, A. Paudel, B. Paulos, L. Paulucci, Z. Pavlovic, G. Pawloski, D. Payne, V. Pec, S. J. M. Peeters, E. Pennacchio, A. Penzo, O. L. G. Peres, J. Perry, D. Pershey, G. Pessina, G. Petrillo, C. Petta, R. Petti, F. Piastra, L. Pickering, F. Pietropaolo, R. Plunkett, R. Poling, X. Pons, N. Poonthottathil, S. Pordes, J. Porter, M. Potekhin, R. Potenza, B. V. K. S. Potukuchi, J. Pozimski, M. Pozzato, S. Prakash, T. Prakash, S. Prince, D. Pugnere, X. Qian, M. C. Queiroga Bazetto, J. L. Raaf, V. Radeka, J. Rademacker, B. Radics, A. Rafique, E. Raguzin, M. Rai, M. Rajaoalisoa, I. Rakhno, A. Rakotonandrasana, L. Rakotondravohitra, Y. A. Ramachers, R. Rameika, M. A. Ramirez Delgado, B. Ramson, A. Rappoldi, G. Raselli, P. Ratoff, S. Raut, R. F. Razakamiandra, J. S. Real, B. Rebel, M. Reggiani-Guzzo, T. Rehak, J. Reichenbacher, S. D. Reitzner, H. Rejeb Sfar, A. Renshaw, S. Rescia, F. Resnati, A. Reynolds, C. Riccio, G. Riccobene, L. C. J. Rice, J. Ricol, A. Rigamonti, Y. Rigaut, D. Rivera, L. Rochester, M. Roda, P. Rodrigues, M. J. Rodriguez Alonso, E. Rodriguez Bonilla, J. Rodriguez Rondon, S. Rosauro-Alcaraz, M. Rosenberg, P. Rosier, B. Roskovec, M. Rossella, J. Rout, P. Roy, S. Roy, A. Rubbia, C. Rubbia, F. C. Rubio, B. Russell, D. Ruterbories, R. Saakyan, S. Sacerdoti, T. Safford, R. Sahay, N. Sahu, P. Sala, N. Samios, O. Samoylov, M. C. Sanchez, D. A. Sanders, D. Sankey, S. Santana, M. Santos-Maldonado, N. Saoulidou, P. Sapienza, C. Sarasty, I. Sarcevic, G. Savage, V. Savinov, A. Scaramelli, A. Scarff, A. Scarpelli, T. Schaffer, H. Schellman, P. Schlabach, D. Schmitz, K. Scholberg, A. Schukraft, E. Segreto, J. Sensenig, I. Seong, A. Sergi, D. Sgalaberna, M. H. Shaevitz, S. Shafaq, M. Shamma, R. Sharankova, H. R. Sharma, R. Sharma, R. Kumar, T. Shaw, C. Shepherd-Themistocleous, S. Shin, D. Shooltz, R. Shrock, L. Simard, F. Simon, N. Simos, J. Sinclair, G. Sinev, J. Singh, J. Singh, V. Singh, R. Sipos, F. W. Sippach, G. Sirri, A. Sitraka, K. Siyeon, K. Skarpaas VIII, A. Smith, E. Smith, P. Smith, J. Smolik, M. Smy, E. L. Snider, P. Snopok, M. Soares Nunes, H. Sobel, M. Soderberg, C. J. Solano Salinas, S. S\u00f6ldner-Rembold, N. Solomey, V. Solovov, W. E. Sondheim, M. Sorel, J. Soto-Oton, A. Sousa, K. Soustruznik, F. Spagliardi, M. Spanu, J. Spitz, N. J. C. Spooner, K. Spurgeon, R. Staley, M. Stancari, L. Stanco, R. Stanley, R. Stein, H. M. Steiner, J. Stewart, B. Stillwell, J. Stock, F. Stocker, T. Stokes, M. Strait, T. Strauss, S. Striganov, A. Stuart, J. G. Suarez, H. Sullivan, D. Summers, A. Surdo, V. Susic, L. Suter, C. M. Sutera, R. Svoboda, B. Szczerbinska, A. M. Szelc, R. Talaga, H. A. Tanaka, B. Tapia Oregui, A. Tapper, S. Tariq, E. Tatar, R. Tayloe, A. M. Teklu, M. Tenti, K. Terao, C. A. Ternes, F. Terranova, G. Testera, A. Thea, J. L. Thompson, C. Thorn, S. C. Timm, J. Todd, A. Tonazzo, D. Torbunov, M. Torti, M. Tortola, F. Tortorici, D. Totani, M. Toups, C. Touramanis, J. Trevor, S. Trilov, W. H. Trzaska, Y. T. Tsai, Z. Tsamalaidze, K. V. Tsang, N. Tsverava, S. Tufanli, C. Tull, E. Tyley, M. Tzanov, M. A. Uchida, J. Urheim, T. Usher, S. Uzunyan, M. R. Vagins, P. Vahle, G. A. Valdiviesso, E. Valencia, Z. Vallari, J. W. F. Valle, S. Vallecorsa, R. Van Berg, R. G. Van de Water, F. Varanini, D. Vargas, G. Varner, J. Vasel, S. Vasina, G. Vasseur, N. Vaughan, K. Vaziri, S. Ventura, A. Verdugo, S. Vergani, M. A. Vermeulen, M. Verzocchi, M. Vicenzi, H. Vieira de Souza, C. Vignoli, C. Vilela, B. Viren, T. Vrba, T. Wachala, A. V. Waldron, M. Wallbank, H. Wang, J. Wang, M. H. L. S. Wang, Y. Wang, Y. Wang, K. Warburton, D. Warner, M. Wascko, D. Waters, A. Watson, P. Weatherly, A. Weber, M. Weber, H. Wei, A. Weinstein, D. Wenman, M. Wetstein, A. White, L. H. Whitehead, D. Whittington, M. J. Wilking, C. Wilkinson, Z. Williams, F. Wilson, R. J. Wilson, J. Wolcott, T. Wongjirad, A. Wood, K. Wood, E. Worcester, M. Worcester, C. Wret, W. Wu, W. Wu, Y. Xiao, E. Yandel, G. Yang, K. Yang, S. Yang, T. Yang, A. Yankelevich, N. Yershov, K. Yonehara, T. Young, B. Yu, H. Yu, J. Yu, W. Yuan, R. Zaki, J. Zalesak, L. Zambelli, B. Zamorano, A. Zani, L. Zazueta, G. Zeit, G. P. Zeller, J. Zennamo, K. Zeug, C. Zhang, M. Zhao, E. Zhivun, G. Zhu, P. Zilberman, E. D. Zimmerman, M. Zito, S. Zucchelli, J. Zuklin, V. Zutshi, and R. Zwaska. Deep underground neutrino experiment (dune) near detector conceptual design report. 2021. URL: https://arxiv.org/abs/2103.13910, arXiv:2103.13910.\u00a0\u21a9</p> </li> <li> <p>J. Kvapil, G. Borca-Tasciuc, H. Bossi, K. Chen, Y. Chen, Y. Corrales Morales, H. Da Costa, C. Da Silva, C. Dean, J. Durham, S. Fu, C. Hao, P. Harris, O. Hen, H. Jheng, Y. Lee, P. Li, X. Li, Y. Lin, M. X. Liu, V. Loncar, J. P. Mitrevski, A. Olvera, M. L. Purschke, J. S. Renck, G. Roland, J. Schambach, Z. Shi, N. Tran, N. Wuerfel, B. Xu, D. Yu, and H. Zhang. Intelligent experiments through real-time ai: fast data processing and autonomous detector control for sphenix and future eic detectors. 2025. URL: https://arxiv.org/abs/2501.04845, arXiv:2501.04845.\u00a0\u21a9</p> </li> <li> <p>Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, and Javier Duarte. Neural architecture codesign for fast physics applications. 2025. URL: https://arxiv.org/abs/2501.05515, arXiv:2501.05515.\u00a0\u21a9</p> </li> <li> <p>Benjamin Parpillon, Chinar Syal, Jieun Yoo, Jennet Dickinson, Morris Swartz, Giuseppe Di Guglielmo, Alice Bean, Douglas Berry, Manuel Blanco Valentin, Karri DiPetrillo, Anthony Badea, Lindsey Gray, Petar Maksimovic, Corrinne Mills, Mark S. Neubauer, Gauri Pradhan, Nhan Tran, Dahai Wen, and Farah Fahim. Smart pixels: in-pixel ai for on-sensor data filtering. 2024. URL: https://arxiv.org/abs/2406.14860, arXiv:2406.14860.\u00a0\u21a9</p> </li> <li> <p>Zhengchun Liu, Hemant Sharma, Jun-Sang Park, Peter Kenesei, Antonino Miceli, Jonathan Almer, Rajkumar Kettimuthu, and Ian Foster. Braggnn: fast x-ray bragg peak analysis using deep learning. 2021. URL: https://arxiv.org/abs/2008.08198, arXiv:2008.08198.\u00a0\u21a9</p> </li> <li> <p>Shuyu Qin, Joshua Agar, and Nhan Tran. Extremely noisy 4d-tem strain mapping using cycle consistent spatial transforming autoencoders. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop. 2023. URL: https://openreview.net/forum?id=7yt3N0o0W9.\u00a0\u21a9</p> </li> <li> <p>Yumou Wei, Ryan F. Forelli, Chris Hansen, Jeffrey P. Levesque, Nhan Tran, Joshua C. Agar, Giuseppe Di Guglielmo, Michael E. Mauel, and Gerald A. Navratil. Low latency optical-based mode tracking with machine learning deployed on fpgas on a tokamak. 2024. URL: https://arxiv.org/abs/2312.00128, arXiv:2312.00128, doi:https://doi.org/10.1063/5.0190354.\u00a0\u21a9</p> </li> <li> <p>Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and Hainan Ye. Aibench: an industry standard internet service ai benchmark suite. 2019. URL: https://arxiv.org/abs/1908.08998, arXiv:1908.08998.\u00a0\u21a9</p> </li> <li> <p>Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao, Shujie Zhang, and Jiahui Dai. Bigdatabench: a scalable and unified big data and ai benchmark suite. 2018. URL: https://arxiv.org/abs/1802.08254, arXiv:1802.08254.\u00a0\u21a9</p> </li> <li> <p>Steven Farrell, Murali Emani, Jacob Balma, Lukas Drescher, Aleksandr Drozd, Andreas Fink, Geoffrey Fox, David Kanter, Thorsten Kurth, Peter Mattson, Dawei Mu, Amit Ruhela, Kento Sato, Koichi Shirahata, Tsuguchika Tabaru, Aristeidis Tsaris, Jan Balewski, Ben Cumming, Takumi Danjo, Jens Domke, Takaaki Fukai, Naoto Fukumoto, Tatsuya Fukushi, Balazs Gerofi, Takumi Honda, Toshiyuki Imamura, Akihiko Kasagi, Kentaro Kawakami, Shuhei Kudo, Akiyoshi Kuroda, Maxime Martinasso, Satoshi Matsuoka, Henrique Mendon\u00e7a, Kazuki Minami, Prabhat Ram, Takashi Sawada, Mallikarjun Shankar, Tom St. John, Akihiro Tabuchi, Venkatram Vishwanath, Mohamed Wahib, Masafumi Yamazaki, and Junqi Yin. Mlperf hpc: a holistic benchmark suite for scientific machine learning on hpc systems. 2021. URL: https://arxiv.org/abs/2110.11466, arXiv:2110.11466.\u00a0\u21a9</p> </li> <li> <p>Jeyan Thiyagalingam, Gregor von Laszewski, Junqi Yin, Murali Emani, Juri Papay, Gregg Barrett, Piotr Luszczek, Aristeidis Tsaris, Christine Kirkpatrick, Feiyi Wang, Tom Gibbs, Venkatram Vishwanath, Mallikarjun Shankar, Geoffrey Fox, and Tony Hey. Ai benchmarking for science: efforts from the mlcommons science working group. In Hartwig Anzt, Amanda Bienz, Piotr Luszczek, and Marc Baboulin, editors, High Performance Computing. ISC High Performance 2022 International Workshops, 47\u201364. Cham, 2022. Springer International Publishing.\u00a0\u21a9</p> </li> <li> <p>Thea Aarrestad, Ekaterina Govorkova, Jennifer Ngadiuba, Ema Puljak, Maurizio Pierini, and Kinga Anna Wozniak. Unsupervised new physics detection at 40 mhz: training dataset. 2021. URL: https://zenodo.org/record/5046389, doi:10.5281/ZENODO.5046389.\u00a0\u21a9</p> </li> <li> <p>Alexandros Karargyris, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, Prakash Narayana Moorthy, Alexander Chowdhury, Junyi Guo, Sahil Nalawade, Jacob Rosenthal, David Kanter, Maria Xenochristou, Daniel J. Beutel, Verena Chung, Timothy Bergquist, James Eddy, Abubakar Abid, Lewis Tunstall, Omar Sanseviero, Dimitrios Dimitriadis, Yiming Qian, Xinxing Xu, Yong Liu, Rick Siow Mong Goh, Srini Bala, Victor Bittorf, Sreekar Reddy Puchala, Biagio Ricciuti, Soujanya Samineni, Eshna Sengupta, Akshay Chaudhari, Cody Coleman, Bala Desinghu, Gregory Diamos, Debo Dutta, Diane Feddema, Grigori Fursin, Xinyuan Huang, Satyananda Kashyap, Nicholas Lane, Indranil Mallick, Pietro Mascagni, Virendra Mehta, Cassiano Ferro Moraes, Vivek Natarajan, Nikola Nikolov, Nicolas Padoy, Gennady Pekhimenko, Vijay Janapa Reddi, G. Anthony Reina, Pablo Ribalta, Abhishek Singh, Jayaraman J. Thiagarajan, Jacob Albrecht, Thomas Wolf, Geralyn Miller, Huazhu Fu, Prashant Shah, Daguang Xu, Poonam Yadav, David Talby, Mark M. Awad, Jeremy P. Howard, Michael Rosenthal, Luigi Marchionni, Massimo Loda, Jason M. Johnson, Spyridon Bakas, Peter Mattson, FeTS Consortium, BraTS-2020 Consortium, and AI4SafeChole Consortium. Federated benchmarking of medical artificial intelligence with medperf. Nature Machine Intelligence, 5(7):799\u2013810, July 2023. URL: https://doi.org/10.1038/s42256-023-00652-2, doi:10.1038/s42256-023-00652-2.\u00a0\u21a9</p> </li> <li> <p>Claudius Krause, Michele Faucci Giannelli, Gregor Kasieczka, Benjamin Nachman, Dalila Salamani, David Shih, Anna Zaborowska, Oz Amram, Kerstin Borras, Matthew R. Buckley, Erik Buhmann, Thorsten Buss, Renato Paulo Da Costa Cardoso, Anthony L. Caterini, Nadezda Chernyavskaya, Federico A. G. Corchia, Jesse C. Cresswell, Sascha Diefenbacher, Etienne Dreyer, Vijay Ekambaram, Engin Eren, Florian Ernst, Luigi Favaro, Matteo Franchini, Frank Gaede, Eilam Gross, Shih-Chieh Hsu, Kristina Jaruskova, Benno K\u00e4ch, Jayant Kalagnanam, Raghav Kansal, Taewoo Kim, Dmitrii Kobylianskii, Anatolii Korol, William Korcari, Dirk Kr\u00fccker, Katja Kr\u00fcger, Marco Letizia, Shu Li, Qibin Liu, Xiulong Liu, Gabriel Loaiza-Ganem, Thandikire Madula, Peter McKeown, Isabell-A. Melzer-Pellmann, Vinicius Mikuni, Nam Nguyen, Ayodele Ore, Sofia Palacios Schweitzer, Ian Pang, Kevin Pedro, Tilman Plehn, Witold Pokorski, Huilin Qu, Piyush Raikwar, John A. Raine, Humberto Reyes-Gonzalez, Lorenzo Rinaldi, Brendan Leigh Ross, Moritz A. W. Scham, Simon Schnake, Chase Shimmin, Eli Shlizerman, Nathalie Soybelman, Mudhakar Srivatsa, Kalliopi Tsolaki, Sofia Vallecorsa, Kyongmin Yeo, and Rui Zhang. Calochallenge 2022: a community challenge for fast calorimeter simulation. 2024. URL: https://arxiv.org/abs/2410.21611, arXiv:2410.21611.\u00a0\u21a9</p> </li> <li> <p>Avrim Blum and Moritz Hardt. The ladder: a reliable leaderboard for machine learning competitions. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, 1006\u20131014. Lille, France, July 2015. PMLR. URL: https://proceedings.mlr.press/v37/blum15.html.\u00a0\u21a9</p> </li> <li> <p>Zhen Xu, Sergio Escalera, Adrien Pav\u00e3o, Magali Richard, Wei-Wei Tu, Quanming Yao, Huan Zhao, and Isabelle Guyon. Codabench: flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543, July 2022. URL: http://dx.doi.org/10.1016/j.patter.2022.100543, doi:10.1016/j.patter.2022.100543.\u00a0\u21a9</p> </li> <li> <p>Piotr Luszczek. Sabath: fair metadata technology for surrogate benchmarks. Technical Report, University of Tennessee, 2021. URL: https://github.com/icl-utk-edu/slip/tree/sabath.\u00a0\u21a9</p> </li> <li> <p>Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. Pdebench: an extensive benchmark for scientific machine learning. 2024. URL: https://arxiv.org/abs/2210.07182, arXiv:2210.07182.\u00a0\u21a9</p> </li> <li> <p>Ruben Ohana, Michael McCabe, Lucas Meyer, Rudy Morel, Fruzsina J. Agocs, Miguel Beneitez, Marsha Berger, Blakesley Burkhart, Stuart B. Dalziel, Drummond B. Fielding, Daniel Fortunato, Jared A. Goldberg, Keiya Hirashima, Yan-Fei Jiang, Rich R. Kerswell, Suryanarayana Maddu, Jonah Miller, Payel Mukhopadhyay, Stefan S. Nixon, Jeff Shen, Romain Watteaux, Bruno R\u00e9galdo-Saint Blancard, Fran\u00e7ois Rozet, Liam H. Parker, Miles Cranmer, and Shirley Ho. The well: a large-scale collection of diverse physics simulations for machine learning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 44989\u201345037. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, volume, 1362 1379. 2024. doi:10.1109/SCW63240.2024.00178.\u00a0\u21a9</p> </li> <li> <p>Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: efficient execution of structured language model programs. 2024. URL: https://arxiv.org/abs/2312.07104, arXiv:2312.07104.\u00a0\u21a9</p> </li> <li> <p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, 611 626. New York, NY, USA, 2023. Association for Computing Machinery. URL: https://doi.org/10.1145/3600006.3613165, doi:10.1145/3600006.3613165.\u00a0\u21a9</p> </li> <li> <p>Simon Mo. Vllm performance dashboard. 2024. URL: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/.\u00a0\u21a9</p> </li> <li> <p>Kin G. Olivares, Cristian Chall\u00fa, Federico Garza, Max Mergenthaler Canseco, and Artur Dubrawski. Neuralforecast: user friendly state-of-the-art neural forecasting models. PyCon Salt Lake City, Utah, US 2022, 2022. URL: https://github.com/Nixtla/neuralforecast.\u00a0\u21a9</p> </li> <li> <p>Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 37, 6989\u20136997. 2023.\u00a0\u21a9</p> </li> <li> <p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: time series forecasting by reprogramming large language models. 2024. URL: https://arxiv.org/abs/2310.01728, arXiv:2310.01728.\u00a0\u21a9</p> </li> <li> <p>Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1. 2024. URL: https://arxiv.org/abs/2310.03589, arXiv:2310.03589.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Giuseppe Di Guglielmo, Botao Du, Javier Campos, Alexandra Boltasseva, Akash V. Dixit, Farah Fahim, Zhaxylyk Kudyshev, Santiago Lopez, Ruichao Ma, Gabriel N. Perdue, Nhan Tran, Omer Yesilyurt, and Daniel Bowring. End-to-end workflow for machine learning-based qubit readout with qick and hls4ml. 2025. URL: https://arxiv.org/abs/2501.14663, arXiv:2501.14663.\u00a0\u21a9</p> </li> <li> <p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.\u00a0\u21a9</p> </li> <li> <p>Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, and Xi Peng. Seafloorai: a large-scale vision-language dataset for seafloor geological survey. 2024. URL: https://arxiv.org/abs/2411.00172, arXiv:2411.00172.\u00a0\u21a9</p> </li> <li> <p>Pin Chen, Luoxuan Peng, Rui Jiao, Qing Mo, Zhen Wang, Wenbing Huang, Yang Liu, and Yutong Lu. Learning superconductivity from ordered and disordered material structures. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 108902\u2013108928. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Deyu Zou, Shikun Liu, Siqi Miao, Victor Fung, Shiyu Chang, and Pan Li. Gess: benchmarking geometric deep learning under scientific applications with distribution shifts. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 92499\u201392528. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Ralph E Peterson, Aramis Tanelus, Christopher Ick, Bartul Mimica, Niegil Francis, Violet J Ivan, Aman Choudhri, Annegret L Falkner, Mala Murthy, David M Schneider, Dan H Sanes, and Alex H Williams. Vocal call locator benchmark (vcl) for localizing rodent vocalizations from multi-channel audio. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 106370\u2013106382. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Roman Bushuiev, Anton Bushuiev, Niek F. de Jonge, Adamo Young, Fleming Kretschmer, Raman Samusevich, Janne Heirman, Fei Wang, Luke Zhang, Kai D\u00fchrkop, Marcus Ludwig, Nils A. Haupt, Apurva Kalia, Corinna Brungs, Robin Schmid, Russell Greiner, Bo Wang, David S. Wishart, Li-Ping Liu, Juho Rousu, Wout Bittremieux, Hannes Rost, Tytus D. Mak, Soha Hassoun, Florian Huber, Justin J.J. van der Hooft, Michael A. Stravs, Sebastian B\u00f6cker, Josef Sivic, and Tom\u00e1\u0161 Pluskal. Massspecgym: a benchmark for the discovery and identification of molecules. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 110010\u2013110027. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Yiheng Wang, Tianyu Wang, Yuying Zhang, Hongji Zhang, Haoyu Zheng, Guanjie Zheng, and Linghe Kong. Urbandatalayer: a unified data pipeline for urban science. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 7296\u20137310. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Kuzma Khrabrov, Anton Ber, Artem Tsypin, Konstantin Ushenin, Egor Rumiantsev, Alexander Telepov, Dmitry Protasov, Ilya Shenbin, Anton Alekseev, Mikhail Shirokikh, Sergey Nikolenko, Elena Tutubalina, and Artur Kadurin. Delta-squared dft: a universal quantum chemistry dataset of drug-like molecules and a benchmark for neural network potentials. 2024. URL: https://arxiv.org/abs/2406.14347, arXiv:2406.14347.\u00a0\u21a9</p> </li> <li> <p>Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, and Enhong Chen. Exploring user retrieval integration towards large language models for cross-domain sequential recommendation. 2024. URL: https://arxiv.org/abs/2406.03085, arXiv:2406.03085.\u00a0\u21a9</p> </li> <li> <p>Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. Spiqa: a dataset for multimodal question answering on scientific papers. 2025. URL: https://arxiv.org/abs/2407.09413, arXiv:2407.09413.\u00a0\u21a9</p> </li> </ol>"},{"location":"md/benchmarks_table/","title":"Table","text":""},{"location":"md/benchmarks_table/#benchmarks-table","title":"Benchmarks (Table)","text":"Date Name Domain Focus Keywords Task Types Metrics Models Citation Software Rating Software Reason Specification Rating Specification Reason Dataset Rating Dataset Reason Metrics Rating Metrics Reason Reference Solution Rating Reference Solution Reason Documentation Rating Documentation Reason Average Ratings 2020-09-07 MMLU (Massive Multitask Language Understanding) Multidomain Academic knowledge and reasoning across 57 subjects multitask, multiple-choice, zero-shot, few-shot, knowledge probing Multiple choice Accuracy GPT-4o, Gemini 1.5 Pro, o1, DeepSeek-R1 <sup>1</sup> 0 No instructions to download or run data given on the site 4 No system constraints 5 Meets all FAIR principles and properly versioned. 5 Fully defined, represents a solution's performance. 2 Reference models are available (i.e. GPT-3), but are not trainable or publicly documented 5 Well-explained in a provided paper. 3.5 2023-11-20 GPQA Diamond Science Graduate-level scientific reasoning Google-proof, graduate-level, science QA, chemistry, physics Multiple choice, Multi-step QA Accuracy o1, DeepSeek-R1 <sup>2</sup> 5 Python version and requirements specified on Github site 2 No system constraints or I/O specified 5 Easily able to access dataset. Comes with predefined splits as mentioned in the paper 5 Each question has a correct answer, representing the tested model's performance. 1 Common models such as GPT-3.5 were compared. They are not open and don't provide requirements 5 All information is listed in the associated paper 3.833 2018-03-14 ARC-Challenge (Advanced Reasoning Challenge) Science Grade-school science with reasoning emphasis grade-school, science QA, challenge set, reasoning Multiple choice Accuracy GPT-4, Claude <sup>3</sup> 0 No link to code or documentation 2 Task is clear, but no constraints or format is mentioned 4 Data accessible, offers instructions on how to download the data via CLI tools. No splits. 5 (by default) All questions in the dataset are multiple choice, all have a correct answer 1 There are over 300 models listed, but very few, if any, show performance on the dataset or list constraints 5 Explains all necessary information inside a paper 2.833 2025-01-24 Humanity's Last Exam Multidomain Broad cross-domain academic reasoning cross-domain, academic exam, multiple-choice, multidisciplinary Multiple choice Accuracy unkown <sup>4</sup> 4 Code for testing models posted on the github. Unknown how to run a custom model. 2 Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified 2 Data accessible through Hugging Face, but requires giving contact information to access 5 (by default) All questions in the dataset are multiple choice, all have a correct answer 2 Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result 5 Paper available with necessary information 3.333 2024-11-07 FrontierMath Mathematics Challenging advanced mathematical reasoning symbolic reasoning, number theory, algebraic geometry, category theory Problem solving Accuracy unkown <sup>5</sup> 0 No link to code provided 3 Well-specified process for asking questions and receiving answers. No software or hardware constraints 0 Paper and website had no link to any dataset. It may still exist somewhere 5 (by default) All questions in the dataset have a correct answer 2 Displays result of leading models on the benchmark, but none are trainable or list constraints 0 No specified way to reproduce the reference solution 1.667 2024-07-18 SciCode Scientific Programming Scientific code generation and problem solving code synthesis, scientific computing, programming benchmark Coding Solve rate (%) Claude3.5-Sonnet <sup>6</sup> 5 Code to run exists on github repo 4.5 Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints. 0 Paper and website had no link to any dataset. It may still exist somewhere 2 Metrics stated, but method of grading is not specified 1 Models presented with scores, but none are open or list constraints 4 Paper containing all needed info except for evlauation criteria 2.75 2025-03-13 AIME (American Invitational Mathematics Examination) Mathematics Pre-college advanced problem solving algebra, combinatorics, number theory, geometry Problem solving Accuracy unkown <sup>7</sup> 0 No code available 0 Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints 4 Easily accessible data with problems and solutions, but no splits 5 (by default) Answer is correct or it's not 0 Not given. Human performance stats exist, but no mentions of AI performance 0 Not given 1.5 2025-02-15 MATH-500 Mathematics Math reasoning generalization calculus, algebra, number theory, geometry Problem solving Accuracy unkown <sup>8</sup> 0 No code provided 0 No method of presentation and evaluation is not stated. No constraints 5 Problems and solutions are easily downloaded. Could not find a way to download the data 2 Problem spec states that all of the AI reasoning steps are subject to grading, but no specified way to evaluate the steps 0 Not given 0 Not given. Implicit instructions to download dataset. 1.167 2024-04-02 CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction) Multidomain Science Long-context scientific reasoning long-context, information extraction, multimodal Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension Accuracy unkown <sup>9</sup> 4 Code is available, but not well documented 1 Explains types of problems in detail, but does not state exactly how to administer them. 4 Dataset is available via Github, but hard to find 5 Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem. 1 Exists, but is not open 5 Associated paper explains all criteria 3.333 2023-01-26 FEABench (Finite Element Analysis Benchmark) Computational Engineering FEA simulation accuracy and performance finite element, simulation, PDE Simulation, Performance evaluation Solve time, Error norm FEniCS, deal.II <sup>10</sup> 4 Code is available, but poorly documented 1.5 Output is defined and task clarity is questionable 4 Available, but not split into sets 5 Fully defined metrics 4 Three open-source models were used. No system constraints. 5 In associated paper 3.917 2024-07-12 SPIQA (Scientific Paper Image Question Answering) Computer Science Multimodal QA on scientific figures multimodal QA, figure understanding, table comprehension, chain-of-thought Question answering, Multimodal QA, Chain-of-Thought evaluation Accuracy, F1 score Chain-of-Thought models, Multimodal QA systems <sup>11</sup> 0 Not provided 5 Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope. 4.5 Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization. 5 Uses quantitative metrics (Accuracy, F1) aligned with the task 2 Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all. 5 All information provided in paper 3.583 2020-09-28 MedQA Medical Question Answering Medical board exam QA USMLE, diagnostic QA, medical knowledge, multilingual Multiple choice Accuracy Neural reader, Retrieval-based QA systems <sup>12</sup> 5 All code available on the github 3 Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified. 4 Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria. 5 Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models. 0 No reference solution mentioned. 4 Paper is available. Evaluation criteria are not mentioned. 3.5 2025-05-13 BaisBench (Biological AI Scientist Benchmark) Computational Biology Omics-driven AI research tasks single-cell annotation, biological QA, autonomous discovery Cell type annotation, Multiple choice Annotation accuracy, QA accuracy LLM-based AI scientist agents <sup>13</sup> 5 Instructions for environment setup available 4 Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified. 5 Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards. 5 Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals. 0 Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet. 5 Dataset and paper accessible; IPYNB files for setup are available on the github repo. 4.0 2023-01-26 MOLGEN Computational Chemistry Molecular generation and optimization SELFIES, GAN, property optimization Distribution learning, Goal-oriented generation Validity%, Novelty%, QED, Docking score MolGen <sup>14</sup> 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0 This is a pre-trained model 0.0 2020-05-02 Open Graph Benchmark (OGB) - Biology Graph ML Biological graph property prediction node prediction, link prediction, graph classification Node property prediction, Link property prediction, Graph property prediction Accuracy, ROC-AUC GCN, GraphSAGE, GAT <sup>15</sup> 5 All necessary information is provided on the Github 4 Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined. No constraints. 5 Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included. 5 Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks. 3 Multiple baselines implemented and documented (GCN, GAT, GraphSAGE). No contraints. 5 All necessary information is included in a paper. 4.5 2011-10-01 Materials Project Materials Science DFT-based property prediction DFT, materials genome, high-throughput Property prediction MAE, R^2 Automatminer, Crystal Graph Neural Networks <sup>16</sup> 0 No instructions available 1.5 The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases. 3 API key required to access data. No predefined splits. 5 Uses numerical metrics like MAE and R^2 2 Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed. 0 No explanations or paper provided 1.917 2020-10-20 OCP (Open Catalyst Project) Chemistry; Materials Science Catalyst adsorption energy prediction DFT relaxations, adsorption energy, graph neural networks Energy prediction, Force prediction MAE (energy), MAE (force) CGCNN, SchNet, DimeNet++, GemNet-OC <sup>17</sup>, <sup>18</sup>, <sup>19</sup>, <sup>20</sup> 5 Data provided in Github links 5 Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy. 5 Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable. 5 MAE (energy and force) are standard and reproducible. 4 Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed. 1 Paper exists, but content is behind a paywall. 4.167 2023-06-20 JARVIS-Leaderboard Materials Science; Benchmarking Comparative evaluation of materials design methods leaderboards, materials methods, simulation Method benchmarking, Leaderboard ranking MAE, RMSE, Accuracy unkown <sup>21</sup> 1 Setup script provided, but no code provided 1 Only dataset format is defined. 4 Data is public and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks. No splits. 5 Metrics stated for each benchmark. 4 Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); no constraints specified. 1 Only the task is specified. 2.667 2022-02-22 Quantum Computing Benchmarks (QML) Quantum Computing Quantum algorithm performance evaluation quantum circuits, state preparation, error correction Circuit benchmarking, State classification Fidelity, Success probability IBM Q, IonQ, AQT@LBNL <sup>22</sup> 4 Run instructions exist, but are not easy to follow 3 No system constraints. Task clarity and dataset format are not clearly specified. 4 Datasets are accessible, but not split. 3 Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured. 0 Not provided 1 Only the task is defined. 2.5 2024-10-01 CFDBench (Fluid Dynamics) Fluid Dynamics; Scientific ML Neural operator surrogate modeling neural operators, CFD, FNO, DeepONet Surrogate modeling L2 error, MAE FNO, DeepONet, U-Net <sup>23</sup> 5 The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation 0 Not listed 0 Not given 5 Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives. 5 Baseline models like FNO and DeepONet are implemented, hardware specified. 5 Associated paper gives all necessary information. 3.333 2023-04-23 SatImgNet Remote Sensing Satellite imagery classification land-use, zero-shot, multi-task Image classification Accuracy CLIP, BLIP, ALBEF <sup>24</sup> 0 No scripts or environment information provided 4 Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation. 5 Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks. 5 Accuracy of classification is an appropriate metric 4 Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified 5 Paper provides all required information 3.833 2023-07-19 ClimateLearn Climate Science; Forecasting ML for weather and climate modeling medium-range forecasting, ERA5, data-driven Forecasting RMSE, Anomaly correlation CNN baselines, ResNet variants <sup>25</sup> 5 Quickstart notebook makes for easy usage 5 Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints. 5 Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant. 5 ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary. 0 The benchmark is geared for CNN architectures, but no specific model was mentioned. 5 Explained in the benchmark's paper. 4.167 2022-06-09 BIG-Bench (Beyond the Imitation Game Benchmark) NLP; AI Evaluation Diverse reasoning and generalization tasks few-shot, multi-task, bias analysis Few-shot evaluation, Multi-task evaluation Accuracy, Task-specific metrics GPT-3, Dense Transformers, Sparse Transformers <sup>26</sup> 4.5 Quick start notebook provided, but instructions on how to run it are lacking. 4.5 Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized. 5 Public, versioned, and well-documented; FAIR overall 5 Many tasks use standard quantitative metrics (accuracy, BLEU, F1). Others involve subjective ratings (e.g., Likert), which reduces cross-task comparability. 2 Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey. 5 Explained in the associated paper. 4.333 2019-11-20 CommonSenseQA NLP; Commonsense Commonsense question answering ConceptNet, multiple-choice, adversarial Multiple choice Accuracy BERT-large, RoBERTa, GPT-3 <sup>27</sup> 5 All code given on Github site 4 Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified. 5 Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries. 5 Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation. 4 Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not run with hardware constraints 5 Given in paper. 4.667 2019-07-24 Winogrande NLP; Commonsense Winograd Schema-style pronoun resolution adversarial, pronoun resolution Pronoun resolution Accuracy, AUC RoBERTa, BERT, GPT-2 <sup>28</sup> 0 No template code provided 5 Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included. 5 Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata. 5 Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations. 4 Baseline results available, requiring users to submit their methods along with their submissions. Constraints are not required in submissions. 5 Dataset page and paper provide sufficient detail 4.0 2024-05-01 Jet Classification Particle Physics Real-time classification of particle jets using HL-LHC simulation features classification, real-time ML, jet tagging, QKeras Classification Accuracy, AUC Keras DNN, QKeras quantized DNN <sup>29</sup> 3 Not containerized; Setup automation/documentation could be improved 4 System constraints missing 5 None 5 None 4 HW/SW requirements missing; Reference not bundled as official starter kit 4 Full reproducibility requires manual setup 4.167 2024-05-01 Irregular Sensor Data Compression Particle Physics Real-time compression of sparse sensor data with autoencoders compression, autoencoder, sparse data, irregular sampling Compression MSE, Compression ratio Autoencoder, Quantized autoencoder <sup>30</sup> 3 Not containerized; Full automation and documentation could be improved 4 Exact latency or resource constraints not numerically specified 5 All criteria met 5 All criteria met 4 Not fully documented or automated for reproducibility 4 Setup for deployment (e.g., FPGA pipeline) requires familiarity with tooling 4.167 2024-05-01 Beam Control Accelerators and Magnets Reinforcement learning control of accelerator beam position RL, beam stabilization, control systems, simulation Control Stability, Control loss DDPG, PPO (planned) <sup>31</sup>, <sup>32</sup> 1 Code not documented; Incomplete setup and not containerized 4 Latency/resource constraints not fully quantified 3 Not findable (no DOI/indexing); Not interoperable (format/schema unspecified) 5 All criteria met 2 HW/SW requirements missing; Metrics not evaluated with reference; Baseline not trainable/open 3 Setup instructions and pretrained model details are missing 3.0 2024-07-08 Ultrafast jet classification at the HL-LHC Particle Physics FPGA-optimized real-time jet origin classification at the HL-LHC jet classification, FPGA, quantization-aware training, Deep Sets, Interaction Networks Classification Accuracy, Latency, Resource utilization MLP, Deep Sets, Interaction Network <sup>33</sup> 3 Not containerized; Setup and automation incomplete 4 Hardware constraints are referenced but not fully detailed or standardized 4 FAIR metadata limited; no clear mention of dataset format or splits 3 Metrics exist (accuracy, latency, utilization), but formal definitions and evaluation guidance are limited 2 Reference implementations not fully reproducible; no evaluation pipeline or training setup provided 3 No linked GitHub repo or setup instructions; paper provides partial guidance only 3.167 2024-10-15 Quench detection Accelerators and Magnets Real-time detection of superconducting magnet quenches using ML quench detection, autoencoder, anomaly detection, real-time Anomaly detection, Quench localization ROC-AUC, Detection latency Autoencoder, RL agents (in development) <sup>34</sup> 1 Code not provided; no evidence of documentation or containerization 4 Real-time detection task is clearly described, but exact constraints, inputs/outputs, and evaluation protocol are only partially specified 2 Dataset URL is missing; FAIR principles largely unmet 3 ROC-AUC and latency are mentioned, but metric definitions and formal evaluation setup are missing 1 No baseline or reproducible model implementation available 2 Only a conference slide deck is available; lacks detailed instructions or repository for reproduction 2.167 2024-10-15 DUNE Particle Physics Real-time ML for DUNE DAQ time-series data DUNE, time-series, real-time, trigger Trigger selection, Time-series anomaly detection Detection efficiency, Latency CNN, LSTM (planned) <sup>35</sup> 1 Code not available; no containerization or setup provided 4 Constraints like latency thresholds are described qualitatively but not numerically defined 3 Dataset lacks a public URL; FAIR metadata and versioning are missing 4 Metrics are relevant but no benchmark baseline or detailed evaluation guidance is provided 2 Autoencoder prototype exists but is not reproducible; RL model still in development 3 Documentation exists only in slides/GDocs; no implementation guide or structured release 2.833 2025-01-08 Intelligent experiments through real-time AI Instrumentation and Detectors; Nuclear Physics; Particle Physics Real-time FPGA-based triggering and detector control for sPHENIX and future EIC FPGA, Graph Neural Network, hls4ml, real-time inference, detector control Trigger classification, Detector control, Real-time inference Accuracy (charm and beauty detection), Latency (micros), Resource utilization (LUT/FF/BRAM/DSP) Bipartite Graph Network with Set Transformers (BGN-ST), GarNet (edge-classifier) <sup>36</sup> 3 No containerized or open-source setup provided 4 Architectural/system specifications are incomplete 2 Dataset is internal and not publicly available or FAIR-compliant 3 Metrics relevant but not supported by evaluation scripts or baselines 3 No public or reproducible implementation released 3 No public GitHub or complete pipeline documentation 3.0 2025-01-09 Neural Architecture Codesign for Fast Physics Applications Physics; Materials Science; Particle Physics Automated neural architecture search and hardware-efficient model codesign for fast physics applications neural architecture search, FPGA deployment, quantization, pruning, hls4ml Classification, Peak finding Accuracy, Latency, Resource utilization NAC-based BraggNN, NAC-optimized Deep Sets (jet) <sup>37</sup> 3 Toolchain (hls4ml, nac-opt) described but not yet containerized or fully packaged 5 Fully specified task with constraints and target deployment; includes hardware context 2 Simulated datasets referenced but not publicly available or FAIR-compliant 5 Clear, quantitative metrics aligned with task goals and hardware evaluation 4 Models tested on hardware with source code references; full training pipeline not yet released 4 Detailed paper and tools described; open repo planned but not yet complete 3.833 2024-06-24 Smart Pixels for LHC Particle Physics; Instrumentation and Detectors On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors smart pixel, on-sensor inference, data reduction, trigger Image Classification, Data filtering Data rejection rate, Power per pixel 2-layer pixel NN <sup>38</sup> 2 No packaged code or setup scripts available; replication depends on hardware description and paper 5 None 2 No dataset links; not publicly hosted or FAIR-compliant 5 None 3 In-pixel 2-layer NN described and evaluated, but reproducibility and source files are not released 3 Paper contains detailed descriptions, but no repo or external guide for reproducing results 3.333 2023-10-03 HEDM (BraggNN) Material Science Fast Bragg peak analysis using deep learning in diffraction microscopy BraggNN, diffraction, peak finding, HEDM Peak detection Localization accuracy, Inference time BraggNN <sup>39</sup> 2 No standalone code repository or setup instructions provided 5 None 2 No dataset links or FAIR metadata; unclear public access 4 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts 3 BraggNN model is described and evaluated, but no direct implementation or inference scripts available 3 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline 3.167 2023-12-03 4D-STEM Material Science Real-time ML for scanning transmission electron microscopy 4D-STEM, electron microscopy, real-time, image processing Image Classification, Streamed data inference Classification accuracy, Throughput CNN models (prototype) <sup>40</sup> 2 No standalone code repository or setup instructions provided 5 None 2 No dataset links or FAIR metadata; unclear public access 4 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts 3 BraggNN model is described and evaluated, but no direct implementation or inference scripts available 3 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline 3.167 2023-12-05 In-Situ High-Speed Computer Vision Fusion/Plasma Real-time image classification for in-situ plasma diagnostics plasma, in-situ vision, real-time ML Image Classification Accuracy, FPS CNN <sup>41</sup> 1 No public implementation or containerized setup released 3 No standardized I/O, latency constraint, or complete framing 0 Dataset not provided or described in any formal way 2 Throughput and accuracy mentioned, but not defined or benchmarked 1 Prototype CNNs described; no code, baseline, or training details available 2 Some insight via papers, but no working repo, setup, or replication path 1.5 2020-01-01 BenchCouncil AIBench General End-to-end AI benchmarking across micro, component, and application levels benchmarking, AI systems, application-level evaluation Training, Inference, End-to-end AI workloads Throughput, Latency, Accuracy ResNet, BERT, GANs, Recommendation systems <sup>42</sup> 3 No containerized or automated implementation provided for full benchmark suite 4 Task coverage is broad and well-scoped, but system constraints and expected outputs are not uniformly defined 3 Multiple datasets are mentioned, but not consistently FAIR-documented, versioned, or linked 4 Metrics are appropriate, but standardization and reproducibility across tasks vary 3 Reference models (e.g., ResNet, BERT) described; no turnkey implementation or results repository for all levels 3 Paper is comprehensive, but minimal user-facing documentation or structured reproduction guide 3.333 2020-01-01 BenchCouncil BigDataBench General Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads big data, AI benchmarking, data analytics Data preprocessing, Inference, End-to-end data pipelines Data throughput, Latency, Accuracy CNN, LSTM, SVM, XGBoost <sup>43</sup> 3 No automated setup across all tasks; some components require manual integration. 4 Specific I/O formats and hardware constraints are not uniformly detailed across all tasks. 4 Some datasets lack consistent versioning or rich metadata annotations. 5 None 4 Not all benchmark components have fully reproducible baselines; deployment across platforms is fragmented. 4 Setup requires manual steps; some task-specific instructions lack clarity. 4.0 2021-10-20 MLPerf HPC Cosmology, Climate, Protein Structure, Catalysis Scientific ML training and inference on HPC systems HPC, training, inference, scientific ML Training, Inference Training time, Accuracy, GPU utilization CosmoFlow, DeepCAM, OpenCatalyst <sup>44</sup> 3 Reference implementations exist but containerization and environment setup require manual effort across HPC systems. 4 Hardware constraints and I/O formats are not fully defined for all scenarios. 5 Not all data is independently versioned or comes with standardized FAIR metadata. 5 None 4 Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled. 4 Central guidance is available but requires domain-specific effort to replicate results across systems. 4.167 2023-06-01 MLCommons Science Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD AI benchmarks for scientific applications including time-series, imaging, and simulation science AI, benchmark, MLCommons, HPC Time-series analysis, Image classification, Simulation surrogate modeling MAE, Accuracy, Speedup vs simulation CNN, GNN, Transformer <sup>45</sup> 5 Actively maintained GitHub repository available at https://github.com/mlcommons/science with implementations, scripts, and reproducibility support. 5 All five specification aspects are covered: system constraints, task, dataset format, benchmark inputs, and outputs. 5 Public scientific datasets are used with defined splits. At least 4 FAIR principles are followed. 5 Clearly defined metrics such as accuracy, training time, and GPU utilization are used. These metrics are explained and effectively capture solution performance. 5 A reference implementation is available, well-documented, trainable/open, and includes full metric evaluation and software/hardware details. 5 Thorough documentation exists covering the task, background, motivation, evaluation criteria, and includes a supporting paper. 5.0 2021-07-05 LHC New Physics Dataset Particle Physics; Real-time Triggering Real-time LHC event filtering for anomaly detection using proton collision data anomaly detection, proton collision, real-time inference, event filtering, unsupervised ML Anomaly detection, Event classification ROC-AUC, Detection efficiency Autoencoder, Variational autoencoder, Isolation forest <sup>46</sup> 3 While not formally evaluated in the previous version, Zenodo and paper links suggest available code for baseline models (e.g., autoencoders, GANs), though they are scattered and not unified in a single repository. 3 The task and context are clearly described, but system constraints and formal inputs/outputs are not fully specified. 5 Large-scale dataset hosted on Zenodo, publicly available, well-documented, with defined train/test structure. Appears to follow at least 4 FAIR principles. 4 Uses reasonable metrics (ROC-AUC, detection efficiency) that capture performance but lacks full explanation and standard evaluation tools. 2 Baselines are described across multiple papers but lack centralized, reproducible implementations and hardware/software setup details. 3 Some description in papers and dataset metadata exists, but lacks a unified guide, README, or training setup in a central location. 3.333 2023-07-17 MLCommons Medical AI Healthcare; Medical AI Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data medical AI, federated evaluation, privacy-preserving, fairness, healthcare benchmarks Federated evaluation, Model validation ROC AUC, Accuracy, Fairness metrics MedPerf-validated CNNs, GaNDLF workflows <sup>47</sup> 5 GitHub repository (https://github.com/mlcommons/medical) provides actively maintained open-source tools like MedPerf and GaNDLF for federated medical AI evaluation. 4 The platform defines federated tasks and model evaluation scenarios. Some clinical and system-level constraints are implied but not uniformly formalized across all use cases. 4 Multi-institutional datasets used in federated settings; real-world data is handled privately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit. 5 Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly support goals like generalizability and equity. 3 GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models are centrally documented or easily reproducible. 5 Extensive documentation, papers, and community support exist. Clear examples and usage instructions are provided in GitHub and publications. 4.333 2024-10-28 CaloChallenge 2022 LHC Calorimeter; Particle Physics Fast generative-model-based calorimeter shower simulation evaluation calorimeter simulation, generative models, surrogate modeling, LHC, fast simulation Surrogate modeling Histogram similarity, Classifier AUC, Generation latency VAE variants, GAN variants, Normalizing flows, Diffusion models <sup>48</sup> 4 Community GitHub repos and model implementations are available for the 31 submissions. While not fully unified in one place, the software is accessible and reproducible. 5 The task\u2014evaluating fast generative calorimeter simulations\u2014is clearly defined with benchmarking protocols, constraints like latency and model complexity, and structured evaluation criteria. 5 Four well-structured calorimeter datasets are provided, with different voxel resolutions, open access, signal/background separation, and metadata. FAIR principles are well covered. 5 Metrics like histogram similarity, classifier AUC, and generation latency are well defined and relevant for simulation quality, fidelity, and performance. 4 Several baselines (GANs, VAEs, flows, diffusion models) are documented and evaluated. Some are available via community repos, though not all are fully standardized or bundled. 4 Accompanied by a detailed paper and dataset description. Reproduction of pipelines may require additional setup or familiarity with the model submissions. 4.5 ongoing Papers With Code (SOTA Platform) General ML; All domains Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers leaderboard, benchmarking, reproducibility, open-source Multiple (Classification, Detection, NLP, etc.) Task-specific (Accuracy, F1, BLEU, etc.) All published models with code <sup>49</sup> 5 Actively maintained open-source platform (https://paperswithcode.com) under Apache 2.0 license; includes automatic integration with GitHub, datasets, and models for reproducibility. 4 Task and benchmark structures are well organized and standardized, but due to its broad coverage, input/output formats vary significantly between tasks and are not always tightly controlled. 3 Relies on external datasets submitted by the community. While links are available, FAIR compliance is not guaranteed or systematically enforced across all benchmarks. 5 Tracks state-of-the-art using task-specific metrics like Accuracy, F1, BLEU, etc., with consistent aggregation and historical SOTA tracking. 3 Provides links to implementations of many SOTA models, but no single unified reference baseline is required or maintained per benchmark. 4 Strong front-end documentation and metadata on benchmarks, tasks, and models; however, some benchmark-specific instructions are sparse or dependent on external paper links. 4.0 2022-01-01 Codabench General ML; Multiple Open-source platform for organizing reproducible AI benchmarks and competitions benchmark platform, code submission, competitions, meta-benchmark Multiple Submission count, Leaderboard ranking, Task-specific metrics Arbitrary code submissions <sup>50</sup> 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1 This is a platform for posting benchmarks, not a benchmark in itself. 1.0 2021-09-27 Sabath (SBI-FAIR) Systems; Metadata FAIR metadata framework for ML-driven surrogate workflows in HPC systems meta-benchmark, metadata, HPC, surrogate modeling Systems benchmarking Metadata completeness, FAIR compliance NA <sup>51</sup> 4 Actively maintained GitHub repository (https://github.com/icl-utk-edu/slip/tree/sabath) with BSD-licensed tooling for FAIR metadata capture; integrates with existing surrogate modeling benchmarks. 4 FAIR metadata structure and logging goals are clearly described. Input/output definitions are implied through integrations (e.g., MiniWeatherML), though not always formalized. 4 Datasets used in surrogate benchmarks are publicly available, well-structured, and FAIR-aligned, but not independently hosted by Sabath itself. 4 Emphasizes metadata completeness and FAIR compliance. Metrics are clear and well-matched to its metadata-focused benchmarking context. 3 Includes integration with multiple surrogate benchmarks and models, though not all are fully documented or packaged as standardized reference solutions. 3 Basic instructions and code are provided on GitHub, but more detailed walkthroughs, use-case examples, or tutorials are limited. 3.667 2022-10-13 PDEBench CFD; Weather Modeling Benchmark suite for ML-based surrogates solving time-dependent PDEs PDEs, CFD, scientific ML, surrogate modeling, NeurIPS Supervised Learning RMSE, boundary RMSE, Fourier RMSE FNO, U-Net, PINN, Gradient-Based inverse methods <sup>52</sup> 5 GitHub repository (https://github.com/pdebench/PDEBench) is actively maintained and includes training pipelines, data loaders, and evaluation scripts. Installation and usage are well-documented. 5 Clearly defined tasks for forward and inverse PDE problems, with structured input/output formats, system constraints, and task specifications. 5 Diverse PDE datasets (synthetic and real-world) hosted on DaRUS with DOIs. Datasets are well-documented, structured, and follow FAIR practices. 4 Includes RMSE, boundary RMSE, and Fourier-domain RMSE. These are well-suited to PDE problems, though rationale behind metric choices could be expanded in some cases. 4 Baselines (FNO, U-Net, PINN, etc.) are available and documented, but not every model includes full training and evaluation reproducibility out-of-the-box. 4 Strong documentation on GitHub including examples, configs, and usage instructions. Some model-specific details and tutorials could be further expanded. 4.5 2024-12-03 The Well biological systems, fluid dynamics, acoustic scattering, astrophysical MHD Foundation model + surrogate dataset spanning 16 physical simulation domains surrogate modeling, foundation model, physics simulations, spatiotemporal dynamics Supervised Learning Dataset size, Domain breadth FNO baselines, U-Net baselines <sup>53</sup> 5 BSD-licensed software and unified API are available via GitHub and PyPI. Supports loading and manipulating large HDF5 datasets across 16 domains. 4 The benchmark includes clearly defined surrogate modeling tasks, data structure, and metadata. However, constraints and formal task specs vary slightly across domains. 5 15 TB of ML-ready HDF5 datasets across 16 physics domains. Public, well-structured, richly annotated, and designed with FAIR principles in mind. 3 Domain breadth and dataset size are emphasized. Standardized quantitative metrics for model evaluation (e.g., RMSE, accuracy) are not uniformly applied across all domains. 3 Includes FNO and U-Net baselines, but does not yet provide fully trained, reproducible models or scripts across all datasets. 4 The GitHub repo and NeurIPS paper provide detailed guidance on dataset use, structure, and training setup. Tutorials and walkthroughs could be expanded further. 4.0 2024-10-31 LLM-Inference-Bench LLM; HPC/inference Hardware performance benchmarking of LLMs on AI accelerators LLM, inference benchmarking, GPU, accelerator, throughput Inference Benchmarking Token throughput (tok/s), Latency, Framework-hardware mix performance LLaMA-2-7B, LLaMA-2-70B, Mistral-7B, Qwen-7B <sup>54</sup> 5 Public GitHub repository (https://github.com/argonne-lcf/LLM-Inference-Bench) under BSD-3 license. Includes scripts, configurations, and dashboards for running and visualizing LLM inference benchmarks across multiple accelerator platforms. 5 Benchmark scope, models, accelerator targets, and supported frameworks are clearly specified. Input configurations and output metrics are standardized across hardware types. 2 No novel dataset is introduced; benchmark relies on pre-trained LLMs and synthetic inference inputs. Dataset structure and FAIR considerations are minimal. 5 Hardware-specific metrics (token throughput, latency, utilization) are well-defined, consistently measured, and aggregated in dashboards. 3 Inference configurations and baseline performance results are provided, but there are no full reference training pipelines or model implementations. 4 GitHub repo provides clear usage instructions, setup guides, and interactive dashboard tooling. Some areas like benchmarking extensions or advanced tuning are less detailed. 4.0 2023-12-12 SGLang Framework LLM Vision Fast serving framework for LLMs and vision-language models LLM serving, vision-language, RadixAttention, performance, JSON decoding Model serving framework Tokens/sec, Time-to-first-token, Throughput gain vs baseline LLaVA, DeepSeek, Llama <sup>55</sup> 5 Actively maintained and production-deployed (e.g., xAI, NVIDIA); source code available under Apache 2.0. Includes efficient backends (RadixAttention, quantization, batching) and full serving infrastructure. 4 The framework clearly defines performance targets, serving logic, and model integration. Input/output expectations are consistent, but not all benchmarks are standardized. 2 Does not introduce new datasets; instead, it evaluates performance using existing model benchmarks. Only configuration files are included. 5 Serving-related metrics such as tokens/sec, time-to-first-token, and throughput gain vs. baselines are well-defined and consistently applied. 3 Provides benchmark configs and example integrations (e.g., with LLaVA, DeepSeek), but not all models or scripts are runnable out-of-the-box. 4 Strong GitHub documentation, install guides, and benchmarks. Some advanced topics (e.g., scaling, hardware tuning) could use deeper walkthroughs. 3.833 2023-09-12 vLLM Inference and Serving Engine LLM; HPC/inference High-throughput, memory-efficient inference and serving engine for LLMs LLM inference, PagedAttention, CUDA graph, streaming API, quantization Inference Benchmarking Tokens/sec, Time to First Token (TTFT), Memory footprint LLaMA, Mixtral, FlashAttention-based models <sup>56</sup> 5 Actively maintained open-source project under Apache 2.0. GitHub repo includes full serving engine, benchmarking scripts, CUDA integration, and deployment examples. 5 Inference benchmarks are well-defined with clear input/output formats and platform-specific constraints. Covers multiple models, hardware backends, and batching configurations. 3 No traditional dataset is included. Instead, it uses structured configs and logs suitable for inference benchmarking. FAIR principles are only partially applicable. 5 Comprehensive performance metrics like tokens/sec, time-to-first-token (TTFT), and memory footprint are consistently applied and benchmarked across frameworks. 4 Provides runnable scripts and configs for several models (LLaMA, Mixtral, etc.) across platforms. Baselines are reproducible, though not all models are fully wrapped or hosted. 4 Well-structured GitHub documentation with setup instructions, config examples, benchmarking comparisons, and performance tuning guides. 4.333 2022-06-22 vLLM Performance Dashboard LLM; HPC/inference Interactive dashboard showing inference performance of vLLM Dashboard, Throughput visualization, Latency analysis, Metric tracking Performance visualization Tokens/sec, TTFT, Memory usage LLaMA-2, Mistral, Qwen <sup>57</sup> 4 Interactive dashboard built with ObservableHQ and linked to vLLM benchmarks. Source code is not fully open, but backend integration with vLLM is well-maintained. 4 While primarily a visualization tool, it includes benchmark configurations, metric definitions, and supports comparison across models and hardware. 2 No datasets are bundled; the dashboard visualizes metrics derived from model inference logs or external endpoints, not a formal dataset. 4 Tracks tokens/sec, TTFT, memory usage, and platform comparisons. Metrics are clear but focused on visualization rather than statistical robustness. 3 Dashboards include reproducible views of benchmarked models, but do not ship with runnable model code. Relies on external serving infrastructure. 4 Public dashboard with instructions and tooltips; documentation is clear, though access is restricted (login required) and backend setup is opaque to users. 3.5 2022-04-01 Nixtla NeuralForecast Time-series forecasting; General ML High-performance neural forecasting library with &gt;30 models time-series, neural forecasting, NBEATS, NHITS, TFT, probabilistic forecasting, usability Time-series forecasting RMSE, MAPE, CRPS NBEATS, NHITS, TFT, DeepAR <sup>58</sup> 5 Actively maintained open-source library under Apache 2.0. Offers a clean API, extensive model zoo (&gt;30 models), integration with Ray, Optuna, and supports scalable training and inference workflows. 5 Forecasting task is well-defined with clear input/output structures. Framework supports probabilistic and deterministic forecasting, with unified interfaces and support for batch evaluation. 3 NeuralForecast does not include its own datasets but supports standard datasets (e.g., M4, M5, ETT). FAIR compliance depends on user-supplied data. 5 RMSE, MAPE, CRPS, and other domain-relevant metrics are well supported and integrated into the evaluation loop. 4 Includes runnable model baselines and training scripts for all supported models. Some models have pretrained weights, but not all are fully benchmarked out-of-the-box. 5 Rich documentation with examples, API references, tutorials, notebooks, and CLI support. PyPI, GitHub, and official blog posts offer clear guidance for usage and extension. 4.5 2023-06-01 Nixtla Neural Forecast NHITS Time-series; General ML Official NHITS implementation for long-horizon time series forecasting NHITS, long-horizon forecasting, neural interpolation, time-series Time-series forecasting RMSE, MAPE NHITS <sup>59</sup> 5 Implemented within the open-source NeuralForecast library under Apache 2.0. Includes training, evaluation, and hyperparameter tuning pipelines. Actively maintained. 5 The NHITS forecasting task is clearly defined with structured input/output formats. Model design targets long-horizon accuracy and compute efficiency. 3 Uses standard benchmark datasets like M4, but does not bundle them directly. FAIR compliance depends on external dataset sources and user setup. 5 Evaluated using RMSE, MAPE, and other standard forecasting metrics, integrated into training and evaluation APIs. 4 Official NHITS implementation is fully reproducible with training/eval configs, though pretrained weights are not always provided. 4 Well-documented on GitHub and in AAAI paper, with code examples, training guidance, and usage tutorials. More model-specific docs could improve clarity further. 4.333 2023-10-03 Nixtla Neural Forecast TimeLLM Time-series; General ML Reprogramming LLMs for time series forecasting Time-LLM, language model, time-series, reprogramming Time-series forecasting RMSE, MAPE Time-LLM <sup>60</sup> 4 Fully open-source under Apache 2.0, integrated into the NeuralForecast library. Includes Time-LLM implementation with example usage and training scripts. 3 High-level framing of forecasting as language modeling is clear, but detailed input/output specifications, constraints, and task formalization are minimal. 3 Evaluated on standard datasets like M4 and ETT, but dataset splits and versioning are not bundled or explicitly FAIR-compliant. 4 Standard forecasting metrics such as RMSE, MAPE, and SMAPE are reported. Evaluation is consistent, though deeper metric justification is limited. 3 Time-LLM implementation is open and reproducible, but limited baselines or comparative implementations are included directly. 3 GitHub README provides installation and quick usage examples, but lacks detailed API docs, training walkthroughs, or extended tutorials. 3.333 2023-10-05 Nixtla Neural Forecast TimeGPT Time-series; General ML Time-series foundation model \"TimeGPT\" for forecasting and anomaly detection TimeGPT, foundation model, time-series, generative model Time-series forecasting, Anomaly detection RMSE, Anomaly detection metrics TimeGPT <sup>61</sup> 4 Fully open-source Apache 2.0 implementation integrated in NeuralForecast, supporting training and evaluation via API. Production-grade deployment available via Nixtla API and Azure. 3 Concept and forecasting goals are described, but formal input/output definitions and task constraints are not rigorously specified. 3 Evaluated on existing open datasets, but consolidated data release, splits, and FAIR metadata are not provided. 4 Uses standard forecasting metrics such as RMSE, MASE, SMAPE, and anomaly detection metrics consistently across evaluations. 3 TimeGPT implementation is available, but baseline comparisons and additional reference models are limited. 3 Basic README with installation and usage examples; more detailed API docs and tutorials would improve usability. 3.333 2025-03-03 HDR ML Anomaly Challenge (Gravitational Waves) Astrophysics; Time-series Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets anomaly detection, gravitational waves, astrophysics, time-series Anomaly detection ROC-AUC, Precision/Recall Deep latent CNNs, Autoencoders <sup>62</sup> 4 Benchmark platform provided on Codabench with starter kits and submission infrastructure. Code and baseline models are publicly accessible but not extensively maintained beyond the challenge. 4 Well-defined anomaly detection task on gravitational-wave time series with clear input/output expectations and challenge constraints. 5 Uses preprocessed LIGO/Virgo time series data at 4096 Hz, publicly available and standard in astrophysics. 4 ROC-AUC, precision, and recall metrics are clearly specified and appropriate for anomaly detection. 4 Baseline deep latent CNNs and autoencoders are provided and reproducible, but not extensively documented. 4 Documentation includes challenge instructions, starter kit details, and baseline descriptions, but could benefit from more thorough tutorials and code walkthroughs. 4.167 2025-03-03 HDR ML Anomaly Challenge (Butterfly) Genomics; Image/CV Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset anomaly detection, computer vision, genomics, butterfly hybrids Anomaly detection Classification accuracy, F1 score CNN-based detectors <sup>63</sup> 3 Codabench platform provides submission infrastructure but no fully maintained code repository or reproducible baseline implementations. 4 Task is clearly described with domain-specific anomaly detection objectives and relevant physics motivation. 3 Dataset consists of real detector data with synthetic anomaly injections; access is restricted and requires NDA, limiting openness and FAIR compliance. 3 Standard metrics (ROC, F1, precision) are used; evaluation protocols are clear but not deeply elaborated. 2 Baselines are partially described but lack public code or reproducible execution scripts. 3 Challenge website provides basic descriptions and evaluation metrics but lacks comprehensive tutorials or example workflows. 3.0 2025-03-03 HDR ML Anomaly Challenge (Sea Level Rise) Climate Science; Time-series, Image/CV Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery anomaly detection, climate science, sea-level rise, time-series, remote sensing Anomaly detection ROC-AUC, Precision/Recall CNNs, RNNs, Transformers <sup>64</sup> 2 Benchmark platform exists on Codabench, but no baseline code or maintained repository for reference solutions provided yet. 5 Well-defined anomaly detection task combining satellite imagery and time-series data, with clear physical and domain-specific framing. 5 Uses preprocessed, public, and well-structured sensor and satellite data for the North Atlantic sea-level rise region. 5 Standard metrics such as ROC-AUC, precision, and recall are specified and suitable for the anomaly detection tasks. 1 No starter models or baseline implementations linked or provided publicly. 5 Challenge page, starter kits, and related papers offer strong guidance for participants. 3.833 2025-01-24 Single Qubit Readout on QICK System Quantum Computing Real-time single-qubit state classification using FPGA firmware qubit readout, hls4ml, FPGA, QICK Classification Accuracy, Latency hls4ml quantized NN <sup>65</sup> 3 Code and FPGA firmware available on GitHub; integration with hls4ml demonstrated. Some deployment details and examples are provided but overall software maturity is moderate. 4 Task clearly defined: real-time single-qubit state classification with latency and fidelity constraints. Labeling and ground truth definitions could be more explicit. 4 Dataset hosted on Zenodo with structured data; however, detailed documentation on image acquisition and labeling pipeline is limited. 5 Standard classification metrics (accuracy, latency) are used and directly relevant to the quantum readout task. 1 No baseline or starter models with runnable code are linked publicly. 4 Codabench task page and GitHub repo provide descriptions and usage instructions, but detailed API or deployment tutorials are limited. 3.5 2023-11-20 GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark Science (Biology, Physics, Chemistry) Graduate-level, expert-validated multiple-choice questions hard even with web access Google-proof, multiple-choice, expert reasoning, science QA Multiple choice Accuracy GPT-4 baseline <sup>66</sup> 3 Dataset and benchmark materials are publicly available via HuggingFace and GitHub, but no integrated runnable code or software framework is provided. 5 Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning. Input/output formats and evaluation criteria are well described. 5 The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits. 5 Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA. 1 No baseline implementations or starter code are linked or provided for reproduction. 3 Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines. 3.667 2024-12-13 SeafloorAI Marine Science; Vision-Language Large-scale vision-language dataset for seafloor mapping and geological classification sonar imagery, vision-language, seafloor mapping, segmentation, QA Image segmentation, Vision-language QA Segmentation pixel accuracy, QA accuracy SegFormer, ViLT-style multimodal models <sup>67</sup> 3 Data processing code is publicly available, but no full benchmark framework or runnable model implementations are provided yet. 5 Tasks (image segmentation and vision-language QA) are clearly defined with geospatial and multimodal objectives well specified. 5 Large-scale, well-annotated sonar imagery dataset with segmentation masks and natural language descriptions; curated with domain experts. 5 Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified and appropriate for the tasks. 4 Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but reproducible code or pretrained weights are not fully available yet. 4 Dataset description and data processing instructions are provided, but tutorials and benchmark usage guides are limited. 4.333 2024-12-13 SuperCon3D Materials Science; Superconductivity Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures superconductivity, crystal structures, equivariant GNN, generative models Regression (Tc prediction), Generative modeling MAE (Tc), Validity of generated structures SODNet, DiffCSP-SC <sup>68</sup> 3 Baseline models (SODNet, DiffCSP-SC) are described in the paper; however, fully reproducible code and pretrained models are not publicly available yet. 5 Tasks for regression (Tc prediction) and generative modeling with clear input/output structures and domain constraints are well defined. 5 Dataset contains 3D crystal structures and associated properties; well-curated but not fully released publicly at this time. 4 Metrics such as MAE for Tc prediction and validity checks for generated structures are appropriate and clearly described. 4 Paper provides model architecture details and some training insights, but no complete open-source reference implementations yet. 4 Paper and GitHub provide good metadata and data processing descriptions; tutorials and user guides could be expanded. 4.167 2024-12-13 GeSS Scientific ML; Geometric Deep Learning Benchmark suite evaluating geometric deep learning models under real-world distribution shifts geometric deep learning, distribution shift, OOD robustness, scientific applications Classification, Regression Accuracy, RMSE, OOD robustness delta GCN, EGNN, DimeNet++ <sup>69</sup> 3 Reference code expected post-conference; current public software availability limited. Benchmark infrastructure partially described but not fully released yet. 5 Benchmark clearly defines OOD robustness scenarios with classification and regression tasks in scientific domains, though no explicit hardware constraints are given. 5 Curated datasets of 3D crystal structures and material properties are included and publicly available for reproducible research. 5 Uses well-established metrics such as MAE and structural validity for materials modeling, plus accuracy and OOD robustness deltas. 4 Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected to be released soon. 4 Paper and poster provide solid explanation of benchmarks and scientific motivation; more extensive user documentation forthcoming. 4.333 2024-12-13 Vocal Call Locator (VCL) Neuroscience; Bioacoustics Benchmarking sound-source localization of rodent vocalizations from multi-channel audio source localization, bioacoustics, time-series, SSL Sound source localization Localization error (cm), Recall/Precision CNN-based SSL models <sup>70</sup> 3 Some baseline CNN models for sound source localization are reported, but no publicly available or fully integrated runnable codebase yet. 5 Well-defined localization tasks with multiple scenarios and real-world environment conditions; input/output formats clearly described. 4 Large-scale audio dataset covering real and simulated data with standardized splits, though exact data formats are not fully detailed. 5 Includes localization error, precision, recall, and other relevant metrics for robust evaluation. 5 Multiple baselines evaluated over diverse models and architectures, supporting reproducibility of benchmark comparisons. 1 Methodology and paper are thorough, but setup instructions and runnable code are not publicly provided, limiting user onboarding. 3.833 2024-12-13 MassSpecGym Cheminformatics; Molecular Discovery Benchmark suite for discovery and identification of molecules via MS/MS mass spectrometry, molecular structure, de novo generation, retrieval, dataset De novo generation, Retrieval, Simulation Structure accuracy, Retrieval precision, Simulation MSE Graph-based generative models, Retrieval baselines <sup>71</sup> 3 Open-source GitHub repository available; baseline models and training code partially provided but overall framework maturity is moderate. 5 Clearly defined tasks including molecule generation, retrieval, and spectrum simulation, scoped for MS/MS molecular identification. 5 Largest public MS/MS dataset with extensive annotations; minor point deducted for lack of explicit train/validation/test splits. 5 Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE used consistently. 3.5 CNN-based baselines are referenced, but pretrained weights and comprehensive training pipelines are not fully documented. 1 Paper and poster describe benchmark goals and design, but documentation and user guides are minimal and repo status uncertain. 3.75 2024-12-13 Urban Data Layer (UDL) Urban Computing; Data Engineering Unified data pipeline for multi-modal urban science research data pipeline, urban science, multi-modal, benchmark Prediction, Classification Task-specific accuracy or RMSE Baseline regression/classification pipelines <sup>72</sup> 3 Source code is publicly available on GitHub; baseline regression and classification pipelines are included but framework maturity is moderate. 5 Multiple urban science tasks like prediction and classification are well specified with clear input/output and evaluation criteria. 5 Large, multi-modal urban datasets are open-source, well-documented, and support reproducible research. 5 Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification. 4 Baseline models available but not exhaustive; community adoption and extensions expected. 5 GitHub repository and conference poster provide comprehensive code and reproducibility instructions. 4.5 2024-12-13 Delta Squared-DFT Computational Chemistry; Materials Science Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies density functional theory, Delta Squared-ML correction, reaction energetics, quantum chemistry Regression Mean Absolute Error (eV), Energy ranking accuracy Delta Squared-ML correction networks, Kernel ridge regression <sup>73</sup> 3 Source code and baseline models available for ML correction to DFT; framework maturity is moderate. 4 Benchmark focuses on reaction energy prediction with clear goals, though some task specifics could be formalized further. 4.5 Multi-modal quantum chemistry datasets are standardized and accessible; repository available. 4 Uses standard regression metrics like MAE and energy ranking accuracy; appropriate for task. 3.5 Includes baseline regression and kernel ridge models; implementations are reproducible. 4 Source code supports pipeline reuse, but formal evaluation splits may vary. 3.833 2024-12-13 LLMs for Crop Science Agricultural Science; NLP Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts crop science, prompt engineering, domain adaptation, question answering Question Answering, Inference Accuracy, F1 score GPT-4, LLaMA-2-13B, T5-XXL <sup>74</sup> 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0 This is a model, not a benchmark. 0.0 2024-12-13 SPIQA (LLM) Multimodal Scientific QA; Computer Vision Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance) multimodal QA, scientific figures, image+text, chain-of-thought prompting Multimodal QA Accuracy, F1 score LLaVA, MiniGPT-4, Owl-LLM adapter variants <sup>75</sup> 5 Well-documented codebase available on Github 3.5 Task of QA over scientific figures is sufficient but not fully formalized in input/output terms. No hawrdware constraints. 5 Full dataset available on Hugging Face with train/test/valid splits. 4 Reports accuracy and F1; fair but no visual reasoning-specific metric. 4 10 LLM adapter baselines; results included without constraints. 5 Full paper available 4.417 <ol> <li> <p>Dan Hendrycks, Collin Burns, and Saurav Kadavath. Measuring massive multitask language understanding. 2021. URL: https://arxiv.org/abs/2009.03300.\u00a0\u21a9</p> </li> <li> <p>David Rein, Betty Li Hou, and Asa Cooper Stickland. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022.\u00a0\u21a9</p> </li> <li> <p>Peter Clark, Isaac Cowhey, and Oren Etzioni. Think you have solved question answering? try arc, the ai2 reasoning challenge. In EMNLP 2018, 237 248. 2018. URL: https://allenai.org/data/arc.\u00a0\u21a9</p> </li> <li> <p>Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, S\u00f8ren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, G\u00f6zdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Br\u00fcssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Mart\u00ed Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, V\u00e1clav Rozho\u0148, Vincent Ginis, Christian Stump, Niv Cohen, Rafa\u0142 Po\u015bwiata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givr\u00e9, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar \u00c4ngquist, Yanxu Chen, Harrison K Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, J\u00e9r\u00e9my Andr\u00e9oletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Kh\u00e1nh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Bir\u00f3 B\u00e1lint, Eve J. Y. Lo, Jiaqi Wang, Maria In\u00eas S. Nunes, Jeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciob\u00e2c\u0103, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekstr\u00f6m, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Pe\u00f1aflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yi\u011fit Yalin, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Bosc\u00e1, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle H\u00e4ggstr\u00f6m, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hern\u00e1ndez-C\u00e1mara, Emanuele Rodol\u00e0, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro Jos\u00e9 Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Ra\u00fal Adri\u00e1n Huerta Rodr\u00edguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benj\u00e1min Borb\u00e1s, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran \u0110uc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub \u0141ucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels M\u00fcndler, S\u00f6ren M\u00f6ller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, M\u00e1ty\u00e1s Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P V, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubi\u0107, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vil\u00e9m Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Micka\u00ebl Noy\u00e9, Micha\u0142 Pere\u0142kiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, D\u00e1niel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Gin\u00e9s, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han L\u00f9, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Bria\u0144ski, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanovi\u0107, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Ga\u00ebl Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, K\u00e1roly Zsolnai-Feh\u00e9r, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gon\u00e7alves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Y\u00fccel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity's last exam. 2025. URL: https://arxiv.org/abs/2501.14249, arXiv:2501.14249.\u00a0\u21a9</p> </li> <li> <p>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli J\u00e4rviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: a benchmark for evaluating advanced mathematical reasoning in ai. 2024. URL: https://arxiv.org/abs/2411.04872, arXiv:2411.04872.\u00a0\u21a9</p> </li> <li> <p>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: a research coding benchmark curated by scientists. 2024. URL: https://arxiv.org/abs/2407.13168, arXiv:2407.13168.\u00a0\u21a9</p> </li> <li> <p>TBD. Aime. March 2025. [Online accessed 2025-06-24]. URL: https://www.vals.ai/benchmarks/aime-2025-03-13.\u00a0\u21a9</p> </li> <li> <p>HuggingFaceH4. Math-500. 2025. URL: https://huggingface.co/datasets/HuggingFaceH4/MATH-500.\u00a0\u21a9</p> </li> <li> <p>Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, and Subhashini Venugopalan. Curie: evaluating llms on multitask scientific long context understanding and reasoning. 2025. URL: https://arxiv.org/abs/2503.13517, arXiv:2503.13517.\u00a0\u21a9</p> </li> <li> <p>Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, and Peter Norgaard. Feabench: evaluating language models on multiphysics reasoning ability. 2025. URL: https://arxiv.org/abs/2504.06260, arXiv:2504.06260.\u00a0\u21a9</p> </li> <li> <p>Xiaoyan Zhong, Yijian Gao, and Suchin Gururangan. Spiqa: scientific paper image question answering. 2024. URL: https://arxiv.org/abs/2407.09413.\u00a0\u21a9</p> </li> <li> <p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. 2020. URL: https://arxiv.org/abs/2009.13081, arXiv:2009.13081.\u00a0\u21a9</p> </li> <li> <p>Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu, Lei Wei, and Xuegong Zhang. Benchmarking ai scientists in omics data-driven biological research. 2025. URL: https://arxiv.org/abs/2505.08341, arXiv:2505.08341.\u00a0\u21a9</p> </li> <li> <p>Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen. Domain-agnostic molecular generation with chemical feedback. 2024. URL: https://arxiv.org/abs/2301.11259, arXiv:2301.11259.\u00a0\u21a9</p> </li> <li> <p>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: datasets for machine learning on graphs. 2021. URL: https://arxiv.org/abs/2005.00687, arXiv:2005.00687.\u00a0\u21a9</p> </li> <li> <p>Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin A. Persson. The materials project: a materials genome approach. APL Materials, 2013. URL: https://materialsproject.org/, doi:10.1063/1.4812323.\u00a0\u21a9</p> </li> <li> <p>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. The open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059\u20136072, 2021. URL: https://pubs.acs.org/doi/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.\u00a0\u21a9</p> </li> <li> <p>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066\u20133084, 2023. URL: https://pubs.acs.org/doi/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.\u00a0\u21a9</p> </li> <li> <p>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059 6072, 2021. URL: https://doi.org/10.1021/acscatal.0c04525, arXiv:https://doi.org/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.\u00a0\u21a9</p> </li> <li> <p>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066 3084, February 2023. URL: http://dx.doi.org/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.\u00a0\u21a9</p> </li> <li> <p>Kamal Choudhary, Daniel Wines, Kangming Li, Kevin F. Garrity, Vishu Gupta, Aldo H. Romero, Jaron T. Krogel, Kayahan Saritas, Addis Fuhr, Panchapakesan Ganesh, Paul R. C. Kent, Keqiang Yan, Yuchao Lin, Shuiwang Ji, Ben Blaiszik, Patrick Reiser, Pascal Friederich, Ankit Agrawal, Pratyush Tiwary, Eric Beyerle, Peter Minch, Trevor D. Rhone, Ichiro Takeuchi, Robert B. Wexler, Arun Mannodi-Kanakkithodi, Elif Ertekin, Avanish Mishra, Nithin Mathew, Mitchell Wood, Andrew D. Rohskopf, Jason Hattrick-Simpers, Shih-Han Wang, Luke E. K. Achenie, Hongliang Xin, Maureen Williams, Adam J. Biacchi, and Francesca Tavazza. JARVIS-Leaderboard: a large scale benchmark of materials design methods. npj Computational Materials, 10(1):93, 2024. URL: https://doi.org/10.1038/s41524-024-01259-w, doi:10.1038/s41524-024-01259-w.\u00a0\u21a9</p> </li> <li> <p>Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofr\u00edo, Johannes Klepsch, and Andre Luckow. Application-oriented benchmarking of quantum generative learning using quark. In 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), 475 484. IEEE, September 2023. URL: http://dx.doi.org/10.1109/QCE57702.2023.00061, doi:10.1109/qce57702.2023.00061.\u00a0\u21a9</p> </li> <li> <p>Yining Luo, Yingfa Chen, and Zhen Zhang. Cfdbench: a large-scale benchmark for machine learning methods in fluid dynamics. 2024. URL: https://arxiv.org/abs/2310.05963.\u00a0\u21a9</p> </li> <li> <p>Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: a multi-task metadataset for classifying satellite imagery using vision-language models. 2023. URL: https://huggingface.co/datasets/saral-ai/satimagnet.\u00a0\u21a9</p> </li> <li> <p>Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, and Aditya Grover. Climatelearn: benchmarking machine learning for weather and climate modeling. 2023. URL: https://arxiv.org/abs/2307.01909, arXiv:2307.01909.\u00a0\u21a9</p> </li> <li> <p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u015f, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed Gonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco\u0144, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col\u00f3n, Luke Metz, L\u00fctfi Kerem \u015eenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ram\u00edrez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142 Sw\u0119drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. 2023. URL: https://arxiv.org/abs/2206.04615, arXiv:2206.04615.\u00a0\u21a9</p> </li> <li> <p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: a question answering challenge targeting commonsense knowledge. 2019. URL: https://arxiv.org/abs/1811.00937, arXiv:1811.00937.\u00a0\u21a9</p> </li> <li> <p>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. 2019. URL: https://arxiv.org/abs/1907.10641, arXiv:1907.10641.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.\u00a0\u21a9</p> </li> <li> <p>Diana Kafkes and Jason St. John. Boostr: a dataset for accelerator control systems. 2021. URL: https://arxiv.org/abs/2101.08359, arXiv:2101.08359.\u00a0\u21a9</p> </li> <li> <p>Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, and Thea K. Aarrestad. Ultrafast jet classification on fpgas for the hl-lhc. 2024. URL: https://arxiv.org/abs/2402.01876, arXiv:2402.01876, doi:https://doi.org/10.1088/2632-2153/ad5f10.\u00a0\u21a9</p> </li> <li> <p>Maira Khan, Steve Krave, Vittorio Marinozzi, Jennifer Ngadiuba, Stoyan Stoynev, and Nhan Tran. Benchmarking and interpreting real time quench detection algorithms. In Fast Machine Learning for Science Conference 2024. Purdue University, IN, October 2024. indico.cern.ch. URL: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf.\u00a0\u21a9</p> </li> <li> <p>A. Abed Abud, B. Abi, R. Acciarri, M. A. Acero, G. Adamov, D. Adams, M. Adinolfi, A. Aduszkiewicz, Z. Ahmad, J. Ahmed, T. Alion, S. Alonso Monsalve, M. Alrashed, C. Alt, A. Alton, P. Amedo, J. Anderson, C. Andreopoulos, M. P. Andrews, F. Andrianala, S. Andringa, N. Anfimov, A. Ankowski, M. Antonova, S. Antusch, A. Aranda-Fernandez, A. Ariga, L. O. Arnold, M. A. Arroyave, J. Asaadi, A. Aurisano, V. Aushev, D. Autiero, M. Ayala-Torres, F. Azfar, H. Back, J. J. Back, C. Backhouse, P. Baesso, I. Bagaturia, L. Bagby, S. Balasubramanian, P. Baldi, B. Baller, B. Bambah, F. Barao, G. Barenboim, G. J. Barker, W. Barkhouse, C. Barnes, G. Barr, J. Barranco Monarca, N. Barros, J. L. Barrow, A. Basharina-Freshville, A. Bashyal, V. Basque, E. Belchior, J. B. R. Battat, F. Battisti, F. Bay, J. L. Bazo Alba, J. F. Beacom, E. Bechetoille, B. Behera, L. Bellantoni, G. Bellettini, V. Bellini, O. Beltramello, D. Belver, N. Benekos, F. Bento Neves, S. Berkman, P. Bernardini, R. M. Berner, H. Berns, S. Bertolucci, M. Betancourt, A. Betancur Rodr\u00edguez, M. Bhattacharjee, S. Bhuller, B. Bhuyan, S. Biagi, J. Bian, M. Biassoni, K. Biery, B. Bilki, M. Bishai, A. Bitadze, A. Blake, F. D. M. Blaszczyk, G. C. Blazey, E. Blucher, J. Boissevain, S. Bolognesi, T. Bolton, L. Bomben, M. Bonesini, M. Bongrand, F. Bonini, A. Booth, C. Booth, S. Bordoni, A. Borkum, T. Boschi, N. Bostan, P. Bour, C. Bourgeois, S. B. Boyd, D. Boyden, J. Bracinik, D. Braga, D. Brailsford, A. Brandt, J. Bremer, C. Brew, E. Brianne, S. J. Brice, C. Brizzolari, C. Bromberg, G. Brooijmans, J. Brooke, A. Bross, G. Brunetti, M. Brunetti, N. Buchanan, H. Budd, D. Caiulo, P. Calafiura, J. Calcutt, M. Calin, S. Calvez, E. Calvo, A. Caminata, M. Campanelli, K. Cankocak, D. Caratelli, G. Carini, B. Carlus, P. Carniti, I. Caro Terrazas, H. Carranza, T. Carroll, J. F. Casta\u00f1o Forero, A. Castillo, C. Castromonte, E. Catano-Mur, C. Cattadori, F. Cavalier, F. Cavanna, S. Centro, G. Cerati, A. Cervelli, A. Cervera Villanueva, M. Chalifour, A. Chappell, E. Chardonnet, N. Charitonidis, A. Chatterjee, S. Chattopadhyay, H. Chen, M. Chen, Y. Chen, Z. Chen, D. Cherdack, C. Chi, S. Childress, A. Chiriacescu, G. Chisnall, K. Cho, S. Choate, D. Chokheli, S. Choubey, A. Christensen, D. Christian, G. Christodoulou, A. Chukanov, E. Church, P. Clarke, T. E. Coan, A. G. Cocco, J. A. B. Coelho, E. Conley, R. Conley, J. M. Conrad, M. Convery, S. Copello, L. Corwin, L. Cremaldi, L. Cremonesi, J. I. Crespo-Anad\u00f3n, E. Cristaldo, R. Cross, A. Cudd, C. Cuesta, Y. Cui, D. Cussans, M. Dabrowski, O. Dalager, H. da Motta, L. Da Silva Peres, C. David, Q. David, G. S. Davies, S. Davini, J. Dawson, K. De, R. M. De Almeida, P. Debbins, I. De Bonis, M. P. Decowski, A. de Gouv\u00eaa, P. C. De Holanda, I. L. De Icaza Astiz, A. Deisting, P. De Jong, A. Delbart, D. Delepine, M. Delgado, A. Dell'Acqua, P. De Lurgio, J. R. T. de Mello Neto, D. M. DeMuth, S. Dennis, C. Densham, G. W. Deptuch, A. De Roeck, V. De Romeri, G. De Souza, R. Dharmapalan, F. Diaz, J. S. D\u00edaz, S. Di Domizio, L. Di Giulio, P. Ding, L. Di Noto, C. Distefano, R. Diurba, M. Diwan, Z. Djurcic, N. Dokania, S. Dolan, M. J. Dolinski, L. Domine, D. Douglas, D. Douillet, G. Drake, F. Drielsma, D. Duchesneau, K. Duffy, P. Dunne, T. Durkin, H. Duyang, O. Dvornikov, D. A. Dwyer, A. S. Dyshkant, M. Eads, A. Earle, D. Edmunds, J. Eisch, L. Emberger, S. Emery, A. Ereditato, C. O. Escobar, G. Eurin, J. J. Evans, E. Ewart, A. C. Ezeribe, K. Fahey, A. Falcone, C. Farnese, Y. Farzan, J. Felix, M. Fernandes Carneiro da Silva, E. Fernandez-Martinez, P. Fernandez Menendez, F. Ferraro, L. Fields, F. Filthaut, A. Fiorentini, R. S. Fitzpatrick, W. Flanagan, B. Fleming, R. Flight, D. V. Forero, J. Fowler, W. Fox, J. Franc, K. Francis, D. Franco, J. Freeman, J. Freestone, J. Fried, A. Friedland, S. Fuess, I. Furic, A. P. Furmanski, A. Gago, H. Gallagher, A. Gallas, A. Gallego-Ros, N. Gallice, V. Galymov, E. Gamberini, T. Gamble, R. Gandhi, R. Gandrajula, F. Gao, S. Gao, D. Garcia-Gamez, M. \u00c1 Garc\u00eda-Peris, S. Gardiner, D. Gastler, G. Ge, B. Gelli, A. Gendotti, S. Gent, Z. Ghorbani-Moghaddam, D. Gibin, I. Gil-Botella, S. Gilligan, C. Girerd, A. K. Giri, D. Gnani, O. Gogota, M. Gold, S. Gollapinni, K. Gollwitzer, R. A. Gomes, L. V. Gomez Bermeo, L. S. Gomez Fajardo, F. Gonnella, J. A. Gonzalez-Cuevas, D. Gonzalez-Diaz, M. Gonzalez-Lopez, M. C. Goodman, O. Goodwin, S. Goswami, C. Gotti, E. Goudzovski, C. Grace, M. Graham, R. Gran, E. Granados, P. Granger, A. Grant, C. Grant, D. Gratieri, P. Green, L. Greenler, J. Greer, W. C. Griffith, M. Groh, J. Grudzinski, K. Grzelak, W. Gu, V. Guarino, R. Guenette, E. Guerard, A. Guglielmi, B. Guo, K. K. Guthikonda, R. Gutierrez, P. Guzowski, M. M. Guzzo, S. Gwon, A. Habig, H. Hadavand, R. Haenni, A. Hahn, J. Haiston, P. Hamacher-Baumann, T. Hamernik, P. Hamilton, J. Han, D. A. Harris, J. Hartnell, J. Harton, T. Hasegawa, C. Hasnip, R. Hatcher, K. W. Hatfield, A. Hatzikoutelis, C. Hayes, E. Hazen, A. Heavey, K. M. Heeger, J. Heise, K. Hennessy, S. Henry, M. A. Hernandez Morquecho, K. Herner, L. Hertel, V Hewes, A. Higuera, T. Hill, S. J. Hillier, A. Himmel, J. Hoff, C. Hohl, A. Holin, E. Hoppe, G. A. Horton-Smith, M. Hostert, A. Hourlier, B. Howard, R. Howell, J. Huang, J. Huang, J. Hugon, G. Iles, N. Ilic, A. M. Iliescu, R. Illingworth, A. Ioannisian, L. Isenhower, R. Itay, A. Izmaylov, S. Jackson, V. Jain, E. James, B. Jargowsky, F. Jediny, D. Jena, Y. S. Jeong, C. Jes\u00fas-Valls, X. Ji, L. Jiang, S. Jim\u00e9nez, A. Jipa, R. Johnson, B. Jones, S. B. Jones, M. Judah, C. K. Jung, T. Junk, Y. Jwa, M. Kabirnezhad, A. Kaboth, I. Kadenko, I. Kakorin, F. Kamiya, N. Kaneshige, G. Karagiorgi, G. Karaman, A. Karcher, M. Karolak, Y. Karyotakis, S. Kasai, S. P. Kasetti, L. Kashur, N. Kazaryan, E. Kearns, P. Keener, K. J. Kelly, E. Kemp, O. Kemularia, W. Ketchum, S. H. Kettell, M. Khabibullin, A. Khotjantsev, A. Khvedelidze, D. Kim, B. King, B. Kirby, M. Kirby, J. Klein, K. Koehler, L. W. Koerner, S. Kohn, P. P. Koller, L. Kolupaeva, M. Kordosky, T. Kosc, U. Kose, V. A. Kosteleck\u00fd, K. Kothekar, F. Krennrich, I. Kreslo, Y. Kudenko, V. A. Kudryavtsev, S. Kulagin, J. Kumar, P. Kumar, P. Kunze, N. Kurita, C. Kuruppu, V. Kus, T. Kutter, A. Lambert, B. Land, K. Lande, C. E. Lane, K. Lang, T. Langford, J. Larkin, P. Lasorak, D. Last, C. Lastoria, A. Laundrie, A. Lawrence, I. Lazanu, R. LaZur, T. Le, S. Leardini, J. Learned, P. LeBrun, T. LeCompte, G. Lehmann Miotto, R. Lehnert, M. A. Leigui de Oliveira, M. Leitner, L. Li, S. W. Li, T. Li, Y. Li, H. Liao, C. S. Lin, Q. Lin, S. Lin, A. Lister, B. R. Littlejohn, J. Liu, S. Lockwitz, T. Loew, M. Lokajicek, I. Lomidze, K. Long, K. Loo, D. Lorca, T. Lord, J. M. LoSecco, W. C. Louis, X. -G. Lu, K. B. Luk, X. Luo, N. Lurkin, T. Lux, V. P. Luzio, D. MacFarlane, A. A. Machado, P. Machado, C. T. Macias, J. R. Macier, A. Maddalena, A. Madera, P. Madigan, S. Magill, K. Mahn, A. Maio, A. Major, J. A. Maloney, G. Mandrioli, R. C. Mandujano, J. Maneira, L. Manenti, S. Manly, A. Mann, K. Manolopoulos, M. Manrique Plata, V. N. Manyam, L. Manzanillas, M. Marchan, A. Marchionni, W. Marciano, D. Marfatia, C. Mariani, J. Maricic, R. Marie, F. Marinho, A. D. Marino, D. Marsden, M. Marshak, C. M. Marshall, J. Marshall, J. Marteau, J. Martin-Albo, N. Martinez, D. A. Martinez Caicedo, S. Martynenko, K. Mason, A. Mastbaum, M. Masud, S. Matsuno, J. Matthews, C. Mauger, N. Mauri, K. Mavrokoridis, I. Mawby, R. Mazza, A. Mazzacane, E. Mazzucato, T. McAskill, E. McCluskey, N. McConkey, K. S. McFarland, C. McGrew, A. McNab, A. Mefodiev, P. Mehta, P. Melas, O. Mena, S. Menary, H. Mendez, D. P. M\u00e9ndez, A. Menegolli, G. Meng, M. D. Messier, W. Metcalf, T. Mettler, M. Mewes, H. Meyer, T. Miao, G. Michna, T. Miedema, J. Migenda, V. Mikola, R. Milincic, W. Miller, J. Mills, C. Milne, O. Mineev, O. G. Miranda, S. Miryala, C. S. Mishra, S. R. Mishra, A. Mislivec, D. Mladenov, I. Mocioiu, K. Moffat, N. Moggi, R. Mohanta, T. A. Mohayai, N. Mokhov, J. Molina, L. Molina Bueno, A. Montanari, C. Montanari, D. Montanari, L. M. Montano Zetina, J. Moon, M. Mooney, A. F. Moor, D. Moreno, C. Morris, C. Mossey, E. Motuk, C. A. Moura, J. Mousseau, W. Mu, L. Mualem, J. Mueller, M. Muether, S. Mufson, F. Muheim, A. Muir, M. Mulhearn, D. Munford, H. Muramatsu, S. Murphy, J. Musser, J. Nachtman, S. Nagu, M. Nalbandyan, R. Nandakumar, D. Naples, S. Narita, D. Navas-Nicol\u00e1s, A. Navrer-Agasson, N. Nayak, M. Nebot-Guinot, K. Negishi, J. K. Nelson, J. Nesbit, M. Nessi, D. Newbold, M. Newcomer, D. Newhart, H. Newton, R. Nichol, F. Nicolas-Arnaldos, E. Niner, K. Nishimura, A. Norman, A. Norrick, R. Northrop, P. Novella, J. A. Nowak, M. Oberling, J. P. Ochoa-Ricoux, A. Olivares Del Campo, A. Olivier, A. Olshevskiy, Y. Onel, Y. Onishchuk, J. Ott, L. Pagani, S. Pakvasa, G. Palacio, O. Palamara, S. Palestini, J. M. Paley, M. Pallavicini, C. Palomares, J. L. Palomino-Gallo, E. Pantic, V. Paolone, V. Papadimitriou, R. Papaleo, A. Papanestis, S. Paramesvaran, S. Parke, Z. Parsa, M. Parvu, S. Pascoli, L. Pasqualini, J. Pasternak, J. Pater, C. Patrick, L. Patrizii, R. B. Patterson, S. J. Patton, T. Patzak, A. Paudel, B. Paulos, L. Paulucci, Z. Pavlovic, G. Pawloski, D. Payne, V. Pec, S. J. M. Peeters, E. Pennacchio, A. Penzo, O. L. G. Peres, J. Perry, D. Pershey, G. Pessina, G. Petrillo, C. Petta, R. Petti, F. Piastra, L. Pickering, F. Pietropaolo, R. Plunkett, R. Poling, X. Pons, N. Poonthottathil, S. Pordes, J. Porter, M. Potekhin, R. Potenza, B. V. K. S. Potukuchi, J. Pozimski, M. Pozzato, S. Prakash, T. Prakash, S. Prince, D. Pugnere, X. Qian, M. C. Queiroga Bazetto, J. L. Raaf, V. Radeka, J. Rademacker, B. Radics, A. Rafique, E. Raguzin, M. Rai, M. Rajaoalisoa, I. Rakhno, A. Rakotonandrasana, L. Rakotondravohitra, Y. A. Ramachers, R. Rameika, M. A. Ramirez Delgado, B. Ramson, A. Rappoldi, G. Raselli, P. Ratoff, S. Raut, R. F. Razakamiandra, J. S. Real, B. Rebel, M. Reggiani-Guzzo, T. Rehak, J. Reichenbacher, S. D. Reitzner, H. Rejeb Sfar, A. Renshaw, S. Rescia, F. Resnati, A. Reynolds, C. Riccio, G. Riccobene, L. C. J. Rice, J. Ricol, A. Rigamonti, Y. Rigaut, D. Rivera, L. Rochester, M. Roda, P. Rodrigues, M. J. Rodriguez Alonso, E. Rodriguez Bonilla, J. Rodriguez Rondon, S. Rosauro-Alcaraz, M. Rosenberg, P. Rosier, B. Roskovec, M. Rossella, J. Rout, P. Roy, S. Roy, A. Rubbia, C. Rubbia, F. C. Rubio, B. Russell, D. Ruterbories, R. Saakyan, S. Sacerdoti, T. Safford, R. Sahay, N. Sahu, P. Sala, N. Samios, O. Samoylov, M. C. Sanchez, D. A. Sanders, D. Sankey, S. Santana, M. Santos-Maldonado, N. Saoulidou, P. Sapienza, C. Sarasty, I. Sarcevic, G. Savage, V. Savinov, A. Scaramelli, A. Scarff, A. Scarpelli, T. Schaffer, H. Schellman, P. Schlabach, D. Schmitz, K. Scholberg, A. Schukraft, E. Segreto, J. Sensenig, I. Seong, A. Sergi, D. Sgalaberna, M. H. Shaevitz, S. Shafaq, M. Shamma, R. Sharankova, H. R. Sharma, R. Sharma, R. Kumar, T. Shaw, C. Shepherd-Themistocleous, S. Shin, D. Shooltz, R. Shrock, L. Simard, F. Simon, N. Simos, J. Sinclair, G. Sinev, J. Singh, J. Singh, V. Singh, R. Sipos, F. W. Sippach, G. Sirri, A. Sitraka, K. Siyeon, K. Skarpaas VIII, A. Smith, E. Smith, P. Smith, J. Smolik, M. Smy, E. L. Snider, P. Snopok, M. Soares Nunes, H. Sobel, M. Soderberg, C. J. Solano Salinas, S. S\u00f6ldner-Rembold, N. Solomey, V. Solovov, W. E. Sondheim, M. Sorel, J. Soto-Oton, A. Sousa, K. Soustruznik, F. Spagliardi, M. Spanu, J. Spitz, N. J. C. Spooner, K. Spurgeon, R. Staley, M. Stancari, L. Stanco, R. Stanley, R. Stein, H. M. Steiner, J. Stewart, B. Stillwell, J. Stock, F. Stocker, T. Stokes, M. Strait, T. Strauss, S. Striganov, A. Stuart, J. G. Suarez, H. Sullivan, D. Summers, A. Surdo, V. Susic, L. Suter, C. M. Sutera, R. Svoboda, B. Szczerbinska, A. M. Szelc, R. Talaga, H. A. Tanaka, B. Tapia Oregui, A. Tapper, S. Tariq, E. Tatar, R. Tayloe, A. M. Teklu, M. Tenti, K. Terao, C. A. Ternes, F. Terranova, G. Testera, A. Thea, J. L. Thompson, C. Thorn, S. C. Timm, J. Todd, A. Tonazzo, D. Torbunov, M. Torti, M. Tortola, F. Tortorici, D. Totani, M. Toups, C. Touramanis, J. Trevor, S. Trilov, W. H. Trzaska, Y. T. Tsai, Z. Tsamalaidze, K. V. Tsang, N. Tsverava, S. Tufanli, C. Tull, E. Tyley, M. Tzanov, M. A. Uchida, J. Urheim, T. Usher, S. Uzunyan, M. R. Vagins, P. Vahle, G. A. Valdiviesso, E. Valencia, Z. Vallari, J. W. F. Valle, S. Vallecorsa, R. Van Berg, R. G. Van de Water, F. Varanini, D. Vargas, G. Varner, J. Vasel, S. Vasina, G. Vasseur, N. Vaughan, K. Vaziri, S. Ventura, A. Verdugo, S. Vergani, M. A. Vermeulen, M. Verzocchi, M. Vicenzi, H. Vieira de Souza, C. Vignoli, C. Vilela, B. Viren, T. Vrba, T. Wachala, A. V. Waldron, M. Wallbank, H. Wang, J. Wang, M. H. L. S. Wang, Y. Wang, Y. Wang, K. Warburton, D. Warner, M. Wascko, D. Waters, A. Watson, P. Weatherly, A. Weber, M. Weber, H. Wei, A. Weinstein, D. Wenman, M. Wetstein, A. White, L. H. Whitehead, D. Whittington, M. J. Wilking, C. Wilkinson, Z. Williams, F. Wilson, R. J. Wilson, J. Wolcott, T. Wongjirad, A. Wood, K. Wood, E. Worcester, M. Worcester, C. Wret, W. Wu, W. Wu, Y. Xiao, E. Yandel, G. Yang, K. Yang, S. Yang, T. Yang, A. Yankelevich, N. Yershov, K. Yonehara, T. Young, B. Yu, H. Yu, J. Yu, W. Yuan, R. Zaki, J. Zalesak, L. Zambelli, B. Zamorano, A. Zani, L. Zazueta, G. Zeit, G. P. Zeller, J. Zennamo, K. Zeug, C. Zhang, M. Zhao, E. Zhivun, G. Zhu, P. Zilberman, E. D. Zimmerman, M. Zito, S. Zucchelli, J. Zuklin, V. Zutshi, and R. Zwaska. Deep underground neutrino experiment (dune) near detector conceptual design report. 2021. URL: https://arxiv.org/abs/2103.13910, arXiv:2103.13910.\u00a0\u21a9</p> </li> <li> <p>J. Kvapil, G. Borca-Tasciuc, H. Bossi, K. Chen, Y. Chen, Y. Corrales Morales, H. Da Costa, C. Da Silva, C. Dean, J. Durham, S. Fu, C. Hao, P. Harris, O. Hen, H. Jheng, Y. Lee, P. Li, X. Li, Y. Lin, M. X. Liu, V. Loncar, J. P. Mitrevski, A. Olvera, M. L. Purschke, J. S. Renck, G. Roland, J. Schambach, Z. Shi, N. Tran, N. Wuerfel, B. Xu, D. Yu, and H. Zhang. Intelligent experiments through real-time ai: fast data processing and autonomous detector control for sphenix and future eic detectors. 2025. URL: https://arxiv.org/abs/2501.04845, arXiv:2501.04845.\u00a0\u21a9</p> </li> <li> <p>Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, and Javier Duarte. Neural architecture codesign for fast physics applications. 2025. URL: https://arxiv.org/abs/2501.05515, arXiv:2501.05515.\u00a0\u21a9</p> </li> <li> <p>Benjamin Parpillon, Chinar Syal, Jieun Yoo, Jennet Dickinson, Morris Swartz, Giuseppe Di Guglielmo, Alice Bean, Douglas Berry, Manuel Blanco Valentin, Karri DiPetrillo, Anthony Badea, Lindsey Gray, Petar Maksimovic, Corrinne Mills, Mark S. Neubauer, Gauri Pradhan, Nhan Tran, Dahai Wen, and Farah Fahim. Smart pixels: in-pixel ai for on-sensor data filtering. 2024. URL: https://arxiv.org/abs/2406.14860, arXiv:2406.14860.\u00a0\u21a9</p> </li> <li> <p>Zhengchun Liu, Hemant Sharma, Jun-Sang Park, Peter Kenesei, Antonino Miceli, Jonathan Almer, Rajkumar Kettimuthu, and Ian Foster. Braggnn: fast x-ray bragg peak analysis using deep learning. 2021. URL: https://arxiv.org/abs/2008.08198, arXiv:2008.08198.\u00a0\u21a9</p> </li> <li> <p>Shuyu Qin, Joshua Agar, and Nhan Tran. Extremely noisy 4d-tem strain mapping using cycle consistent spatial transforming autoencoders. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop. 2023. URL: https://openreview.net/forum?id=7yt3N0o0W9.\u00a0\u21a9</p> </li> <li> <p>Yumou Wei, Ryan F. Forelli, Chris Hansen, Jeffrey P. Levesque, Nhan Tran, Joshua C. Agar, Giuseppe Di Guglielmo, Michael E. Mauel, and Gerald A. Navratil. Low latency optical-based mode tracking with machine learning deployed on fpgas on a tokamak. 2024. URL: https://arxiv.org/abs/2312.00128, arXiv:2312.00128, doi:https://doi.org/10.1063/5.0190354.\u00a0\u21a9</p> </li> <li> <p>Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and Hainan Ye. Aibench: an industry standard internet service ai benchmark suite. 2019. URL: https://arxiv.org/abs/1908.08998, arXiv:1908.08998.\u00a0\u21a9</p> </li> <li> <p>Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao, Shujie Zhang, and Jiahui Dai. Bigdatabench: a scalable and unified big data and ai benchmark suite. 2018. URL: https://arxiv.org/abs/1802.08254, arXiv:1802.08254.\u00a0\u21a9</p> </li> <li> <p>Steven Farrell, Murali Emani, Jacob Balma, Lukas Drescher, Aleksandr Drozd, Andreas Fink, Geoffrey Fox, David Kanter, Thorsten Kurth, Peter Mattson, Dawei Mu, Amit Ruhela, Kento Sato, Koichi Shirahata, Tsuguchika Tabaru, Aristeidis Tsaris, Jan Balewski, Ben Cumming, Takumi Danjo, Jens Domke, Takaaki Fukai, Naoto Fukumoto, Tatsuya Fukushi, Balazs Gerofi, Takumi Honda, Toshiyuki Imamura, Akihiko Kasagi, Kentaro Kawakami, Shuhei Kudo, Akiyoshi Kuroda, Maxime Martinasso, Satoshi Matsuoka, Henrique Mendon\u00e7a, Kazuki Minami, Prabhat Ram, Takashi Sawada, Mallikarjun Shankar, Tom St. John, Akihiro Tabuchi, Venkatram Vishwanath, Mohamed Wahib, Masafumi Yamazaki, and Junqi Yin. Mlperf hpc: a holistic benchmark suite for scientific machine learning on hpc systems. 2021. URL: https://arxiv.org/abs/2110.11466, arXiv:2110.11466.\u00a0\u21a9</p> </li> <li> <p>Jeyan Thiyagalingam, Gregor von Laszewski, Junqi Yin, Murali Emani, Juri Papay, Gregg Barrett, Piotr Luszczek, Aristeidis Tsaris, Christine Kirkpatrick, Feiyi Wang, Tom Gibbs, Venkatram Vishwanath, Mallikarjun Shankar, Geoffrey Fox, and Tony Hey. Ai benchmarking for science: efforts from the mlcommons science working group. In Hartwig Anzt, Amanda Bienz, Piotr Luszczek, and Marc Baboulin, editors, High Performance Computing. ISC High Performance 2022 International Workshops, 47\u201364. Cham, 2022. Springer International Publishing.\u00a0\u21a9</p> </li> <li> <p>Thea Aarrestad, Ekaterina Govorkova, Jennifer Ngadiuba, Ema Puljak, Maurizio Pierini, and Kinga Anna Wozniak. Unsupervised new physics detection at 40 mhz: training dataset. 2021. URL: https://zenodo.org/record/5046389, doi:10.5281/ZENODO.5046389.\u00a0\u21a9</p> </li> <li> <p>Alexandros Karargyris, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, Prakash Narayana Moorthy, Alexander Chowdhury, Junyi Guo, Sahil Nalawade, Jacob Rosenthal, David Kanter, Maria Xenochristou, Daniel J. Beutel, Verena Chung, Timothy Bergquist, James Eddy, Abubakar Abid, Lewis Tunstall, Omar Sanseviero, Dimitrios Dimitriadis, Yiming Qian, Xinxing Xu, Yong Liu, Rick Siow Mong Goh, Srini Bala, Victor Bittorf, Sreekar Reddy Puchala, Biagio Ricciuti, Soujanya Samineni, Eshna Sengupta, Akshay Chaudhari, Cody Coleman, Bala Desinghu, Gregory Diamos, Debo Dutta, Diane Feddema, Grigori Fursin, Xinyuan Huang, Satyananda Kashyap, Nicholas Lane, Indranil Mallick, Pietro Mascagni, Virendra Mehta, Cassiano Ferro Moraes, Vivek Natarajan, Nikola Nikolov, Nicolas Padoy, Gennady Pekhimenko, Vijay Janapa Reddi, G. Anthony Reina, Pablo Ribalta, Abhishek Singh, Jayaraman J. Thiagarajan, Jacob Albrecht, Thomas Wolf, Geralyn Miller, Huazhu Fu, Prashant Shah, Daguang Xu, Poonam Yadav, David Talby, Mark M. Awad, Jeremy P. Howard, Michael Rosenthal, Luigi Marchionni, Massimo Loda, Jason M. Johnson, Spyridon Bakas, Peter Mattson, FeTS Consortium, BraTS-2020 Consortium, and AI4SafeChole Consortium. Federated benchmarking of medical artificial intelligence with medperf. Nature Machine Intelligence, 5(7):799\u2013810, July 2023. URL: https://doi.org/10.1038/s42256-023-00652-2, doi:10.1038/s42256-023-00652-2.\u00a0\u21a9</p> </li> <li> <p>Claudius Krause, Michele Faucci Giannelli, Gregor Kasieczka, Benjamin Nachman, Dalila Salamani, David Shih, Anna Zaborowska, Oz Amram, Kerstin Borras, Matthew R. Buckley, Erik Buhmann, Thorsten Buss, Renato Paulo Da Costa Cardoso, Anthony L. Caterini, Nadezda Chernyavskaya, Federico A. G. Corchia, Jesse C. Cresswell, Sascha Diefenbacher, Etienne Dreyer, Vijay Ekambaram, Engin Eren, Florian Ernst, Luigi Favaro, Matteo Franchini, Frank Gaede, Eilam Gross, Shih-Chieh Hsu, Kristina Jaruskova, Benno K\u00e4ch, Jayant Kalagnanam, Raghav Kansal, Taewoo Kim, Dmitrii Kobylianskii, Anatolii Korol, William Korcari, Dirk Kr\u00fccker, Katja Kr\u00fcger, Marco Letizia, Shu Li, Qibin Liu, Xiulong Liu, Gabriel Loaiza-Ganem, Thandikire Madula, Peter McKeown, Isabell-A. Melzer-Pellmann, Vinicius Mikuni, Nam Nguyen, Ayodele Ore, Sofia Palacios Schweitzer, Ian Pang, Kevin Pedro, Tilman Plehn, Witold Pokorski, Huilin Qu, Piyush Raikwar, John A. Raine, Humberto Reyes-Gonzalez, Lorenzo Rinaldi, Brendan Leigh Ross, Moritz A. W. Scham, Simon Schnake, Chase Shimmin, Eli Shlizerman, Nathalie Soybelman, Mudhakar Srivatsa, Kalliopi Tsolaki, Sofia Vallecorsa, Kyongmin Yeo, and Rui Zhang. Calochallenge 2022: a community challenge for fast calorimeter simulation. 2024. URL: https://arxiv.org/abs/2410.21611, arXiv:2410.21611.\u00a0\u21a9</p> </li> <li> <p>Avrim Blum and Moritz Hardt. The ladder: a reliable leaderboard for machine learning competitions. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, 1006\u20131014. Lille, France, July 2015. PMLR. URL: https://proceedings.mlr.press/v37/blum15.html.\u00a0\u21a9</p> </li> <li> <p>Zhen Xu, Sergio Escalera, Adrien Pav\u00e3o, Magali Richard, Wei-Wei Tu, Quanming Yao, Huan Zhao, and Isabelle Guyon. Codabench: flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543, July 2022. URL: http://dx.doi.org/10.1016/j.patter.2022.100543, doi:10.1016/j.patter.2022.100543.\u00a0\u21a9</p> </li> <li> <p>Piotr Luszczek. Sabath: fair metadata technology for surrogate benchmarks. Technical Report, University of Tennessee, 2021. URL: https://github.com/icl-utk-edu/slip/tree/sabath.\u00a0\u21a9</p> </li> <li> <p>Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. Pdebench: an extensive benchmark for scientific machine learning. 2024. URL: https://arxiv.org/abs/2210.07182, arXiv:2210.07182.\u00a0\u21a9</p> </li> <li> <p>Ruben Ohana, Michael McCabe, Lucas Meyer, Rudy Morel, Fruzsina J. Agocs, Miguel Beneitez, Marsha Berger, Blakesley Burkhart, Stuart B. Dalziel, Drummond B. Fielding, Daniel Fortunato, Jared A. Goldberg, Keiya Hirashima, Yan-Fei Jiang, Rich R. Kerswell, Suryanarayana Maddu, Jonah Miller, Payel Mukhopadhyay, Stefan S. Nixon, Jeff Shen, Romain Watteaux, Bruno R\u00e9galdo-Saint Blancard, Fran\u00e7ois Rozet, Liam H. Parker, Miles Cranmer, and Shirley Ho. The well: a large-scale collection of diverse physics simulations for machine learning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 44989\u201345037. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, volume, 1362 1379. 2024. doi:10.1109/SCW63240.2024.00178.\u00a0\u21a9</p> </li> <li> <p>Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: efficient execution of structured language model programs. 2024. URL: https://arxiv.org/abs/2312.07104, arXiv:2312.07104.\u00a0\u21a9</p> </li> <li> <p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, 611 626. New York, NY, USA, 2023. Association for Computing Machinery. URL: https://doi.org/10.1145/3600006.3613165, doi:10.1145/3600006.3613165.\u00a0\u21a9</p> </li> <li> <p>Simon Mo. Vllm performance dashboard. 2024. URL: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/.\u00a0\u21a9</p> </li> <li> <p>Kin G. Olivares, Cristian Chall\u00fa, Federico Garza, Max Mergenthaler Canseco, and Artur Dubrawski. Neuralforecast: user friendly state-of-the-art neural forecasting models. PyCon Salt Lake City, Utah, US 2022, 2022. URL: https://github.com/Nixtla/neuralforecast.\u00a0\u21a9</p> </li> <li> <p>Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 37, 6989\u20136997. 2023.\u00a0\u21a9</p> </li> <li> <p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: time series forecasting by reprogramming large language models. 2024. URL: https://arxiv.org/abs/2310.01728, arXiv:2310.01728.\u00a0\u21a9</p> </li> <li> <p>Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1. 2024. URL: https://arxiv.org/abs/2310.03589, arXiv:2310.03589.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.\u00a0\u21a9</p> </li> <li> <p>Giuseppe Di Guglielmo, Botao Du, Javier Campos, Alexandra Boltasseva, Akash V. Dixit, Farah Fahim, Zhaxylyk Kudyshev, Santiago Lopez, Ruichao Ma, Gabriel N. Perdue, Nhan Tran, Omer Yesilyurt, and Daniel Bowring. End-to-end workflow for machine learning-based qubit readout with qick and hls4ml. 2025. URL: https://arxiv.org/abs/2501.14663, arXiv:2501.14663.\u00a0\u21a9</p> </li> <li> <p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.\u00a0\u21a9</p> </li> <li> <p>Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, and Xi Peng. Seafloorai: a large-scale vision-language dataset for seafloor geological survey. 2024. URL: https://arxiv.org/abs/2411.00172, arXiv:2411.00172.\u00a0\u21a9</p> </li> <li> <p>Pin Chen, Luoxuan Peng, Rui Jiao, Qing Mo, Zhen Wang, Wenbing Huang, Yang Liu, and Yutong Lu. Learning superconductivity from ordered and disordered material structures. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 108902\u2013108928. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Deyu Zou, Shikun Liu, Siqi Miao, Victor Fung, Shiyu Chang, and Pan Li. Gess: benchmarking geometric deep learning under scientific applications with distribution shifts. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 92499\u201392528. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Ralph E Peterson, Aramis Tanelus, Christopher Ick, Bartul Mimica, Niegil Francis, Violet J Ivan, Aman Choudhri, Annegret L Falkner, Mala Murthy, David M Schneider, Dan H Sanes, and Alex H Williams. Vocal call locator benchmark (vcl) for localizing rodent vocalizations from multi-channel audio. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 106370\u2013106382. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Roman Bushuiev, Anton Bushuiev, Niek F. de Jonge, Adamo Young, Fleming Kretschmer, Raman Samusevich, Janne Heirman, Fei Wang, Luke Zhang, Kai D\u00fchrkop, Marcus Ludwig, Nils A. Haupt, Apurva Kalia, Corinna Brungs, Robin Schmid, Russell Greiner, Bo Wang, David S. Wishart, Li-Ping Liu, Juho Rousu, Wout Bittremieux, Hannes Rost, Tytus D. Mak, Soha Hassoun, Florian Huber, Justin J.J. van der Hooft, Michael A. Stravs, Sebastian B\u00f6cker, Josef Sivic, and Tom\u00e1\u0161 Pluskal. Massspecgym: a benchmark for the discovery and identification of molecules. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 110010\u2013110027. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Yiheng Wang, Tianyu Wang, Yuying Zhang, Hongji Zhang, Haoyu Zheng, Guanjie Zheng, and Linghe Kong. Urbandatalayer: a unified data pipeline for urban science. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 7296\u20137310. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf.\u00a0\u21a9</p> </li> <li> <p>Kuzma Khrabrov, Anton Ber, Artem Tsypin, Konstantin Ushenin, Egor Rumiantsev, Alexander Telepov, Dmitry Protasov, Ilya Shenbin, Anton Alekseev, Mikhail Shirokikh, Sergey Nikolenko, Elena Tutubalina, and Artur Kadurin. Delta-squared dft: a universal quantum chemistry dataset of drug-like molecules and a benchmark for neural network potentials. 2024. URL: https://arxiv.org/abs/2406.14347, arXiv:2406.14347.\u00a0\u21a9</p> </li> <li> <p>Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, and Enhong Chen. Exploring user retrieval integration towards large language models for cross-domain sequential recommendation. 2024. URL: https://arxiv.org/abs/2406.03085, arXiv:2406.03085.\u00a0\u21a9</p> </li> <li> <p>Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. Spiqa: a dataset for multimodal question answering on scientific papers. 2025. URL: https://arxiv.org/abs/2407.09413, arXiv:2407.09413.\u00a0\u21a9</p> </li> </ol>"},{"location":"md/benchmarks/","title":"Index of Benchmarks","text":"All domains Sort: Date \u2193 Sort: Date \u2191 Sort: Name A\u2013Z Sort: Rating \u2193 Reset MMLU (Massive Multitask Language Understanding) <p>Multidomain \u2022 Accuracy</p> multitask multiple-choice zero-shot few-shot knowledge probing <p>Avg rating: 3.50/5</p> <p>Details</p> GPQA Diamond <p>Science \u2022 Accuracy</p> Google-proof graduate-level science QA chemistry physics <p>Avg rating: 3.83/5</p> <p>Details</p> ARC-Challenge (Advanced Reasoning Challenge) <p>Science \u2022 Accuracy</p> grade-school science QA challenge set reasoning <p>Avg rating: 2.83/5</p> <p>Details</p> Humanity's Last Exam <p>Multidomain \u2022 Accuracy</p> cross-domain academic exam multiple-choice multidisciplinary <p>Avg rating: 3.33/5</p> <p>Details</p> FrontierMath <p>Mathematics \u2022 Accuracy</p> symbolic reasoning number theory algebraic geometry category theory <p>Avg rating: 1.67/5</p> <p>Details</p> SciCode <p>Scientific Programming \u2022 Solve rate (%)</p> code synthesis scientific computing programming benchmark <p>Avg rating: 2.75/5</p> <p>Details</p> AIME (American Invitational Mathematics Examination) <p>Mathematics \u2022 Accuracy</p> algebra combinatorics number theory geometry <p>Avg rating: 1.50/5</p> <p>Details</p> MATH-500 <p>Mathematics \u2022 Accuracy</p> calculus algebra number theory geometry <p>Avg rating: 1.17/5</p> <p>Details</p> CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction) <p>Multidomain Science \u2022 Accuracy</p> long-context information extraction multimodal <p>Avg rating: 3.33/5</p> <p>Details</p> FEABench (Finite Element Analysis Benchmark) <p>Computational Engineering \u2022 Solve time, Error norm</p> finite element simulation PDE <p>Avg rating: 3.92/5</p> <p>Details</p> SPIQA (Scientific Paper Image Question Answering) <p>Computer Science \u2022 Accuracy, F1 score</p> multimodal QA figure understanding table comprehension chain-of-thought <p>Avg rating: 3.58/5</p> <p>Details</p> MedQA <p>Medical Question Answering \u2022 Accuracy</p> USMLE diagnostic QA medical knowledge multilingual <p>Avg rating: 3.50/5</p> <p>Details</p> BaisBench (Biological AI Scientist Benchmark) <p>Computational Biology \u2022 Annotation accuracy, QA accuracy</p> single-cell annotation biological QA autonomous discovery <p>Avg rating: 4.00/5</p> <p>Details</p> MOLGEN <p>Computational Chemistry \u2022 Validity%, Novelty%, QED, Docking score</p> SELFIES GAN property optimization <p>Avg rating: 0.00/5</p> <p>Details</p> Open Graph Benchmark (OGB) - Biology <p>Graph ML \u2022 Accuracy, ROC-AUC</p> node prediction link prediction graph classification <p>Avg rating: 4.50/5</p> <p>Details</p> Materials Project <p>Materials Science \u2022 MAE, R^2</p> DFT materials genome high-throughput <p>Avg rating: 1.92/5</p> <p>Details</p> OCP (Open Catalyst Project) <p>Chemistry; Materials Science \u2022 MAE (energy), MAE (force)</p> DFT relaxations adsorption energy graph neural networks <p>Avg rating: 4.17/5</p> <p>Details</p> JARVIS-Leaderboard <p>Materials Science; Benchmarking \u2022 MAE, RMSE, Accuracy</p> leaderboards materials methods simulation <p>Avg rating: 2.67/5</p> <p>Details</p> Quantum Computing Benchmarks (QML) <p>Quantum Computing \u2022 Fidelity, Success probability</p> quantum circuits state preparation error correction <p>Avg rating: 2.50/5</p> <p>Details</p> CFDBench (Fluid Dynamics) <p>Fluid Dynamics; Scientific ML \u2022 L2 error, MAE</p> neural operators CFD FNO DeepONet <p>Avg rating: 3.33/5</p> <p>Details</p> SatImgNet <p>Remote Sensing \u2022 Accuracy</p> land-use zero-shot multi-task <p>Avg rating: 3.83/5</p> <p>Details</p> ClimateLearn <p>Climate Science; Forecasting \u2022 RMSE, Anomaly correlation</p> medium-range forecasting ERA5 data-driven <p>Avg rating: 4.17/5</p> <p>Details</p> BIG-Bench (Beyond the Imitation Game Benchmark) <p>NLP; AI Evaluation \u2022 Accuracy, Task-specific metrics</p> few-shot multi-task bias analysis <p>Avg rating: 4.33/5</p> <p>Details</p> CommonSenseQA <p>NLP; Commonsense \u2022 Accuracy</p> ConceptNet multiple-choice adversarial <p>Avg rating: 4.67/5</p> <p>Details</p> Winogrande <p>NLP; Commonsense \u2022 Accuracy, AUC</p> adversarial pronoun resolution <p>Avg rating: 4.00/5</p> <p>Details</p> Jet Classification <p>Particle Physics \u2022 Accuracy, AUC</p> classification real-time ML jet tagging QKeras <p>Avg rating: 4.17/5</p> <p>Details</p> Irregular Sensor Data Compression <p>Particle Physics \u2022 MSE, Compression ratio</p> compression autoencoder sparse data irregular sampling <p>Avg rating: 4.17/5</p> <p>Details</p> Beam Control <p>Accelerators and Magnets \u2022 Stability, Control loss</p> RL beam stabilization control systems simulation <p>Avg rating: 3.00/5</p> <p>Details</p> Ultrafast jet classification at the HL-LHC <p>Particle Physics \u2022 Accuracy, Latency, Resource utilization</p> jet classification FPGA quantization-aware training Deep Sets Interaction Networks <p>Avg rating: 3.17/5</p> <p>Details</p> Quench detection <p>Accelerators and Magnets \u2022 ROC-AUC, Detection latency</p> quench detection autoencoder anomaly detection real-time <p>Avg rating: 2.17/5</p> <p>Details</p> DUNE <p>Particle Physics \u2022 Detection efficiency, Latency</p> DUNE time-series real-time trigger <p>Avg rating: 2.83/5</p> <p>Details</p> Intelligent experiments through real-time AI <p>Instrumentation and Detectors; Nuclear Physics; Particle Physics \u2022 Accuracy (charm and beauty detection), Latency (micros), Resource utilization (LUT/FF/BRAM/DSP)</p> FPGA Graph Neural Network hls4ml real-time inference detector control <p>Avg rating: 3.00/5</p> <p>Details</p> Neural Architecture Codesign for Fast Physics Applications <p>Physics; Materials Science; Particle Physics \u2022 Accuracy, Latency, Resource utilization</p> neural architecture search FPGA deployment quantization pruning hls4ml <p>Avg rating: 3.83/5</p> <p>Details</p> Smart Pixels for LHC <p>Particle Physics; Instrumentation and Detectors \u2022 Data rejection rate, Power per pixel</p> smart pixel on-sensor inference data reduction trigger <p>Avg rating: 3.33/5</p> <p>Details</p> HEDM (BraggNN) <p>Material Science \u2022 Localization accuracy, Inference time</p> BraggNN diffraction peak finding HEDM <p>Avg rating: 3.17/5</p> <p>Details</p> 4D-STEM <p>Material Science \u2022 Classification accuracy, Throughput</p> 4D-STEM electron microscopy real-time image processing <p>Avg rating: 3.17/5</p> <p>Details</p> In-Situ High-Speed Computer Vision <p>Fusion/Plasma \u2022 Accuracy, FPS</p> plasma in-situ vision real-time ML <p>Avg rating: 1.50/5</p> <p>Details</p> BenchCouncil AIBench <p>General \u2022 Throughput, Latency, Accuracy</p> benchmarking AI systems application-level evaluation <p>Avg rating: 3.33/5</p> <p>Details</p> BenchCouncil BigDataBench <p>General \u2022 Data throughput, Latency, Accuracy</p> big data AI benchmarking data analytics <p>Avg rating: 4.00/5</p> <p>Details</p> MLPerf HPC <p>Cosmology, Climate, Protein Structure, Catalysis \u2022 Training time, Accuracy, GPU utilization</p> HPC training inference scientific ML <p>Avg rating: 4.17/5</p> <p>Details</p> MLCommons Science <p>Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD \u2022 MAE, Accuracy, Speedup vs simulation</p> science AI benchmark MLCommons HPC <p>Avg rating: 5.00/5</p> <p>Details</p> LHC New Physics Dataset <p>Particle Physics; Real-time Triggering \u2022 ROC-AUC, Detection efficiency</p> anomaly detection proton collision real-time inference event filtering unsupervised ML <p>Avg rating: 3.33/5</p> <p>Details</p> MLCommons Medical AI <p>Healthcare; Medical AI \u2022 ROC AUC, Accuracy, Fairness metrics</p> medical AI federated evaluation privacy-preserving fairness healthcare benchmarks <p>Avg rating: 4.33/5</p> <p>Details</p> CaloChallenge 2022 <p>LHC Calorimeter; Particle Physics \u2022 Histogram similarity, Classifier AUC, Generation latency</p> calorimeter simulation generative models surrogate modeling LHC fast simulation <p>Avg rating: 4.50/5</p> <p>Details</p> Papers With Code (SOTA Platform) <p>General ML; All domains \u2022 Task-specific (Accuracy, F1, BLEU, etc.)</p> leaderboard benchmarking reproducibility open-source <p>Avg rating: 4.00/5</p> <p>Details</p> Codabench <p>General ML; Multiple \u2022 Submission count, Leaderboard ranking, Task-specific metrics</p> benchmark platform code submission competitions meta-benchmark <p>Avg rating: 1.00/5</p> <p>Details</p> Sabath (SBI-FAIR) <p>Systems; Metadata \u2022 Metadata completeness, FAIR compliance</p> meta-benchmark metadata HPC surrogate modeling <p>Avg rating: 3.67/5</p> <p>Details</p> PDEBench <p>CFD; Weather Modeling \u2022 RMSE, boundary RMSE, Fourier RMSE</p> PDEs CFD scientific ML surrogate modeling NeurIPS <p>Avg rating: 4.50/5</p> <p>Details</p> The Well <p>biological systems, fluid dynamics, acoustic scattering, astrophysical MHD \u2022 Dataset size, Domain breadth</p> surrogate modeling foundation model physics simulations spatiotemporal dynamics <p>Avg rating: 4.00/5</p> <p>Details</p> LLM-Inference-Bench <p>LLM; HPC/inference \u2022 Token throughput (tok/s), Latency, Framework-hardware mix performance</p> LLM inference benchmarking GPU accelerator throughput <p>Avg rating: 4.00/5</p> <p>Details</p> SGLang Framework <p>LLM Vision \u2022 Tokens/sec, Time-to-first-token, Throughput gain vs baseline</p> LLM serving vision-language RadixAttention performance JSON decoding <p>Avg rating: 3.83/5</p> <p>Details</p> vLLM Inference and Serving Engine <p>LLM; HPC/inference \u2022 Tokens/sec, Time to First Token (TTFT), Memory footprint</p> LLM inference PagedAttention CUDA graph streaming API quantization <p>Avg rating: 4.33/5</p> <p>Details</p> vLLM Performance Dashboard <p>LLM; HPC/inference \u2022 Tokens/sec, TTFT, Memory usage</p> Dashboard Throughput visualization Latency analysis Metric tracking <p>Avg rating: 3.50/5</p> <p>Details</p> Nixtla NeuralForecast <p>Time-series forecasting; General ML \u2022 RMSE, MAPE, CRPS</p> time-series neural forecasting NBEATS, NHITS, TFT probabilistic forecasting usability <p>Avg rating: 4.50/5</p> <p>Details</p> Nixtla Neural Forecast NHITS <p>Time-series; General ML \u2022 RMSE, MAPE</p> NHITS long-horizon forecasting neural interpolation time-series <p>Avg rating: 4.33/5</p> <p>Details</p> Nixtla Neural Forecast TimeLLM <p>Time-series; General ML \u2022 RMSE, MAPE</p> Time-LLM language model time-series reprogramming <p>Avg rating: 3.33/5</p> <p>Details</p> Nixtla Neural Forecast TimeGPT <p>Time-series; General ML \u2022 RMSE, Anomaly detection metrics</p> TimeGPT foundation model time-series generative model <p>Avg rating: 3.33/5</p> <p>Details</p> HDR ML Anomaly Challenge (Gravitational Waves) <p>Astrophysics; Time-series \u2022 ROC-AUC, Precision/Recall</p> anomaly detection gravitational waves astrophysics time-series <p>Avg rating: 4.17/5</p> <p>Details</p> HDR ML Anomaly Challenge (Butterfly) <p>Genomics; Image/CV \u2022 Classification accuracy, F1 score</p> anomaly detection computer vision genomics butterfly hybrids <p>Avg rating: 3.00/5</p> <p>Details</p> HDR ML Anomaly Challenge (Sea Level Rise) <p>Climate Science; Time-series, Image/CV \u2022 ROC-AUC, Precision/Recall</p> anomaly detection climate science sea-level rise time-series remote sensing <p>Avg rating: 3.83/5</p> <p>Details</p> Single Qubit Readout on QICK System <p>Quantum Computing \u2022 Accuracy, Latency</p> qubit readout hls4ml FPGA QICK <p>Avg rating: 3.50/5</p> <p>Details</p> GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark <p>Science (Biology, Physics, Chemistry) \u2022 Accuracy</p> Google-proof multiple-choice expert reasoning science QA <p>Avg rating: 3.67/5</p> <p>Details</p> SeafloorAI <p>Marine Science; Vision-Language \u2022 Segmentation pixel accuracy, QA accuracy</p> sonar imagery vision-language seafloor mapping segmentation QA <p>Avg rating: 4.33/5</p> <p>Details</p> SuperCon3D <p>Materials Science; Superconductivity \u2022 MAE (Tc), Validity of generated structures</p> superconductivity crystal structures equivariant GNN generative models <p>Avg rating: 4.17/5</p> <p>Details</p> GeSS <p>Scientific ML; Geometric Deep Learning \u2022 Accuracy, RMSE, OOD robustness delta</p> geometric deep learning distribution shift OOD robustness scientific applications <p>Avg rating: 4.33/5</p> <p>Details</p> Vocal Call Locator (VCL) <p>Neuroscience; Bioacoustics \u2022 Localization error (cm), Recall/Precision</p> source localization bioacoustics time-series SSL <p>Avg rating: 3.83/5</p> <p>Details</p> MassSpecGym <p>Cheminformatics; Molecular Discovery \u2022 Structure accuracy, Retrieval precision, Simulation MSE</p> mass spectrometry molecular structure de novo generation retrieval dataset <p>Avg rating: 3.75/5</p> <p>Details</p> Urban Data Layer (UDL) <p>Urban Computing; Data Engineering \u2022 Task-specific accuracy or RMSE</p> data pipeline urban science multi-modal benchmark <p>Avg rating: 4.50/5</p> <p>Details</p> Delta Squared-DFT <p>Computational Chemistry; Materials Science \u2022 Mean Absolute Error (eV), Energy ranking accuracy</p> density functional theory Delta Squared-ML correction reaction energetics quantum chemistry <p>Avg rating: 3.83/5</p> <p>Details</p> LLMs for Crop Science <p>Agricultural Science; NLP \u2022 Accuracy, F1 score</p> crop science prompt engineering domain adaptation question answering <p>Avg rating: 0.00/5</p> <p>Details</p> SPIQA (LLM) <p>Multimodal Scientific QA; Computer Vision \u2022 Accuracy, F1 score</p> multimodal QA scientific figures image+text chain-of-thought prompting <p>Avg rating: 4.42/5</p> <p>Details</p>"},{"location":"md/benchmarks/aime_american_invitational_mathematics_examination/","title":"AIME (American Invitational Mathematics Examination)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-03-13</p> <p>Name: AIME  American Invitational Mathematics Examination</p> <p>Domain: Mathematics</p> <p>Focus: Pre-college advanced problem solving</p> <p>Task Types: Problem solving</p> <p>Metrics: Accuracy</p> <p>Models: unkown</p> Keywords algebra combinatorics number theory geometry Citation <ul> <li>TBD. Aime. March 2025. [Online accessed 2025-06-24]. URL: https://www.vals.ai/benchmarks/aime-2025-03-13.</li> </ul> <pre><code>@misc{www-aime,\n  author = {TBD},\n  title = {AIME},\n  url = {https://www.vals.ai/benchmarks/aime-2025-03-13},\n  month = mar,\n  year = 2025,\n  note = {[Online accessed 2025-06-24]}\n}</code></pre> Ratings CategoryRating Software 0.00 No code available  Specification 0.00 Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints  Dataset 4.00 Easily accessible data with problems and solutions, but no splits  Metrics 5.00 (by default) Answer is correct or it's not  Reference Solution 0.00 Not given. Human performance stats exist, but no mentions of AI performance  Documentation 0.00 Not given  Average rating: 1.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/arc-challenge_advanced_reasoning_challenge/","title":"ARC-Challenge (Advanced Reasoning Challenge)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2018-03-14</p> <p>Name: ARC-Challenge  Advanced Reasoning Challenge</p> <p>Domain: Science</p> <p>Focus: Grade-school science with reasoning emphasis</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: GPT-4, Claude</p> Keywords grade-school science QA challenge set reasoning Citation <ul> <li>Peter Clark, Isaac Cowhey, and Oren Etzioni. Think you have solved question answering? try arc, the ai2 reasoning challenge. In EMNLP 2018, 237 248. 2018. URL: https://allenai.org/data/arc.</li> </ul> <pre><code>@inproceedings{clark2018think,\n  title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},\n  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren},\n  booktitle={EMNLP 2018},\n  pages={237-248},\n  year={2018},\n  url={https://allenai.org/data/arc}\n}</code></pre> Ratings CategoryRating Software 0.00 No link to code or documentation  Specification 2.00 Task is clear, but no constraints or format is mentioned  Dataset 4.00 Data accessible, offers instructions on how to download the data via CLI tools. No splits.  Metrics 5.00 (by default) All questions in the dataset are multiple choice, all have a correct answer  Reference Solution 1.00 There are over 300 models listed, but very few, if any, show performance on the dataset or list constraints  Documentation 5.00 Explains all necessary information inside a paper  Average rating: 2.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/baisbench_biological_ai_scientist_benchmark/","title":"BaisBench (Biological AI Scientist Benchmark)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-05-13</p> <p>Name: BaisBench  Biological AI Scientist Benchmark</p> <p>Domain: Computational Biology</p> <p>Focus: Omics-driven AI research tasks</p> <p>Task Types: Cell type annotation, Multiple choice</p> <p>Metrics: Annotation accuracy, QA accuracy</p> <p>Models: LLM-based AI scientist agents</p> Keywords single-cell annotation biological QA autonomous discovery Citation <ul> <li>Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu, Lei Wei, and Xuegong Zhang. Benchmarking ai scientists in omics data-driven biological research. 2025. URL: https://arxiv.org/abs/2505.08341, arXiv:2505.08341.</li> </ul> <pre><code>@misc{luo2025benchmarkingaiscientistsomics,\n  archiveprefix = {arXiv},\n  author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},\n  eprint        = {2505.08341},\n  primaryclass  = {cs.AI},\n  title         = {Benchmarking AI scientists in omics data-driven biological research},\n  url           = {https://arxiv.org/abs/2505.08341},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 5.00 Instructions for environment setup available  Specification 4.00 Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not quantified.  Dataset 5.00 Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.  Metrics 5.00 Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.  Reference Solution 0.00 Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline confirmed yet.  Documentation 5.00 Dataset and paper accessible; IPYNB files for setup are available on the github repo.  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/beam_control/","title":"Beam Control","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-05-01</p> <p>Name: Beam Control</p> <p>Domain: Accelerators and Magnets</p> <p>Focus: Reinforcement learning control of accelerator beam position</p> <p>Task Types: Control</p> <p>Metrics: Stability, Control loss</p> <p>Models: DDPG, PPO (planned)</p> Keywords RL beam stabilization control systems simulation Citation <ul> <li>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.</li> </ul> <pre><code>@misc{duarte2022fastmlsciencebenchmarksaccelerating3,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}</code></pre> <ul> <li>Diana Kafkes and Jason St. John. Boostr: a dataset for accelerator control systems. 2021. URL: https://arxiv.org/abs/2101.08359, arXiv:2101.08359.</li> </ul> <pre><code>@misc{kafkes2021boostrdatasetacceleratorcontrol,\n  archiveprefix = {arXiv},\n  author        = {Diana Kafkes and Jason St. John},\n  eprint        = {2101.08359},\n  primaryclass  = {physics.acc-ph},\n  title         = {BOOSTR: A Dataset for Accelerator Control Systems},\n  url           = {https://arxiv.org/abs/2101.08359},\n  year          = {2021}\n}</code></pre> Ratings CategoryRating Software 1.00 Code not documented; Incomplete setup and not containerized  Specification 4.00 Latency/resource constraints not fully quantified  Dataset 3.00 Not findable (no DOI/indexing); Not interoperable (format/schema unspecified)  Metrics 5.00 All criteria met  Reference Solution 2.00 HW/SW requirements missing; Metrics not evaluated with reference; Baseline not trainable/open  Documentation 3.00 Setup instructions and pretrained model details are missing  Average rating: 3.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/benchcouncil_aibench/","title":"BenchCouncil AIBench","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-01-01</p> <p>Name: BenchCouncil AIBench</p> <p>Domain: General</p> <p>Focus: End-to-end AI benchmarking across micro, component, and application levels</p> <p>Task Types: Training, Inference, End-to-end AI workloads</p> <p>Metrics: Throughput, Latency, Accuracy</p> <p>Models: ResNet, BERT, GANs, Recommendation systems</p> Keywords benchmarking AI systems application-level evaluation Citation <ul> <li>Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and Hainan Ye. Aibench: an industry standard internet service ai benchmark suite. 2019. URL: https://arxiv.org/abs/1908.08998, arXiv:1908.08998.</li> </ul> <pre><code>@misc{gao2019aibenchindustrystandardinternet,\n  archiveprefix = {arXiv},\n  author        = {Wanling Gao and Fei Tang and Lei Wang and Jianfeng Zhan and Chunxin Lan and Chunjie Luo and Yunyou Huang and Chen Zheng and Jiahui Dai and Zheng Cao and Daoyi Zheng and Haoning Tang and Kunlin Zhan and Biao Wang and Defei Kong and Tong Wu and Minghe Yu and Chongkang Tan and Huan Li and Xinhui Tian and Yatao Li and Junchao Shao and Zhenyu Wang and Xiaoyu Wang and Hainan Ye},\n  eprint        = {1908.08998},\n  primaryclass  = {cs.CV},\n  title         = {AIBench: An Industry Standard Internet Service AI Benchmark Suite},\n  url           = {https://arxiv.org/abs/1908.08998},\n  year          = {2019}\n}</code></pre> Ratings CategoryRating Software 3.00 No containerized or automated implementation provided for full benchmark suite  Specification 4.00 Task coverage is broad and well-scoped, but system constraints and expected outputs are not uniformly defined  Dataset 3.00 Multiple datasets are mentioned, but not consistently FAIR-documented, versioned, or linked  Metrics 4.00 Metrics are appropriate, but standardization and reproducibility across tasks vary  Reference Solution 3.00 Reference models (e.g., ResNet, BERT) described; no turnkey implementation or results repository for all levels  Documentation 3.00 Paper is comprehensive, but minimal user-facing documentation or structured reproduction guide  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/benchcouncil_bigdatabench/","title":"BenchCouncil BigDataBench","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-01-01</p> <p>Name: BenchCouncil BigDataBench</p> <p>Domain: General</p> <p>Focus: Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads</p> <p>Task Types: Data preprocessing, Inference, End-to-end data pipelines</p> <p>Metrics: Data throughput, Latency, Accuracy</p> <p>Models: CNN, LSTM, SVM, XGBoost</p> Keywords big data AI benchmarking data analytics Citation <ul> <li>Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao, Shujie Zhang, and Jiahui Dai. Bigdatabench: a scalable and unified big data and ai benchmark suite. 2018. URL: https://arxiv.org/abs/1802.08254, arXiv:1802.08254.</li> </ul> <pre><code>@misc{gao2018bigdatabenchscalableunifiedbig,\n  archiveprefix = {arXiv},\n  author        = {Wanling Gao and Jianfeng Zhan and Lei Wang and Chunjie Luo and Daoyi Zheng and Xu Wen and Rui Ren and Chen Zheng and Xiwen He and Hainan Ye and Haoning Tang and Zheng Cao and Shujie Zhang and Jiahui Dai},\n  eprint        = {1802.08254},\n  primaryclass  = {cs.DC},\n  title         = {BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite},\n  url           = {https://arxiv.org/abs/1802.08254},\n  year          = {2018}\n}</code></pre> Ratings CategoryRating Software 3.00 No automated setup across all tasks; some components require manual integration.  Specification 4.00 Specific I/O formats and hardware constraints are not uniformly detailed across all tasks.  Dataset 4.00 Some datasets lack consistent versioning or rich metadata annotations.  Metrics 5.00 None  Reference Solution 4.00 Not all benchmark components have fully reproducible baselines; deployment across platforms is fragmented.  Documentation 4.00 Setup requires manual steps; some task-specific instructions lack clarity.  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/big-bench_beyond_the_imitation_game_benchmark/","title":"BIG-Bench (Beyond the Imitation Game Benchmark)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-06-09</p> <p>Name: BIG-Bench  Beyond the Imitation Game Benchmark</p> <p>Domain: NLP; AI Evaluation</p> <p>Focus: Diverse reasoning and generalization tasks</p> <p>Task Types: Few-shot evaluation, Multi-task evaluation</p> <p>Metrics: Accuracy, Task-specific metrics</p> <p>Models: GPT-3, Dense Transformers, Sparse Transformers</p> Keywords few-shot multi-task bias analysis Citation <ul> <li>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u015f, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C\u00e9sar Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed Gonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco\u0144, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col\u00f3n, Luke Metz, L\u00fctfi Kerem \u015eenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ram\u00edrez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142 Sw\u0119drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: quantifying and extrapolating the capabilities of language models. 2023. URL: https://arxiv.org/abs/2206.04615, arXiv:2206.04615.</li> </ul> <pre><code>@misc{srivastava2023imitationgamequantifyingextrapolating,\n  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, \n  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri\u00e0 Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlm\u00fcller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka\u015f and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart\u0142omiej Bojanowski and Batuhan \u00d6zyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C\u00e9sar Ferri Ram\u00edrez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu\u00ed Gonz\u00e1lez and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart\u00ednez-Plumed and Francesca Happ\u00e9 and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ\u00e1n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L\u00f3pez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch\u00fctze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern\u00e1ndez Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco\u0144 and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and J\u00f6rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col\u00f3n and Luke Metz and L\u00fctfi Kerem \u015eenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram\u00edrez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M\u00e1ty\u00e1s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha\u0142 Sw\u0119drowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi\u0142kowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Rapha\u00ebl Milli\u00e8re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Th\u00e9o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},\n  year={2023},\n  eprint={2206.04615},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2206.04615}, \n}</code></pre> Ratings CategoryRating Software 4.50 Quick start notebook provided, but instructions on how to run it are lacking.  Specification 4.50 Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized.  Dataset 5.00 Public, versioned, and well-documented; FAIR overall  Metrics 5.00 Many tasks use standard quantitative metrics (accuracy, BLEU, F1). Others involve subjective ratings (e.g., Likert), which reduces cross-task comparability.  Reference Solution 2.00 Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey.  Documentation 5.00 Explained in the associated paper.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/calochallenge_/","title":"CaloChallenge 2022","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-10-28</p> <p>Name: CaloChallenge 2022</p> <p>Domain: LHC Calorimeter; Particle Physics</p> <p>Focus: Fast generative-model-based calorimeter shower simulation evaluation</p> <p>Task Types: Surrogate modeling</p> <p>Metrics: Histogram similarity, Classifier AUC, Generation latency</p> <p>Models: VAE variants, GAN variants, Normalizing flows, Diffusion models</p> Keywords calorimeter simulation generative models surrogate modeling LHC fast simulation Citation <ul> <li>Claudius Krause, Michele Faucci Giannelli, Gregor Kasieczka, Benjamin Nachman, Dalila Salamani, David Shih, Anna Zaborowska, Oz Amram, Kerstin Borras, Matthew R. Buckley, Erik Buhmann, Thorsten Buss, Renato Paulo Da Costa Cardoso, Anthony L. Caterini, Nadezda Chernyavskaya, Federico A. G. Corchia, Jesse C. Cresswell, Sascha Diefenbacher, Etienne Dreyer, Vijay Ekambaram, Engin Eren, Florian Ernst, Luigi Favaro, Matteo Franchini, Frank Gaede, Eilam Gross, Shih-Chieh Hsu, Kristina Jaruskova, Benno K\u00e4ch, Jayant Kalagnanam, Raghav Kansal, Taewoo Kim, Dmitrii Kobylianskii, Anatolii Korol, William Korcari, Dirk Kr\u00fccker, Katja Kr\u00fcger, Marco Letizia, Shu Li, Qibin Liu, Xiulong Liu, Gabriel Loaiza-Ganem, Thandikire Madula, Peter McKeown, Isabell-A. Melzer-Pellmann, Vinicius Mikuni, Nam Nguyen, Ayodele Ore, Sofia Palacios Schweitzer, Ian Pang, Kevin Pedro, Tilman Plehn, Witold Pokorski, Huilin Qu, Piyush Raikwar, John A. Raine, Humberto Reyes-Gonzalez, Lorenzo Rinaldi, Brendan Leigh Ross, Moritz A. W. Scham, Simon Schnake, Chase Shimmin, Eli Shlizerman, Nathalie Soybelman, Mudhakar Srivatsa, Kalliopi Tsolaki, Sofia Vallecorsa, Kyongmin Yeo, and Rui Zhang. Calochallenge 2022: a community challenge for fast calorimeter simulation. 2024. URL: https://arxiv.org/abs/2410.21611, arXiv:2410.21611.</li> </ul> <pre><code>@misc{krause2024calochallenge2022communitychallenge,\n  archiveprefix = {arXiv},\n  author        = {Claudius Krause and Michele Faucci Giannelli and Gregor Kasieczka and Benjamin Nachman and Dalila Salamani and David Shih and Anna Zaborowska and Oz Amram and Kerstin Borras and Matthew R. Buckley and Erik Buhmann and Thorsten Buss and Renato Paulo Da Costa Cardoso and Anthony L. Caterini and Nadezda Chernyavskaya and Federico A. G. Corchia and Jesse C. Cresswell and Sascha Diefenbacher and Etienne Dreyer and Vijay Ekambaram and Engin Eren and Florian Ernst and Luigi Favaro and Matteo Franchini and Frank Gaede and Eilam Gross and Shih-Chieh Hsu and Kristina Jaruskova and Benno K\u00e4ch and Jayant Kalagnanam and Raghav Kansal and Taewoo Kim and Dmitrii Kobylianskii and Anatolii Korol and William Korcari and Dirk Kr\u00fccker and Katja Kr\u00fcger and Marco Letizia and Shu Li and Qibin Liu and Xiulong Liu and Gabriel Loaiza-Ganem and Thandikire Madula and Peter McKeown and Isabell-A. Melzer-Pellmann and Vinicius Mikuni and Nam Nguyen and Ayodele Ore and Sofia Palacios Schweitzer and Ian Pang and Kevin Pedro and Tilman Plehn and Witold Pokorski and Huilin Qu and Piyush Raikwar and John A. Raine and Humberto Reyes-Gonzalez and Lorenzo Rinaldi and Brendan Leigh Ross and Moritz A. W. Scham and Simon Schnake and Chase Shimmin and Eli Shlizerman and Nathalie Soybelman and Mudhakar Srivatsa and Kalliopi Tsolaki and Sofia Vallecorsa and Kyongmin Yeo and Rui Zhang},\n  eprint        = {2410.21611},\n  primaryclass  = {physics.ins-det},\n  title         = {CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation},\n  url           = {https://arxiv.org/abs/2410.21611},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 4.00 Community GitHub repos and model implementations are available for the 31 submissions. While not fully unified in one place, the software is accessible and reproducible.  Specification 5.00 The task\u2014evaluating fast generative calorimeter simulations\u2014is clearly defined with benchmarking protocols, constraints like latency and model complexity, and structured evaluation criteria.  Dataset 5.00 Four well-structured calorimeter datasets are provided, with different voxel resolutions, open access, signal/background separation, and metadata. FAIR principles are well covered.  Metrics 5.00 Metrics like histogram similarity, classifier AUC, and generation latency are well defined and relevant for simulation quality, fidelity, and performance.  Reference Solution 4.00 Several baselines (GANs, VAEs, flows, diffusion models) are documented and evaluated. Some are available via community repos, though not all are fully standardized or bundled.  Documentation 4.00 Accompanied by a detailed paper and dataset description. Reproduction of pipelines may require additional setup or familiarity with the model submissions.  Average rating: 4.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/cfdbench_fluid_dynamics/","title":"CFDBench (Fluid Dynamics)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-10-01</p> <p>Name: CFDBench  Fluid Dynamics</p> <p>Domain: Fluid Dynamics; Scientific ML</p> <p>Focus: Neural operator surrogate modeling</p> <p>Task Types: Surrogate modeling</p> <p>Metrics: L2 error, MAE</p> <p>Models: FNO, DeepONet, U-Net</p> Keywords neural operators CFD FNO DeepONet Citation <ul> <li>Yining Luo, Yingfa Chen, and Zhen Zhang. Cfdbench: a large-scale benchmark for machine learning methods in fluid dynamics. 2024. URL: https://arxiv.org/abs/2310.05963.</li> </ul> <pre><code>@misc{luo2024cfdbenchlargescalebenchmarkmachine,\n  title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},\n  author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},\n  year={2024},\n  url={https://arxiv.org/abs/2310.05963}\n}</code></pre> Ratings CategoryRating Software 5.00 The benchmark provides Python scripts for data loading, preprocessing, and model training/evaluation  Specification 0.00 Not listed  Dataset 0.00 Not given  Metrics 5.00 Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.  Reference Solution 5.00 Baseline models like FNO and DeepONet are implemented, hardware specified.  Documentation 5.00 Associated paper gives all necessary information.  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/climatelearn/","title":"ClimateLearn","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-07-19</p> <p>Name: ClimateLearn</p> <p>Domain: Climate Science; Forecasting</p> <p>Focus: ML for weather and climate modeling</p> <p>Task Types: Forecasting</p> <p>Metrics: RMSE, Anomaly correlation</p> <p>Models: CNN baselines, ResNet variants</p> Keywords medium-range forecasting ERA5 data-driven Citation <ul> <li>Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, and Aditya Grover. Climatelearn: benchmarking machine learning for weather and climate modeling. 2023. URL: https://arxiv.org/abs/2307.01909, arXiv:2307.01909.</li> </ul> <pre><code>@misc{nguyen2023climatelearnbenchmarkingmachinelearning, \n  title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, \n  author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},\n  year={2023}, eprint={2307.01909}, \n  archivePrefix={arXiv}, \n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2307.01909}\n}</code></pre> Ratings CategoryRating Software 5.00 Quickstart notebook makes for easy usage  Specification 5.00 Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.  Dataset 5.00 Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.  Metrics 5.00 ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.  Reference Solution 0.00 The benchmark is geared for CNN architectures, but no specific model was mentioned.  Documentation 5.00 Explained in the benchmark's paper.   Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/codabench/","title":"Codabench","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-01-01</p> <p>Name: Codabench</p> <p>Domain: General ML; Multiple</p> <p>Focus: Open-source platform for organizing reproducible AI benchmarks and competitions</p> <p>Task Types: Multiple</p> <p>Metrics: Submission count, Leaderboard ranking, Task-specific metrics</p> <p>Models: Arbitrary code submissions</p> Keywords benchmark platform code submission competitions meta-benchmark Citation <ul> <li>Zhen Xu, Sergio Escalera, Adrien Pav\u00e3o, Magali Richard, Wei-Wei Tu, Quanming Yao, Huan Zhao, and Isabelle Guyon. Codabench: flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543, July 2022. URL: http://dx.doi.org/10.1016/j.patter.2022.100543, doi:10.1016/j.patter.2022.100543.</li> </ul> <pre><code>@article{xu-2022,\n  author    = {Xu, Zhen and Escalera, Sergio and Pav\u00e3o, Adrien and Richard, Magali and Tu, Wei-Wei and Yao, Quanming and Zhao, Huan and Guyon, Isabelle},\n  doi       = {10.1016/j.patter.2022.100543},\n  issn      = {2666-3899},\n  journal   = {Patterns},\n  month     = jul,\n  number    = {7},\n  pages     = {100543},\n  publisher = {Elsevier BV},\n  title     = {Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},\n  url       = {http://dx.doi.org/10.1016/j.patter.2022.100543},\n  volume    = {3},\n  year      = {2022}\n}</code></pre> Ratings CategoryRating Software 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Specification 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Dataset 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Metrics 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Reference Solution 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Documentation 1.00 This is a platform for posting benchmarks, not a benchmark in itself.  Average rating: 1.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/commonsenseqa/","title":"CommonSenseQA","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2019-11-20</p> <p>Name: CommonSenseQA</p> <p>Domain: NLP; Commonsense</p> <p>Focus: Commonsense question answering</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: BERT-large, RoBERTa, GPT-3</p> Keywords ConceptNet multiple-choice adversarial Citation <ul> <li>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: a question answering challenge targeting commonsense knowledge. 2019. URL: https://arxiv.org/abs/1811.00937, arXiv:1811.00937.</li> </ul> <pre><code>@misc{talmor2019commonsenseqaquestionansweringchallenge,\n  title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, \n  author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},\n  year={2019},\n  eprint={1811.00937},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/1811.00937}, \n}</code></pre> Ratings CategoryRating Software 5.00 All code given on Github site  Specification 4.00 Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.  Dataset 5.00 Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.  Metrics 5.00 Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.  Reference Solution 4.00 Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not run with hardware constraints  Documentation 5.00 Given in paper.  Average rating: 4.67/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/curie_scientific_long-context_understanding_reasoning_and_information_extraction/","title":"CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-04-02</p> <p>Name: CURIE  Scientific Long-Context Understanding, Reasoning and Information Extraction</p> <p>Domain: Multidomain Science</p> <p>Focus: Long-context scientific reasoning</p> <p>Task Types: Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension</p> <p>Metrics: Accuracy</p> <p>Models: unkown</p> Keywords long-context information extraction multimodal Citation <ul> <li>Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, and Subhashini Venugopalan. Curie: evaluating llms on multitask scientific long context understanding and reasoning. 2025. URL: https://arxiv.org/abs/2503.13517, arXiv:2503.13517.</li> </ul> <pre><code>@misc{cui2025curieevaluatingllmsmultitask,\n  title={CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning}, \n  author={Hao Cui and Zahra Shamsi and Gowoon Cheon and Xuejian Ma and Shutong Li and Maria Tikhanovskaya and Peter Norgaard and Nayantara Mudur and Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and Victor V. Albert and Pranesh Srinivasan and Haining Pan and Philippe Faist and Brian Rohr and Ekin Dogus Cubuk and Muratahan Aykol and Amil Merchant and Michael J. Statt and Dan Morris and Drew Purves and Elise Kleeman and Ruth Alcantara and Matthew Abraham and Muqthar Mohammad and Ean Phing VanLee and Chenfei Jiang and Elizabeth Dorfman and Eun-Ah Kim and Michael P Brenner and Viren Jain and Sameera Ponda and Subhashini Venugopalan},\n  year={2025},\n  eprint={2503.13517},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2503.13517}, \n}</code></pre> Ratings CategoryRating Software 4.00 Code is available, but not well documented  Specification 1.00 Explains types of problems in detail, but does not state exactly how to administer them.  Dataset 4.00 Dataset is available via Github, but hard to find  Metrics 5.00 Quantitiative metrics such as ROUGE-L and F1 used. Metrics are tailored to the specific problem.  Reference Solution 1.00 Exists, but is not open  Documentation 5.00 Associated paper explains all criteria  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/d-stem/","title":"4D-STEM","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-12-03</p> <p>Name: 4D-STEM</p> <p>Domain: Material Science</p> <p>Focus: Real-time ML for scanning transmission electron microscopy</p> <p>Task Types: Image Classification, Streamed data inference</p> <p>Metrics: Classification accuracy, Throughput</p> <p>Models: CNN models (prototype)</p> Keywords 4D-STEM electron microscopy real-time image processing Citation <ul> <li>Shuyu Qin, Joshua Agar, and Nhan Tran. Extremely noisy 4d-tem strain mapping using cycle consistent spatial transforming autoencoders. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop. 2023. URL: https://openreview.net/forum?id=7yt3N0o0W9.</li> </ul> <pre><code>@inproceedings{qin2023extremely,\n  title={Extremely Noisy 4D-TEM Strain Mapping Using Cycle Consistent Spatial Transforming Autoencoders},\n  author={Shuyu Qin and Joshua Agar and Nhan Tran},\n  booktitle={AI for Accelerated Materials Design - NeurIPS 2023 Workshop},\n  year={2023},\n  url={https://openreview.net/forum?id=7yt3N0o0W9}\n}</code></pre> Ratings CategoryRating Software 2.00 No standalone code repository or setup instructions provided  Specification 5.00 None  Dataset 2.00 No dataset links or FAIR metadata; unclear public access  Metrics 4.00 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts  Reference Solution 3.00 BraggNN model is described and evaluated, but no direct implementation or inference scripts available  Documentation 3.00 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline  Average rating: 3.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/delta_squared-dft/","title":"Delta Squared-DFT","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: Delta Squared-DFT</p> <p>Domain: Computational Chemistry; Materials Science</p> <p>Focus: Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies</p> <p>Task Types: Regression</p> <p>Metrics: Mean Absolute Error (eV), Energy ranking accuracy</p> <p>Models: Delta Squared-ML correction networks, Kernel ridge regression</p> Keywords density functional theory Delta Squared-ML correction reaction energetics quantum chemistry Citation <ul> <li>Kuzma Khrabrov, Anton Ber, Artem Tsypin, Konstantin Ushenin, Egor Rumiantsev, Alexander Telepov, Dmitry Protasov, Ilya Shenbin, Anton Alekseev, Mikhail Shirokikh, Sergey Nikolenko, Elena Tutubalina, and Artur Kadurin. Delta-squared dft: a universal quantum chemistry dataset of drug-like molecules and a benchmark for neural network potentials. 2024. URL: https://arxiv.org/abs/2406.14347, arXiv:2406.14347.</li> </ul> <pre><code>@misc{khrabrov2024nabla2dftuniversalquantumchemistry,\n  title={Delta-Squared DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials}, \n  author={Kuzma Khrabrov and Anton Ber and Artem Tsypin and Konstantin Ushenin and Egor Rumiantsev and Alexander Telepov and Dmitry Protasov and Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and Sergey Nikolenko and Elena Tutubalina and Artur Kadurin},\n  year={2024},\n  eprint={2406.14347},\n  archivePrefix={arXiv},\n  primaryClass={physics.chem-ph},\n  url={https://arxiv.org/abs/2406.14347}, \n}</code></pre> Ratings CategoryRating Software 3.00 Source code and baseline models available for ML correction to DFT; framework maturity is moderate.  Specification 4.00 Benchmark focuses on reaction energy prediction with clear goals, though some task specifics could be formalized further.  Dataset 4.50 Multi-modal quantum chemistry datasets are standardized and accessible; repository available.  Metrics 4.00 Uses standard regression metrics like MAE and energy ranking accuracy; appropriate for task.  Reference Solution 3.50 Includes baseline regression and kernel ridge models; implementations are reproducible.  Documentation 4.00 Source code supports pipeline reuse, but formal evaluation splits may vary.  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/dune/","title":"DUNE","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-10-15</p> <p>Name: DUNE</p> <p>Domain: Particle Physics</p> <p>Focus: Real-time ML for DUNE DAQ time-series data</p> <p>Task Types: Trigger selection, Time-series anomaly detection</p> <p>Metrics: Detection efficiency, Latency</p> <p>Models: CNN, LSTM (planned)</p> Keywords DUNE time-series real-time trigger Citation <ul> <li>A. Abed Abud, B. Abi, R. Acciarri, M. A. Acero, G. Adamov, D. Adams, M. Adinolfi, A. Aduszkiewicz, Z. Ahmad, J. Ahmed, T. Alion, S. Alonso Monsalve, M. Alrashed, C. Alt, A. Alton, P. Amedo, J. Anderson, C. Andreopoulos, M. P. Andrews, F. Andrianala, S. Andringa, N. Anfimov, A. Ankowski, M. Antonova, S. Antusch, A. Aranda-Fernandez, A. Ariga, L. O. Arnold, M. A. Arroyave, J. Asaadi, A. Aurisano, V. Aushev, D. Autiero, M. Ayala-Torres, F. Azfar, H. Back, J. J. Back, C. Backhouse, P. Baesso, I. Bagaturia, L. Bagby, S. Balasubramanian, P. Baldi, B. Baller, B. Bambah, F. Barao, G. Barenboim, G. J. Barker, W. Barkhouse, C. Barnes, G. Barr, J. Barranco Monarca, N. Barros, J. L. Barrow, A. Basharina-Freshville, A. Bashyal, V. Basque, E. Belchior, J. B. R. Battat, F. Battisti, F. Bay, J. L. Bazo Alba, J. F. Beacom, E. Bechetoille, B. Behera, L. Bellantoni, G. Bellettini, V. Bellini, O. Beltramello, D. Belver, N. Benekos, F. Bento Neves, S. Berkman, P. Bernardini, R. M. Berner, H. Berns, S. Bertolucci, M. Betancourt, A. Betancur Rodr\u00edguez, M. Bhattacharjee, S. Bhuller, B. Bhuyan, S. Biagi, J. Bian, M. Biassoni, K. Biery, B. Bilki, M. Bishai, A. Bitadze, A. Blake, F. D. M. Blaszczyk, G. C. Blazey, E. Blucher, J. Boissevain, S. Bolognesi, T. Bolton, L. Bomben, M. Bonesini, M. Bongrand, F. Bonini, A. Booth, C. Booth, S. Bordoni, A. Borkum, T. Boschi, N. Bostan, P. Bour, C. Bourgeois, S. B. Boyd, D. Boyden, J. Bracinik, D. Braga, D. Brailsford, A. Brandt, J. Bremer, C. Brew, E. Brianne, S. J. Brice, C. Brizzolari, C. Bromberg, G. Brooijmans, J. Brooke, A. Bross, G. Brunetti, M. Brunetti, N. Buchanan, H. Budd, D. Caiulo, P. Calafiura, J. Calcutt, M. Calin, S. Calvez, E. Calvo, A. Caminata, M. Campanelli, K. Cankocak, D. Caratelli, G. Carini, B. Carlus, P. Carniti, I. Caro Terrazas, H. Carranza, T. Carroll, J. F. Casta\u00f1o Forero, A. Castillo, C. Castromonte, E. Catano-Mur, C. Cattadori, F. Cavalier, F. Cavanna, S. Centro, G. Cerati, A. Cervelli, A. Cervera Villanueva, M. Chalifour, A. Chappell, E. Chardonnet, N. Charitonidis, A. Chatterjee, S. Chattopadhyay, H. Chen, M. Chen, Y. Chen, Z. Chen, D. Cherdack, C. Chi, S. Childress, A. Chiriacescu, G. Chisnall, K. Cho, S. Choate, D. Chokheli, S. Choubey, A. Christensen, D. Christian, G. Christodoulou, A. Chukanov, E. Church, P. Clarke, T. E. Coan, A. G. Cocco, J. A. B. Coelho, E. Conley, R. Conley, J. M. Conrad, M. Convery, S. Copello, L. Corwin, L. Cremaldi, L. Cremonesi, J. I. Crespo-Anad\u00f3n, E. Cristaldo, R. Cross, A. Cudd, C. Cuesta, Y. Cui, D. Cussans, M. Dabrowski, O. Dalager, H. da Motta, L. Da Silva Peres, C. David, Q. David, G. S. Davies, S. Davini, J. Dawson, K. De, R. M. De Almeida, P. Debbins, I. De Bonis, M. P. Decowski, A. de Gouv\u00eaa, P. C. De Holanda, I. L. De Icaza Astiz, A. Deisting, P. De Jong, A. Delbart, D. Delepine, M. Delgado, A. Dell'Acqua, P. De Lurgio, J. R. T. de Mello Neto, D. M. DeMuth, S. Dennis, C. Densham, G. W. Deptuch, A. De Roeck, V. De Romeri, G. De Souza, R. Dharmapalan, F. Diaz, J. S. D\u00edaz, S. Di Domizio, L. Di Giulio, P. Ding, L. Di Noto, C. Distefano, R. Diurba, M. Diwan, Z. Djurcic, N. Dokania, S. Dolan, M. J. Dolinski, L. Domine, D. Douglas, D. Douillet, G. Drake, F. Drielsma, D. Duchesneau, K. Duffy, P. Dunne, T. Durkin, H. Duyang, O. Dvornikov, D. A. Dwyer, A. S. Dyshkant, M. Eads, A. Earle, D. Edmunds, J. Eisch, L. Emberger, S. Emery, A. Ereditato, C. O. Escobar, G. Eurin, J. J. Evans, E. Ewart, A. C. Ezeribe, K. Fahey, A. Falcone, C. Farnese, Y. Farzan, J. Felix, M. Fernandes Carneiro da Silva, E. Fernandez-Martinez, P. Fernandez Menendez, F. Ferraro, L. Fields, F. Filthaut, A. Fiorentini, R. S. Fitzpatrick, W. Flanagan, B. Fleming, R. Flight, D. V. Forero, J. Fowler, W. Fox, J. Franc, K. Francis, D. Franco, J. Freeman, J. Freestone, J. Fried, A. Friedland, S. Fuess, I. Furic, A. P. Furmanski, A. Gago, H. Gallagher, A. Gallas, A. Gallego-Ros, N. Gallice, V. Galymov, E. Gamberini, T. Gamble, R. Gandhi, R. Gandrajula, F. Gao, S. Gao, D. Garcia-Gamez, M. \u00c1 Garc\u00eda-Peris, S. Gardiner, D. Gastler, G. Ge, B. Gelli, A. Gendotti, S. Gent, Z. Ghorbani-Moghaddam, D. Gibin, I. Gil-Botella, S. Gilligan, C. Girerd, A. K. Giri, D. Gnani, O. Gogota, M. Gold, S. Gollapinni, K. Gollwitzer, R. A. Gomes, L. V. Gomez Bermeo, L. S. Gomez Fajardo, F. Gonnella, J. A. Gonzalez-Cuevas, D. Gonzalez-Diaz, M. Gonzalez-Lopez, M. C. Goodman, O. Goodwin, S. Goswami, C. Gotti, E. Goudzovski, C. Grace, M. Graham, R. Gran, E. Granados, P. Granger, A. Grant, C. Grant, D. Gratieri, P. Green, L. Greenler, J. Greer, W. C. Griffith, M. Groh, J. Grudzinski, K. Grzelak, W. Gu, V. Guarino, R. Guenette, E. Guerard, A. Guglielmi, B. Guo, K. K. Guthikonda, R. Gutierrez, P. Guzowski, M. M. Guzzo, S. Gwon, A. Habig, H. Hadavand, R. Haenni, A. Hahn, J. Haiston, P. Hamacher-Baumann, T. Hamernik, P. Hamilton, J. Han, D. A. Harris, J. Hartnell, J. Harton, T. Hasegawa, C. Hasnip, R. Hatcher, K. W. Hatfield, A. Hatzikoutelis, C. Hayes, E. Hazen, A. Heavey, K. M. Heeger, J. Heise, K. Hennessy, S. Henry, M. A. Hernandez Morquecho, K. Herner, L. Hertel, V Hewes, A. Higuera, T. Hill, S. J. Hillier, A. Himmel, J. Hoff, C. Hohl, A. Holin, E. Hoppe, G. A. Horton-Smith, M. Hostert, A. Hourlier, B. Howard, R. Howell, J. Huang, J. Huang, J. Hugon, G. Iles, N. Ilic, A. M. Iliescu, R. Illingworth, A. Ioannisian, L. Isenhower, R. Itay, A. Izmaylov, S. Jackson, V. Jain, E. James, B. Jargowsky, F. Jediny, D. Jena, Y. S. Jeong, C. Jes\u00fas-Valls, X. Ji, L. Jiang, S. Jim\u00e9nez, A. Jipa, R. Johnson, B. Jones, S. B. Jones, M. Judah, C. K. Jung, T. Junk, Y. Jwa, M. Kabirnezhad, A. Kaboth, I. Kadenko, I. Kakorin, F. Kamiya, N. Kaneshige, G. Karagiorgi, G. Karaman, A. Karcher, M. Karolak, Y. Karyotakis, S. Kasai, S. P. Kasetti, L. Kashur, N. Kazaryan, E. Kearns, P. Keener, K. J. Kelly, E. Kemp, O. Kemularia, W. Ketchum, S. H. Kettell, M. Khabibullin, A. Khotjantsev, A. Khvedelidze, D. Kim, B. King, B. Kirby, M. Kirby, J. Klein, K. Koehler, L. W. Koerner, S. Kohn, P. P. Koller, L. Kolupaeva, M. Kordosky, T. Kosc, U. Kose, V. A. Kosteleck\u00fd, K. Kothekar, F. Krennrich, I. Kreslo, Y. Kudenko, V. A. Kudryavtsev, S. Kulagin, J. Kumar, P. Kumar, P. Kunze, N. Kurita, C. Kuruppu, V. Kus, T. Kutter, A. Lambert, B. Land, K. Lande, C. E. Lane, K. Lang, T. Langford, J. Larkin, P. Lasorak, D. Last, C. Lastoria, A. Laundrie, A. Lawrence, I. Lazanu, R. LaZur, T. Le, S. Leardini, J. Learned, P. LeBrun, T. LeCompte, G. Lehmann Miotto, R. Lehnert, M. A. Leigui de Oliveira, M. Leitner, L. Li, S. W. Li, T. Li, Y. Li, H. Liao, C. S. Lin, Q. Lin, S. Lin, A. Lister, B. R. Littlejohn, J. Liu, S. Lockwitz, T. Loew, M. Lokajicek, I. Lomidze, K. Long, K. Loo, D. Lorca, T. Lord, J. M. LoSecco, W. C. Louis, X. -G. Lu, K. B. Luk, X. Luo, N. Lurkin, T. Lux, V. P. Luzio, D. MacFarlane, A. A. Machado, P. Machado, C. T. Macias, J. R. Macier, A. Maddalena, A. Madera, P. Madigan, S. Magill, K. Mahn, A. Maio, A. Major, J. A. Maloney, G. Mandrioli, R. C. Mandujano, J. Maneira, L. Manenti, S. Manly, A. Mann, K. Manolopoulos, M. Manrique Plata, V. N. Manyam, L. Manzanillas, M. Marchan, A. Marchionni, W. Marciano, D. Marfatia, C. Mariani, J. Maricic, R. Marie, F. Marinho, A. D. Marino, D. Marsden, M. Marshak, C. M. Marshall, J. Marshall, J. Marteau, J. Martin-Albo, N. Martinez, D. A. Martinez Caicedo, S. Martynenko, K. Mason, A. Mastbaum, M. Masud, S. Matsuno, J. Matthews, C. Mauger, N. Mauri, K. Mavrokoridis, I. Mawby, R. Mazza, A. Mazzacane, E. Mazzucato, T. McAskill, E. McCluskey, N. McConkey, K. S. McFarland, C. McGrew, A. McNab, A. Mefodiev, P. Mehta, P. Melas, O. Mena, S. Menary, H. Mendez, D. P. M\u00e9ndez, A. Menegolli, G. Meng, M. D. Messier, W. Metcalf, T. Mettler, M. Mewes, H. Meyer, T. Miao, G. Michna, T. Miedema, J. Migenda, V. Mikola, R. Milincic, W. Miller, J. Mills, C. Milne, O. Mineev, O. G. Miranda, S. Miryala, C. S. Mishra, S. R. Mishra, A. Mislivec, D. Mladenov, I. Mocioiu, K. Moffat, N. Moggi, R. Mohanta, T. A. Mohayai, N. Mokhov, J. Molina, L. Molina Bueno, A. Montanari, C. Montanari, D. Montanari, L. M. Montano Zetina, J. Moon, M. Mooney, A. F. Moor, D. Moreno, C. Morris, C. Mossey, E. Motuk, C. A. Moura, J. Mousseau, W. Mu, L. Mualem, J. Mueller, M. Muether, S. Mufson, F. Muheim, A. Muir, M. Mulhearn, D. Munford, H. Muramatsu, S. Murphy, J. Musser, J. Nachtman, S. Nagu, M. Nalbandyan, R. Nandakumar, D. Naples, S. Narita, D. Navas-Nicol\u00e1s, A. Navrer-Agasson, N. Nayak, M. Nebot-Guinot, K. Negishi, J. K. Nelson, J. Nesbit, M. Nessi, D. Newbold, M. Newcomer, D. Newhart, H. Newton, R. Nichol, F. Nicolas-Arnaldos, E. Niner, K. Nishimura, A. Norman, A. Norrick, R. Northrop, P. Novella, J. A. Nowak, M. Oberling, J. P. Ochoa-Ricoux, A. Olivares Del Campo, A. Olivier, A. Olshevskiy, Y. Onel, Y. Onishchuk, J. Ott, L. Pagani, S. Pakvasa, G. Palacio, O. Palamara, S. Palestini, J. M. Paley, M. Pallavicini, C. Palomares, J. L. Palomino-Gallo, E. Pantic, V. Paolone, V. Papadimitriou, R. Papaleo, A. Papanestis, S. Paramesvaran, S. Parke, Z. Parsa, M. Parvu, S. Pascoli, L. Pasqualini, J. Pasternak, J. Pater, C. Patrick, L. Patrizii, R. B. Patterson, S. J. Patton, T. Patzak, A. Paudel, B. Paulos, L. Paulucci, Z. Pavlovic, G. Pawloski, D. Payne, V. Pec, S. J. M. Peeters, E. Pennacchio, A. Penzo, O. L. G. Peres, J. Perry, D. Pershey, G. Pessina, G. Petrillo, C. Petta, R. Petti, F. Piastra, L. Pickering, F. Pietropaolo, R. Plunkett, R. Poling, X. Pons, N. Poonthottathil, S. Pordes, J. Porter, M. Potekhin, R. Potenza, B. V. K. S. Potukuchi, J. Pozimski, M. Pozzato, S. Prakash, T. Prakash, S. Prince, D. Pugnere, X. Qian, M. C. Queiroga Bazetto, J. L. Raaf, V. Radeka, J. Rademacker, B. Radics, A. Rafique, E. Raguzin, M. Rai, M. Rajaoalisoa, I. Rakhno, A. Rakotonandrasana, L. Rakotondravohitra, Y. A. Ramachers, R. Rameika, M. A. Ramirez Delgado, B. Ramson, A. Rappoldi, G. Raselli, P. Ratoff, S. Raut, R. F. Razakamiandra, J. S. Real, B. Rebel, M. Reggiani-Guzzo, T. Rehak, J. Reichenbacher, S. D. Reitzner, H. Rejeb Sfar, A. Renshaw, S. Rescia, F. Resnati, A. Reynolds, C. Riccio, G. Riccobene, L. C. J. Rice, J. Ricol, A. Rigamonti, Y. Rigaut, D. Rivera, L. Rochester, M. Roda, P. Rodrigues, M. J. Rodriguez Alonso, E. Rodriguez Bonilla, J. Rodriguez Rondon, S. Rosauro-Alcaraz, M. Rosenberg, P. Rosier, B. Roskovec, M. Rossella, J. Rout, P. Roy, S. Roy, A. Rubbia, C. Rubbia, F. C. Rubio, B. Russell, D. Ruterbories, R. Saakyan, S. Sacerdoti, T. Safford, R. Sahay, N. Sahu, P. Sala, N. Samios, O. Samoylov, M. C. Sanchez, D. A. Sanders, D. Sankey, S. Santana, M. Santos-Maldonado, N. Saoulidou, P. Sapienza, C. Sarasty, I. Sarcevic, G. Savage, V. Savinov, A. Scaramelli, A. Scarff, A. Scarpelli, T. Schaffer, H. Schellman, P. Schlabach, D. Schmitz, K. Scholberg, A. Schukraft, E. Segreto, J. Sensenig, I. Seong, A. Sergi, D. Sgalaberna, M. H. Shaevitz, S. Shafaq, M. Shamma, R. Sharankova, H. R. Sharma, R. Sharma, R. Kumar, T. Shaw, C. Shepherd-Themistocleous, S. Shin, D. Shooltz, R. Shrock, L. Simard, F. Simon, N. Simos, J. Sinclair, G. Sinev, J. Singh, J. Singh, V. Singh, R. Sipos, F. W. Sippach, G. Sirri, A. Sitraka, K. Siyeon, K. Skarpaas VIII, A. Smith, E. Smith, P. Smith, J. Smolik, M. Smy, E. L. Snider, P. Snopok, M. Soares Nunes, H. Sobel, M. Soderberg, C. J. Solano Salinas, S. S\u00f6ldner-Rembold, N. Solomey, V. Solovov, W. E. Sondheim, M. Sorel, J. Soto-Oton, A. Sousa, K. Soustruznik, F. Spagliardi, M. Spanu, J. Spitz, N. J. C. Spooner, K. Spurgeon, R. Staley, M. Stancari, L. Stanco, R. Stanley, R. Stein, H. M. Steiner, J. Stewart, B. Stillwell, J. Stock, F. Stocker, T. Stokes, M. Strait, T. Strauss, S. Striganov, A. Stuart, J. G. Suarez, H. Sullivan, D. Summers, A. Surdo, V. Susic, L. Suter, C. M. Sutera, R. Svoboda, B. Szczerbinska, A. M. Szelc, R. Talaga, H. A. Tanaka, B. Tapia Oregui, A. Tapper, S. Tariq, E. Tatar, R. Tayloe, A. M. Teklu, M. Tenti, K. Terao, C. A. Ternes, F. Terranova, G. Testera, A. Thea, J. L. Thompson, C. Thorn, S. C. Timm, J. Todd, A. Tonazzo, D. Torbunov, M. Torti, M. Tortola, F. Tortorici, D. Totani, M. Toups, C. Touramanis, J. Trevor, S. Trilov, W. H. Trzaska, Y. T. Tsai, Z. Tsamalaidze, K. V. Tsang, N. Tsverava, S. Tufanli, C. Tull, E. Tyley, M. Tzanov, M. A. Uchida, J. Urheim, T. Usher, S. Uzunyan, M. R. Vagins, P. Vahle, G. A. Valdiviesso, E. Valencia, Z. Vallari, J. W. F. Valle, S. Vallecorsa, R. Van Berg, R. G. Van de Water, F. Varanini, D. Vargas, G. Varner, J. Vasel, S. Vasina, G. Vasseur, N. Vaughan, K. Vaziri, S. Ventura, A. Verdugo, S. Vergani, M. A. Vermeulen, M. Verzocchi, M. Vicenzi, H. Vieira de Souza, C. Vignoli, C. Vilela, B. Viren, T. Vrba, T. Wachala, A. V. Waldron, M. Wallbank, H. Wang, J. Wang, M. H. L. S. Wang, Y. Wang, Y. Wang, K. Warburton, D. Warner, M. Wascko, D. Waters, A. Watson, P. Weatherly, A. Weber, M. Weber, H. Wei, A. Weinstein, D. Wenman, M. Wetstein, A. White, L. H. Whitehead, D. Whittington, M. J. Wilking, C. Wilkinson, Z. Williams, F. Wilson, R. J. Wilson, J. Wolcott, T. Wongjirad, A. Wood, K. Wood, E. Worcester, M. Worcester, C. Wret, W. Wu, W. Wu, Y. Xiao, E. Yandel, G. Yang, K. Yang, S. Yang, T. Yang, A. Yankelevich, N. Yershov, K. Yonehara, T. Young, B. Yu, H. Yu, J. Yu, W. Yuan, R. Zaki, J. Zalesak, L. Zambelli, B. Zamorano, A. Zani, L. Zazueta, G. Zeit, G. P. Zeller, J. Zennamo, K. Zeug, C. Zhang, M. Zhao, E. Zhivun, G. Zhu, P. Zilberman, E. D. Zimmerman, M. Zito, S. Zucchelli, J. Zuklin, V. Zutshi, and R. Zwaska. Deep underground neutrino experiment (dune) near detector conceptual design report. 2021. URL: https://arxiv.org/abs/2103.13910, arXiv:2103.13910.</li> </ul> <pre><code>@misc{abud2021deep,\n  title={Deep Underground Neutrino Experiment (DUNE) Near Detector Conceptual Design Report}, \n  author={A. Abed Abud and B. Abi and R. Acciarri and M. A. Acero and G. Adamov and D. Adams and M. Adinolfi and A. Aduszkiewicz and Z. Ahmad and J. Ahmed and T. Alion and S. Alonso Monsalve and M. Alrashed and C. Alt and A. Alton and P. Amedo and J. Anderson and C. Andreopoulos and M. P. Andrews and F. Andrianala and S. Andringa and N. Anfimov and A. Ankowski and M. Antonova and S. Antusch and A. Aranda-Fernandez and A. Ariga and L. O. Arnold and M. A. Arroyave and J. Asaadi and A. Aurisano and V. Aushev and D. Autiero and M. Ayala-Torres and F. Azfar and H. Back and J. J. Back and C. Backhouse and P. Baesso and I. Bagaturia and L. Bagby and S. Balasubramanian and P. Baldi and B. Baller and B. Bambah and F. Barao and G. Barenboim and G. J. Barker and W. Barkhouse and C. Barnes and G. Barr and J. Barranco Monarca and N. Barros and J. L. Barrow and A. Basharina-Freshville and A. Bashyal and V. Basque and E. Belchior and J. B. R. Battat and F. Battisti and F. Bay and J. L. Bazo Alba and J. F. Beacom and E. Bechetoille and B. Behera and L. Bellantoni and G. Bellettini and V. Bellini and O. Beltramello and D. Belver and N. Benekos and F. Bento Neves and S. Berkman and P. Bernardini and R. M. Berner and H. Berns and S. Bertolucci and M. Betancourt and A. Betancur Rodr\u00edguez and M. Bhattacharjee and S. Bhuller and B. Bhuyan and S. Biagi and J. Bian and M. Biassoni and K. Biery and B. Bilki and M. Bishai and A. Bitadze and A. Blake and F. D. M. Blaszczyk and G. C. Blazey and E. Blucher and J. Boissevain and S. Bolognesi and T. Bolton and L. Bomben and M. Bonesini and M. Bongrand and F. Bonini and A. Booth and C. Booth and S. Bordoni and A. Borkum and T. Boschi and N. Bostan and P. Bour and C. Bourgeois and S. B. Boyd and D. Boyden and J. Bracinik and D. Braga and D. Brailsford and A. Brandt and J. Bremer and C. Brew and E. Brianne and S. J. Brice and C. Brizzolari and C. Bromberg and G. Brooijmans and J. Brooke and A. Bross and G. Brunetti and M. Brunetti and N. Buchanan and H. Budd and D. Caiulo and P. Calafiura and J. Calcutt and M. Calin and S. Calvez and E. Calvo and A. Caminata and M. Campanelli and K. Cankocak and D. Caratelli and G. Carini and B. Carlus and P. Carniti and I. Caro Terrazas and H. Carranza and T. Carroll and J. F. Casta\u00f1o Forero and A. Castillo and C. Castromonte and E. Catano-Mur and C. Cattadori and F. Cavalier and F. Cavanna and S. Centro and G. Cerati and A. Cervelli and A. Cervera Villanueva and M. Chalifour and A. Chappell and E. Chardonnet and N. Charitonidis and A. Chatterjee and S. Chattopadhyay and H. Chen and M. Chen and Y. Chen and Z. Chen and D. Cherdack and C. Chi and S. Childress and A. Chiriacescu and G. Chisnall and K. Cho and S. Choate and D. Chokheli and S. Choubey and A. Christensen and D. Christian and G. Christodoulou and A. Chukanov and E. Church and P. Clarke and T. E. Coan and A. G. Cocco and J. A. B. Coelho and E. Conley and R. Conley and J. M. Conrad and M. Convery and S. Copello and L. Corwin and L. Cremaldi and L. Cremonesi and J. I. Crespo-Anad\u00f3n and E. Cristaldo and R. Cross and A. Cudd and C. Cuesta and Y. Cui and D. Cussans and M. Dabrowski and O. Dalager and H. da Motta and L. Da Silva Peres and C. David and Q. David and G. S. Davies and S. Davini and J. Dawson and K. De and R. M. De Almeida and P. Debbins and I. De Bonis and M. P. Decowski and A. de Gouv\u00eaa and P. C. De Holanda and I. L. De Icaza Astiz and A. Deisting and P. De Jong and A. Delbart and D. Delepine and M. Delgado and A. Dell'Acqua and P. De Lurgio and J. R. T. de Mello Neto and D. M. DeMuth and S. Dennis and C. Densham and G. W. Deptuch and A. De Roeck and V. De Romeri and G. De Souza and R. Dharmapalan and F. Diaz and J. S. D\u00edaz and S. Di Domizio and L. Di Giulio and P. Ding and L. Di Noto and C. Distefano and R. Diurba and M. Diwan and Z. Djurcic and N. Dokania and S. Dolan and M. J. Dolinski and L. Domine and D. Douglas and D. Douillet and G. Drake and F. Drielsma and D. Duchesneau and K. Duffy and P. Dunne and T. Durkin and H. Duyang and O. Dvornikov and D. A. Dwyer and A. S. Dyshkant and M. Eads and A. Earle and D. Edmunds and J. Eisch and L. Emberger and S. Emery and A. Ereditato and C. O. Escobar and G. Eurin and J. J. Evans and E. Ewart and A. C. Ezeribe and K. Fahey and A. Falcone and C. Farnese and Y. Farzan and J. Felix and M. Fernandes Carneiro da Silva and E. Fernandez-Martinez and P. Fernandez Menendez and F. Ferraro and L. Fields and F. Filthaut and A. Fiorentini and R. S. Fitzpatrick and W. Flanagan and B. Fleming and R. Flight and D. V. Forero and J. Fowler and W. Fox and J. Franc and K. Francis and D. Franco and J. Freeman and J. Freestone and J. Fried and A. Friedland and S. Fuess and I. Furic and A. P. Furmanski and A. Gago and H. Gallagher and A. Gallas and A. Gallego-Ros and N. Gallice and V. Galymov and E. Gamberini and T. Gamble and R. Gandhi and R. Gandrajula and F. Gao and S. Gao and D. Garcia-Gamez and M. \u00c1 Garc\u00eda-Peris and S. Gardiner and D. Gastler and G. Ge and B. Gelli and A. Gendotti and S. Gent and Z. Ghorbani-Moghaddam and D. Gibin and I. Gil-Botella and S. Gilligan and C. Girerd and A. K. Giri and D. Gnani and O. Gogota and M. Gold and S. Gollapinni and K. Gollwitzer and R. A. Gomes and L. V. Gomez Bermeo and L. S. Gomez Fajardo and F. Gonnella and J. A. Gonzalez-Cuevas and D. Gonzalez-Diaz and M. Gonzalez-Lopez and M. C. Goodman and O. Goodwin and S. Goswami and C. Gotti and E. Goudzovski and C. Grace and M. Graham and R. Gran and E. Granados and P. Granger and A. Grant and C. Grant and D. Gratieri and P. Green and L. Greenler and J. Greer and W. C. Griffith and M. Groh and J. Grudzinski and K. Grzelak and W. Gu and V. Guarino and R. Guenette and E. Guerard and A. Guglielmi and B. Guo and K. K. Guthikonda and R. Gutierrez and P. Guzowski and M. M. Guzzo and S. Gwon and A. Habig and H. Hadavand and R. Haenni and A. Hahn and J. Haiston and P. Hamacher-Baumann and T. Hamernik and P. Hamilton and J. Han and D. A. Harris and J. Hartnell and J. Harton and T. Hasegawa and C. Hasnip and R. Hatcher and K. W. Hatfield and A. Hatzikoutelis and C. Hayes and E. Hazen and A. Heavey and K. M. Heeger and J. Heise and K. Hennessy and S. Henry and M. A. Hernandez Morquecho and K. Herner and L. Hertel and V Hewes and A. Higuera and T. Hill and S. J. Hillier and A. Himmel and J. Hoff and C. Hohl and A. Holin and E. Hoppe and G. A. Horton-Smith and M. Hostert and A. Hourlier and B. Howard and R. Howell and J. Huang and J. Huang and J. Hugon and G. Iles and N. Ilic and A. M. Iliescu and R. Illingworth and A. Ioannisian and L. Isenhower and R. Itay and A. Izmaylov and S. Jackson and V. Jain and E. James and B. Jargowsky and F. Jediny and D. Jena and Y. S. Jeong and C. Jes\u00fas-Valls and X. Ji and L. Jiang and S. Jim\u00e9nez and A. Jipa and R. Johnson and B. Jones and S. B. Jones and M. Judah and C. K. Jung and T. Junk and Y. Jwa and M. Kabirnezhad and A. Kaboth and I. Kadenko and I. Kakorin and F. Kamiya and N. Kaneshige and G. Karagiorgi and G. Karaman and A. Karcher and M. Karolak and Y. Karyotakis and S. Kasai and S. P. Kasetti and L. Kashur and N. Kazaryan and E. Kearns and P. Keener and K. J. Kelly and E. Kemp and O. Kemularia and W. Ketchum and S. H. Kettell and M. Khabibullin and A. Khotjantsev and A. Khvedelidze and D. Kim and B. King and B. Kirby and M. Kirby and J. Klein and K. Koehler and L. W. Koerner and S. Kohn and P. P. Koller and L. Kolupaeva and M. Kordosky and T. Kosc and U. Kose and V. A. Kosteleck\u00fd and K. Kothekar and F. Krennrich and I. Kreslo and Y. Kudenko and V. A. Kudryavtsev and S. Kulagin and J. Kumar and P. Kumar and P. Kunze and N. Kurita and C. Kuruppu and V. Kus and T. Kutter and A. Lambert and B. Land and K. Lande and C. E. Lane and K. Lang and T. Langford and J. Larkin and P. Lasorak and D. Last and C. Lastoria and A. Laundrie and A. Lawrence and I. Lazanu and R. LaZur and T. Le and S. Leardini and J. Learned and P. LeBrun and T. LeCompte and G. Lehmann Miotto and R. Lehnert and M. A. Leigui de Oliveira and M. Leitner and L. Li and S. W. Li and T. Li and Y. Li and H. Liao and C. S. Lin and Q. Lin and S. Lin and A. Lister and B. R. Littlejohn and J. Liu and S. Lockwitz and T. Loew and M. Lokajicek and I. Lomidze and K. Long and K. Loo and D. Lorca and T. Lord and J. M. LoSecco and W. C. Louis and X. -G. Lu and K. B. Luk and X. Luo and N. Lurkin and T. Lux and V. P. Luzio and D. MacFarlane and A. A. Machado and P. Machado and C. T. Macias and J. R. Macier and A. Maddalena and A. Madera and P. Madigan and S. Magill and K. Mahn and A. Maio and A. Major and J. A. Maloney and G. Mandrioli and R. C. Mandujano and J. Maneira and L. Manenti and S. Manly and A. Mann and K. Manolopoulos and M. Manrique Plata and V. N. Manyam and L. Manzanillas and M. Marchan and A. Marchionni and W. Marciano and D. Marfatia and C. Mariani and J. Maricic and R. Marie and F. Marinho and A. D. Marino and D. Marsden and M. Marshak and C. M. Marshall and J. Marshall and J. Marteau and J. Martin-Albo and N. Martinez and D. A. Martinez Caicedo and S. Martynenko and K. Mason and A. Mastbaum and M. Masud and S. Matsuno and J. Matthews and C. Mauger and N. Mauri and K. Mavrokoridis and I. Mawby and R. Mazza and A. Mazzacane and E. Mazzucato and T. McAskill and E. McCluskey and N. McConkey and K. S. McFarland and C. McGrew and A. McNab and A. Mefodiev and P. Mehta and P. Melas and O. Mena and S. Menary and H. Mendez and D. P. M\u00e9ndez and A. Menegolli and G. Meng and M. D. Messier and W. Metcalf and T. Mettler and M. Mewes and H. Meyer and T. Miao and G. Michna and T. Miedema and J. Migenda and V. Mikola and R. Milincic and W. Miller and J. Mills and C. Milne and O. Mineev and O. G. Miranda and S. Miryala and C. S. Mishra and S. R. Mishra and A. Mislivec and D. Mladenov and I. Mocioiu and K. Moffat and N. Moggi and R. Mohanta and T. A. Mohayai and N. Mokhov and J. Molina and L. Molina Bueno and A. Montanari and C. Montanari and D. Montanari and L. M. Montano Zetina and J. Moon and M. Mooney and A. F. Moor and D. Moreno and C. Morris and C. Mossey and E. Motuk and C. A. Moura and J. Mousseau and W. Mu and L. Mualem and J. Mueller and M. Muether and S. Mufson and F. Muheim and A. Muir and M. Mulhearn and D. Munford and H. Muramatsu and S. Murphy and J. Musser and J. Nachtman and S. Nagu and M. Nalbandyan and R. Nandakumar and D. Naples and S. Narita and D. Navas-Nicol\u00e1s and A. Navrer-Agasson and N. Nayak and M. Nebot-Guinot and K. Negishi and J. K. Nelson and J. Nesbit and M. Nessi and D. Newbold and M. Newcomer and D. Newhart and H. Newton and R. Nichol and F. Nicolas-Arnaldos and E. Niner and K. Nishimura and A. Norman and A. Norrick and R. Northrop and P. Novella and J. A. Nowak and M. Oberling and J. P. Ochoa-Ricoux and A. Olivares Del Campo and A. Olivier and A. Olshevskiy and Y. Onel and Y. Onishchuk and J. Ott and L. Pagani and S. Pakvasa and G. Palacio and O. Palamara and S. Palestini and J. M. Paley and M. Pallavicini and C. Palomares and J. L. Palomino-Gallo and E. Pantic and V. Paolone and V. Papadimitriou and R. Papaleo and A. Papanestis and S. Paramesvaran and S. Parke and Z. Parsa and M. Parvu and S. Pascoli and L. Pasqualini and J. Pasternak and J. Pater and C. Patrick and L. Patrizii and R. B. Patterson and S. J. Patton and T. Patzak and A. Paudel and B. Paulos and L. Paulucci and Z. Pavlovic and G. Pawloski and D. Payne and V. Pec and S. J. M. Peeters and E. Pennacchio and A. Penzo and O. L. G. Peres and J. Perry and D. Pershey and G. Pessina and G. Petrillo and C. Petta and R. Petti and F. Piastra and L. Pickering and F. Pietropaolo and R. Plunkett and R. Poling and X. Pons and N. Poonthottathil and S. Pordes and J. Porter and M. Potekhin and R. Potenza and B. V. K. S. Potukuchi and J. Pozimski and M. Pozzato and S. Prakash and T. Prakash and S. Prince and D. Pugnere and X. Qian and M. C. Queiroga Bazetto and J. L. Raaf and V. Radeka and J. Rademacker and B. Radics and A. Rafique and E. Raguzin and M. Rai and M. Rajaoalisoa and I. Rakhno and A. Rakotonandrasana and L. Rakotondravohitra and Y. A. Ramachers and R. Rameika and M. A. Ramirez Delgado and B. Ramson and A. Rappoldi and G. Raselli and P. Ratoff and S. Raut and R. F. Razakamiandra and J. S. Real and B. Rebel and M. Reggiani-Guzzo and T. Rehak and J. Reichenbacher and S. D. Reitzner and H. Rejeb Sfar and A. Renshaw and S. Rescia and F. Resnati and A. Reynolds and C. Riccio and G. Riccobene and L. C. J. Rice and J. Ricol and A. Rigamonti and Y. Rigaut and D. Rivera and L. Rochester and M. Roda and P. Rodrigues and M. J. Rodriguez Alonso and E. Rodriguez Bonilla and J. Rodriguez Rondon and S. Rosauro-Alcaraz and M. Rosenberg and P. Rosier and B. Roskovec and M. Rossella and J. Rout and P. Roy and S. Roy and A. Rubbia and C. Rubbia and F. C. Rubio and B. Russell and D. Ruterbories and R. Saakyan and S. Sacerdoti and T. Safford and R. Sahay and N. Sahu and P. Sala and N. Samios and O. Samoylov and M. C. Sanchez and D. A. Sanders and D. Sankey and S. Santana and M. Santos-Maldonado and N. Saoulidou and P. Sapienza and C. Sarasty and I. Sarcevic and G. Savage and V. Savinov and A. Scaramelli and A. Scarff and A. Scarpelli and T. Schaffer and H. Schellman and P. Schlabach and D. Schmitz and K. Scholberg and A. Schukraft and E. Segreto and J. Sensenig and I. Seong and A. Sergi and D. Sgalaberna and M. H. Shaevitz and S. Shafaq and M. Shamma and R. Sharankova and H. R. Sharma and R. Sharma and R. Kumar and T. Shaw and C. Shepherd-Themistocleous and S. Shin and D. Shooltz and R. Shrock and L. Simard and F. Simon and N. Simos and J. Sinclair and G. Sinev and J. Singh and J. Singh and V. Singh and R. Sipos and F. W. Sippach and G. Sirri and A. Sitraka and K. Siyeon and K. Skarpaas VIII and A. Smith and E. Smith and P. Smith and J. Smolik and M. Smy and E. L. Snider and P. Snopok and M. Soares Nunes and H. Sobel and M. Soderberg and C. J. Solano Salinas and S. S\u00f6ldner-Rembold and N. Solomey and V. Solovov and W. E. Sondheim and M. Sorel and J. Soto-Oton and A. Sousa and K. Soustruznik and F. Spagliardi and M. Spanu and J. Spitz and N. J. C. Spooner and K. Spurgeon and R. Staley and M. Stancari and L. Stanco and R. Stanley and R. Stein and H. M. Steiner and J. Stewart and B. Stillwell and J. Stock and F. Stocker and T. Stokes and M. Strait and T. Strauss and S. Striganov and A. Stuart and J. G. Suarez and H. Sullivan and D. Summers and A. Surdo and V. Susic and L. Suter and C. M. Sutera and R. Svoboda and B. Szczerbinska and A. M. Szelc and R. Talaga and H. A. Tanaka and B. Tapia Oregui and A. Tapper and S. Tariq and E. Tatar and R. Tayloe and A. M. Teklu and M. Tenti and K. Terao and C. A. Ternes and F. Terranova and G. Testera and A. Thea and J. L. Thompson and C. Thorn and S. C. Timm and J. Todd and A. Tonazzo and D. Torbunov and M. Torti and M. Tortola and F. Tortorici and D. Totani and M. Toups and C. Touramanis and J. Trevor and S. Trilov and W. H. Trzaska and Y. T. Tsai and Z. Tsamalaidze and K. V. Tsang and N. Tsverava and S. Tufanli and C. Tull and E. Tyley and M. Tzanov and M. A. Uchida and J. Urheim and T. Usher and S. Uzunyan and M. R. Vagins and P. Vahle and G. A. Valdiviesso and E. Valencia and Z. Vallari and J. W. F. Valle and S. Vallecorsa and R. Van Berg and R. G. Van de Water and F. Varanini and D. Vargas and G. Varner and J. Vasel and S. Vasina and G. Vasseur and N. Vaughan and K. Vaziri and S. Ventura and A. Verdugo and S. Vergani and M. A. Vermeulen and M. Verzocchi and M. Vicenzi and H. Vieira de Souza and C. Vignoli and C. Vilela and B. Viren and T. Vrba and T. Wachala and A. V. Waldron and M. Wallbank and H. Wang and J. Wang and M. H. L. S. Wang and Y. Wang and Y. Wang and K. Warburton and D. Warner and M. Wascko and D. Waters and A. Watson and P. Weatherly and A. Weber and M. Weber and H. Wei and A. Weinstein and D. Wenman and M. Wetstein and A. White and L. H. Whitehead and D. Whittington and M. J. Wilking and C. Wilkinson and Z. Williams and F. Wilson and R. J. Wilson and J. Wolcott and T. Wongjirad and A. Wood and K. Wood and E. Worcester and M. Worcester and C. Wret and W. Wu and W. Wu and Y. Xiao and E. Yandel and G. Yang and K. Yang and S. Yang and T. Yang and A. Yankelevich and N. Yershov and K. Yonehara and T. Young and B. Yu and H. Yu and J. Yu and W. Yuan and R. Zaki and J. Zalesak and L. Zambelli and B. Zamorano and A. Zani and L. Zazueta and G. Zeit and G. P. Zeller and J. Zennamo and K. Zeug and C. Zhang and M. Zhao and E. Zhivun and G. Zhu and P. Zilberman and E. D. Zimmerman and M. Zito and S. Zucchelli and J. Zuklin and V. Zutshi and R. Zwaska},\n  year={2021},\n  eprint={2103.13910},\n  archivePrefix={arXiv},\n  primaryClass={physics.ins-det},\n  url={https://arxiv.org/abs/2103.13910}, \n}</code></pre> Ratings CategoryRating Software 1.00 Code not available; no containerization or setup provided  Specification 4.00 Constraints like latency thresholds are described qualitatively but not numerically defined  Dataset 3.00 Dataset lacks a public URL; FAIR metadata and versioning are missing  Metrics 4.00 Metrics are relevant but no benchmark baseline or detailed evaluation guidance is provided  Reference Solution 2.00 Autoencoder prototype exists but is not reproducible; RL model still in development  Documentation 3.00 Documentation exists only in slides/GDocs; no implementation guide or structured release  Average rating: 2.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/feabench_finite_element_analysis_benchmark/","title":"FEABench (Finite Element Analysis Benchmark)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-01-26</p> <p>Name: FEABench  Finite Element Analysis Benchmark</p> <p>Domain: Computational Engineering</p> <p>Focus: FEA simulation accuracy and performance</p> <p>Task Types: Simulation, Performance evaluation</p> <p>Metrics: Solve time, Error norm</p> <p>Models: FEniCS, deal.II</p> Keywords finite element simulation PDE Citation <ul> <li>Nayantara Mudur, Hao Cui, Subhashini Venugopalan, Paul Raccuglia, Michael P. Brenner, and Peter Norgaard. Feabench: evaluating language models on multiphysics reasoning ability. 2025. URL: https://arxiv.org/abs/2504.06260, arXiv:2504.06260.</li> </ul> <pre><code>@misc{mudur2025feabenchevaluatinglanguagemodels,\n  title={FEABench: Evaluating Language Models on Multiphysics Reasoning Ability}, \n  author={Nayantara Mudur and Hao Cui and Subhashini Venugopalan and Paul Raccuglia and Michael P. Brenner and Peter Norgaard},\n  year={2025},\n  eprint={2504.06260},\n  archivePrefix={arXiv},\n  primaryClass={cs.AI},\n  url={https://arxiv.org/abs/2504.06260}, \n  }</code></pre> Ratings CategoryRating Software 4.00 Code is available, but poorly documented  Specification 1.50 Output is defined and task clarity is questionable  Dataset 4.00 Available, but not split into sets  Metrics 5.00 Fully defined metrics  Reference Solution 4.00 Three open-source models were used. No system constraints.  Documentation 5.00 In associated paper  Average rating: 3.92/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/frontiermath/","title":"FrontierMath","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-11-07</p> <p>Name: FrontierMath</p> <p>Domain: Mathematics</p> <p>Focus: Challenging advanced mathematical reasoning</p> <p>Task Types: Problem solving</p> <p>Metrics: Accuracy</p> <p>Models: unkown</p> Keywords symbolic reasoning number theory algebraic geometry category theory Citation <ul> <li>Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli J\u00e4rviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie Stewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. Frontiermath: a benchmark for evaluating advanced mathematical reasoning in ai. 2024. URL: https://arxiv.org/abs/2411.04872, arXiv:2411.04872.</li> </ul> <pre><code>@misc{glazer2024frontiermathbenchmarkevaluatingadvanced,\n  archiveprefix = {arXiv},\n  author        = {Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli J\u00e4rviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},\n  eprint        = {2411.04872},\n  primaryclass  = {cs.AI},\n  title         = {FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},\n  url           = {https://arxiv.org/abs/2411.04872},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 0.00 No link to code provided  Specification 3.00 Well-specified process for asking questions and receiving answers. No software or hardware constraints  Dataset 0.00 Paper and website had no link to any dataset. It may still exist somewhere  Metrics 5.00 (by default) All questions in the dataset have a correct answer  Reference Solution 2.00 Displays result of leading models on the benchmark, but none are trainable or list constraints  Documentation 0.00 No specified way to reproduce the reference solution  Average rating: 1.67/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/gess/","title":"GeSS","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: GeSS</p> <p>Domain: Scientific ML; Geometric Deep Learning</p> <p>Focus: Benchmark suite evaluating geometric deep learning models under real-world distribution shifts</p> <p>Task Types: Classification, Regression</p> <p>Metrics: Accuracy, RMSE, OOD robustness delta</p> <p>Models: GCN, EGNN, DimeNet++</p> Keywords geometric deep learning distribution shift OOD robustness scientific applications Citation <ul> <li>Deyu Zou, Shikun Liu, Siqi Miao, Victor Fung, Shiyu Chang, and Pan Li. Gess: benchmarking geometric deep learning under scientific applications with distribution shifts. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 92499\u201392528. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_a8063075,\n  author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {92499--92528},\n  publisher = {Curran Associates, Inc.},\n  title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Reference code expected post-conference; current public software availability limited. Benchmark infrastructure partially described but not fully released yet.  Specification 5.00 Benchmark clearly defines OOD robustness scenarios with classification and regression tasks in scientific domains, though no explicit hardware constraints are given.  Dataset 5.00 Curated datasets of 3D crystal structures and material properties are included and publicly available for reproducible research.  Metrics 5.00 Uses well-established metrics such as MAE and structural validity for materials modeling, plus accuracy and OOD robustness deltas.  Reference Solution 4.00 Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected to be released soon.  Documentation 4.00 Paper and poster provide solid explanation of benchmarks and scientific motivation; more extensive user documentation forthcoming.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/gpqa_a_graduate-level_google-proof_question_and_answer_benchmark/","title":"GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-11-20</p> <p>Name: GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark</p> <p>Domain: Science  Biology, Physics, Chemistry</p> <p>Focus: Graduate-level, expert-validated multiple-choice questions hard even with web access</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: GPT-4 baseline</p> Keywords Google-proof multiple-choice expert reasoning science QA Citation <ul> <li>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.</li> </ul> <pre><code>@misc{rein2023gpqagraduatelevelgoogleproofqa2,\n  archiveprefix = {arXiv},\n  author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},\n  eprint        = {2311.12022},\n  primaryclass  = {cs.AI},\n  title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},\n  url           = {https://arxiv.org/abs/2311.12022},\n  year          = {2023}\n}</code></pre> Ratings CategoryRating Software 3.00 Dataset and benchmark materials are publicly available via HuggingFace and GitHub, but no integrated runnable code or software framework is provided.  Specification 5.00 Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning. Input/output formats and evaluation criteria are well described.  Dataset 5.00 The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits.  Metrics 5.00 Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA.  Reference Solution 1.00 No baseline implementations or starter code are linked or provided for reproduction.  Documentation 3.00 Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines.  Average rating: 3.67/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/gpqa_diamond/","title":"GPQA Diamond","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-11-20</p> <p>Name: GPQA Diamond</p> <p>Domain: Science</p> <p>Focus: Graduate-level scientific reasoning</p> <p>Task Types: Multiple choice, Multi-step QA</p> <p>Metrics: Accuracy</p> <p>Models: o1, DeepSeek-R1</p> Keywords Google-proof graduate-level science QA chemistry physics Citation <ul> <li>David Rein, Betty Li Hou, and Asa Cooper Stickland. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022.</li> </ul> <pre><code>@misc{rein2023gpqagraduatelevelgoogleproofqa,\n  title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},\n  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},\n  year={2023},\n  url={https://arxiv.org/abs/2311.12022}\n}</code></pre> Ratings CategoryRating Software 5.00 Python version and requirements specified on Github site  Specification 2.00 No system constraints or I/O specified  Dataset 5.00 Easily able to access dataset. Comes with predefined splits as mentioned in the paper  Metrics 5.00 Each question has a correct answer, representing the tested model's performance.  Reference Solution 1.00 Common models such as GPT-3.5 were compared. They are not open and don't provide requirements  Documentation 5.00 All information is listed in the associated paper  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/hdr_ml_anomaly_challenge_butterfly/","title":"HDR ML Anomaly Challenge (Butterfly)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-03-03</p> <p>Name: HDR ML Anomaly Challenge  Butterfly</p> <p>Domain: Genomics; Image/CV</p> <p>Focus: Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset</p> <p>Task Types: Anomaly detection</p> <p>Metrics: Classification accuracy, F1 score</p> <p>Models: CNN-based detectors</p> Keywords anomaly detection computer vision genomics butterfly hybrids Citation <ul> <li>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.</li> </ul> <pre><code>@misc{campolongo2025buildingmachinelearningchallenges2,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Sa\u00fal Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 3.00 Codabench platform provides submission infrastructure but no fully maintained code repository or reproducible baseline implementations.  Specification 4.00 Task is clearly described with domain-specific anomaly detection objectives and relevant physics motivation.  Dataset 3.00 Dataset consists of real detector data with synthetic anomaly injections; access is restricted and requires NDA, limiting openness and FAIR compliance.  Metrics 3.00 Standard metrics (ROC, F1, precision) are used; evaluation protocols are clear but not deeply elaborated.  Reference Solution 2.00 Baselines are partially described but lack public code or reproducible execution scripts.  Documentation 3.00 Challenge website provides basic descriptions and evaluation metrics but lacks comprehensive tutorials or example workflows.  Average rating: 3.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/hdr_ml_anomaly_challenge_gravitational_waves/","title":"HDR ML Anomaly Challenge (Gravitational Waves)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-03-03</p> <p>Name: HDR ML Anomaly Challenge  Gravitational Waves</p> <p>Domain: Astrophysics; Time-series</p> <p>Focus: Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets</p> <p>Task Types: Anomaly detection</p> <p>Metrics: ROC-AUC, Precision/Recall</p> <p>Models: Deep latent CNNs, Autoencoders</p> Keywords anomaly detection gravitational waves astrophysics time-series Citation <ul> <li>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.</li> </ul> <pre><code>@misc{campolongo2025buildingmachinelearningchallenges,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Sa\u00fal Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 4.00 Benchmark platform provided on Codabench with starter kits and submission infrastructure. Code and baseline models are publicly accessible but not extensively maintained beyond the challenge.  Specification 4.00 Well-defined anomaly detection task on gravitational-wave time series with clear input/output expectations and challenge constraints.  Dataset 5.00 Uses preprocessed LIGO/Virgo time series data at 4096 Hz, publicly available and standard in astrophysics.  Metrics 4.00 ROC-AUC, precision, and recall metrics are clearly specified and appropriate for anomaly detection.  Reference Solution 4.00 Baseline deep latent CNNs and autoencoders are provided and reproducible, but not extensively documented.  Documentation 4.00 Documentation includes challenge instructions, starter kit details, and baseline descriptions, but could benefit from more thorough tutorials and code walkthroughs.  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/hdr_ml_anomaly_challenge_sea_level_rise/","title":"HDR ML Anomaly Challenge (Sea Level Rise)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-03-03</p> <p>Name: HDR ML Anomaly Challenge  Sea Level Rise</p> <p>Domain: Climate Science; Time-series, Image/CV</p> <p>Focus: Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery</p> <p>Task Types: Anomaly detection</p> <p>Metrics: ROC-AUC, Precision/Recall</p> <p>Models: CNNs, RNNs, Transformers</p> Keywords anomaly detection climate science sea-level rise time-series remote sensing Citation <ul> <li>Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Sa\u00fal Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.</li> </ul> <pre><code>@misc{campolongo2025buildingmachinelearningchallenges3,\n  archiveprefix = {arXiv},\n  author        = {Elizabeth G. Campolongo and Yuan-Tang Chou and Ekaterina Govorkova and Wahid Bhimji and Wei-Lun Chao and Chris Harris and Shih-Chieh Hsu and Hilmar Lapp and Mark S. Neubauer and Josephine Namayanja and Aneesh Subramanian and Philip Harris and Advaith Anand and David E. Carlyn and Subhankar Ghosh and Christopher Lawrence and Eric Moreno and Ryan Raikman and Jiaman Wu and Ziheng Zhang and Bayu Adhi and Mohammad Ahmadi Gharehtoragh and Sa\u00fal Alonso Monsalve and Marta Babicz and Furqan Baig and Namrata Banerji and William Bardon and Tyler Barna and Tanya Berger-Wolf and Adji Bousso Dieng and Micah Brachman and Quentin Buat and David C. Y. Hui and Phuong Cao and Franco Cerino and Yi-Chun Chang and Shivaji Chaulagain and An-Kai Chen and Deming Chen and Eric Chen and Chia-Jui Chou and Zih-Chen Ciou and Miles Cochran-Branson and Artur Cordeiro Oudot Choi and Michael Coughlin and Matteo Cremonesi and Maria Dadarlat and Peter Darch and Malina Desai and Daniel Diaz and Steven Dillmann and Javier Duarte and Isla Duporge and Urbas Ekka and Saba Entezari Heravi and Hao Fang and Rian Flynn and Geoffrey Fox and Emily Freed and Hang Gao and Jing Gao and Julia Gonski and Matthew Graham and Abolfazl Hashemi and Scott Hauck and James Hazelden and Joshua Henry Peterson and Duc Hoang and Wei Hu and Mirco Huennefeld and David Hyde and Vandana Janeja and Nattapon Jaroenchai and Haoyi Jia and Yunfan Kang and Maksim Kholiavchenko and Elham E. Khoda and Sangin Kim and Aditya Kumar and Bo-Cheng Lai and Trung Le and Chi-Wei Lee and JangHyeon Lee and Shaocheng Lee and Suzan van der Lee and Charles Lewis and Haitong Li and Haoyang Li and Henry Liao and Mia Liu and Xiaolin Liu and Xiulong Liu and Vladimir Loncar and Fangzheng Lyu and Ilya Makarov and Abhishikth Mallampalli Chen-Yu Mao and Alexander Michels and Alexander Migala and Farouk Mokhtar and Mathieu Morlighem and Min Namgung and Andrzej Novak and Andrew Novick and Amy Orsborn and Anand Padmanabhan and Jia-Cheng Pan and Sneh Pandya and Zhiyuan Pei and Ana Peixoto and George Percivall and Alex Po Leung and Sanjay Purushotham and Zhiqiang Que and Melissa Quinnan and Arghya Ranjan and Dylan Rankin and Christina Reissel and Benedikt Riedel and Dan Rubenstein and Argyro Sasli and Eli Shlizerman and Arushi Singh and Kim Singh and Eric R. Sokol and Arturo Sorensen and Yu Su and Mitra Taheri and Vaibhav Thakkar and Ann Mariam Thomas and Eric Toberer and Chenghan Tsai and Rebecca Vandewalle and Arjun Verma and Ricco C. Venterea and He Wang and Jianwu Wang and Sam Wang and Shaowen Wang and Gordon Watts and Jason Weitz and Andrew Wildridge and Rebecca Williams and Scott Wolf and Yue Xu and Jianqi Yan and Jai Yu and Yulei Zhang and Haoran Zhao and Ying Zhao and Yibo Zhong},\n  eprint        = {2503.02112},\n  primaryclass  = {cs.LG},\n  title         = {Building Machine Learning Challenges for Anomaly Detection in Science},\n  url           = {https://arxiv.org/abs/2503.02112},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 2.00 Benchmark platform exists on Codabench, but no baseline code or maintained repository for reference solutions provided yet.  Specification 5.00 Well-defined anomaly detection task combining satellite imagery and time-series data, with clear physical and domain-specific framing.  Dataset 5.00 Uses preprocessed, public, and well-structured sensor and satellite data for the North Atlantic sea-level rise region.  Metrics 5.00 Standard metrics such as ROC-AUC, precision, and recall are specified and suitable for the anomaly detection tasks.  Reference Solution 1.00 No starter models or baseline implementations linked or provided publicly.  Documentation 5.00 Challenge page, starter kits, and related papers offer strong guidance for participants.  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/hedm_braggnn/","title":"HEDM (BraggNN)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-10-03</p> <p>Name: HEDM  BraggNN</p> <p>Domain: Material Science</p> <p>Focus: Fast Bragg peak analysis using deep learning in diffraction microscopy</p> <p>Task Types: Peak detection</p> <p>Metrics: Localization accuracy, Inference time</p> <p>Models: BraggNN</p> Keywords BraggNN diffraction peak finding HEDM Citation <ul> <li>Zhengchun Liu, Hemant Sharma, Jun-Sang Park, Peter Kenesei, Antonino Miceli, Jonathan Almer, Rajkumar Kettimuthu, and Ian Foster. Braggnn: fast x-ray bragg peak analysis using deep learning. 2021. URL: https://arxiv.org/abs/2008.08198, arXiv:2008.08198.</li> </ul> <pre><code>@misc{liu2021braggnnfastxraybragg,\n  archiveprefix = {arXiv},\n  author        = {Zhengchun Liu and Hemant Sharma and Jun-Sang Park and Peter Kenesei and Antonino Miceli and Jonathan Almer and Rajkumar Kettimuthu and Ian Foster},\n  eprint        = {2008.08198},\n  primaryclass  = {eess.IV},\n  title         = {BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning},\n  url           = {https://arxiv.org/abs/2008.08198},\n  year          = {2021}\n}</code></pre> Ratings CategoryRating Software 2.00 No standalone code repository or setup instructions provided  Specification 5.00 None  Dataset 2.00 No dataset links or FAIR metadata; unclear public access  Metrics 4.00 Only localization accuracy and inference time mentioned; not formally benchmarked with scripts  Reference Solution 3.00 BraggNN model is described and evaluated, but no direct implementation or inference scripts available  Documentation 3.00 Paper is clear, but lacks a GitHub repo or full reproducibility pipeline  Average rating: 3.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/humanitys_last_exam/","title":"Humanity's Last Exam","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-01-24</p> <p>Name: Humanity's Last Exam</p> <p>Domain: Multidomain</p> <p>Focus: Broad cross-domain academic reasoning</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: unkown</p> Keywords cross-domain academic exam multiple-choice multidisciplinary Citation <ul> <li>Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry Dodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav Kazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon, Yongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, S\u00f8ren Riis, Saiteja Utpala, Noah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney, Antrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng, Jennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui Li, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G. Willcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward Vendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow, Natanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod, G\u00f6zdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang, Paolo Rissone, Sean R. Green, Lina Br\u00fcssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin Imperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad Hogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den Houte, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust, Bikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong Yang, John Maar, Julian Wykowski, Mart\u00ed Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Ariel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Joseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan, Sergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes, Alexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Zakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias Kreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger, Kaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov, V\u00e1clav Rozho\u0148, Vincent Ginis, Christian Stump, Niv Cohen, Rafa\u0142 Po\u015bwiata, Josef Tkadlec, Alan Goldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan Stendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek Shukla, Alan Givr\u00e9, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow, Hao He, Ling Zhang, Younesse Kaddar, Ivar \u00c4ngquist, Yanxu Chen, Harrison K Wang, Kalyan Ramakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik Kirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Shreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, J\u00e9r\u00e9my Andr\u00e9oletti, Raghav Singhal, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Marco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Kh\u00e1nh, Paul Rosu, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William Alley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob Loader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida Bosio, Fereshteh Kazemi, Ziye Chen, Bir\u00f3 B\u00e1lint, Eve J. Y. Lo, Jiaqi Wang, Maria In\u00eas S. Nunes, Jeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff, Lynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee, Robin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Jiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak Pradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciob\u00e2c\u0103, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K. Zhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan, Andrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Claudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David Stap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo Rodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna Samuele, Fredrik Ekstr\u00f6m, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Pe\u00f1aflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez, Daniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon Park, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin Yong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo Albani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yi\u011fit Yalin, Gbenga Daniel Obikoya, Rai, Filippo Bigi, M. C. Bosc\u00e1, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle H\u00e4ggstr\u00f6m, Emil Verkama, Joshua Newbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Ting Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder de Witt, Pablo Hern\u00e1ndez-C\u00e1mara, Emanuele Rodol\u00e0, Jules Robins, Dominic Williamson, Vincent Cheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth Anderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro Jos\u00e9 Moyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier, Omid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Ra\u00fal Adri\u00e1n Huerta Rodr\u00edguez, Ali M. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh Ducey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benj\u00e1min Borb\u00e1s, Eric Chu, Jack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah, Marc Carauleanu, Pascal Lauer, Tran \u0110uc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark, Assaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel Poesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam, Juan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume, Wiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo, Jakub \u0141ucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa Gonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo Jiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels M\u00fcndler, S\u00f6ren M\u00f6ller, Luca Arnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony Fruhauff, Glen Sherman, M\u00e1ty\u00e1s Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan, Innocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John Lai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P V, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubi\u0107, Samuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella, Alex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang, Jason Poulos, Vil\u00e9m Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha, Yinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng Wu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Micka\u00ebl Noy\u00e9, Micha\u0142 Pere\u0142kiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa Nguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario Abbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del Rio-Chanona, D\u00e1niel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny Reddish, Jakob Hauser, Francisco-Javier Rodrigo-Gin\u00e9s, Suchandra Datta, Maxwell Shepherd, Thom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt, Satyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar Shridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang, Adam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena, Xing Han L\u00f9, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming Yin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez, Costin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin Bria\u0144ski, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-Orallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen, Alexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan Todoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther Yap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi Wang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira Arrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam Bouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas Subramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim, Yushun Chen, Sara Vera Marjanovi\u0107, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen, Dawn Song, Cedegao E. Zhang, Zhun Wang, Ga\u00ebl Gendron, Yunze Xiao, Leo Smucker, Erica Weng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony Gitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin, Philipp D. Siedler, Jichao Fang, Ritwik Mishra, K\u00e1roly Zsolnai-Feh\u00e9r, Xilin Jiang, Shadab Khan, Jun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang, Isha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao Zheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen, Deepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gon\u00e7alves, Wei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline Geirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Y\u00fccel, Brandon Christof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac Park, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng, Zhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang, Bruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang, Marc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying Liu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe, Hongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset, Zishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia Chernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park, Hieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui Pan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W. Bartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish Cheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay Paek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent Cheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath, Violet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin Wang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny Sun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert Yang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang, Andrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David Sun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan Zhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity's last exam. 2025. URL: https://arxiv.org/abs/2501.14249, arXiv:2501.14249.</li> </ul> <pre><code>@misc{phan2025humanitysexam,\n  archiveprefix = {arXiv},\n  author        = {Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and S\u00f8ren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and G\u00f6zdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Br\u00fcssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Mart\u00ed Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and V\u00e1clav Rozho\u0148 and Vincent Ginis and Christian Stump and Niv Cohen and Rafa\u0142 Po\u015bwiata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givr\u00e9 and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar \u00c4ngquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and J\u00e9r\u00e9my Andr\u00e9oletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Kh\u00e1nh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Bir\u00f3 B\u00e1lint and Eve J. Y. Lo and Jiaqi Wang and Maria In\u00eas S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciob\u00e2c\u0103 and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekstr\u00f6m and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Pe\u00f1aflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yi\u011fit Yalin and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Bosc\u00e1 and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle H\u00e4ggstr\u00f6m and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hern\u00e1ndez-C\u00e1mara and Emanuele Rodol\u00e0 and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro Jos\u00e9 Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Ra\u00fal Adri\u00e1n Huerta Rodr\u00edguez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benj\u00e1min Borb\u00e1s and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran \u0110uc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub \u0141ucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels M\u00fcndler and S\u00f6ren M\u00f6ller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and M\u00e1ty\u00e1s Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubi\u0107 and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vil\u00e9m Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Micka\u00ebl Noy\u00e9 and Micha\u0142 Pere\u0142kiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and D\u00e1niel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Gin\u00e9s and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han L\u00f9 and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Bria\u0144ski and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanovi\u0107 and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Ga\u00ebl Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and K\u00e1roly Zsolnai-Feh\u00e9r and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gon\u00e7alves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Y\u00fccel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks},\n  eprint        = {2501.14249},\n  primaryclass  = {cs.LG},\n  title         = {Humanity's Last Exam},\n  url           = {https://arxiv.org/abs/2501.14249},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 4.00 Code for testing models posted on the github. Unknown how to run a custom model.  Specification 2.00 Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified  Dataset 2.00 Data accessible through Hugging Face, but requires giving contact information to access  Metrics 5.00 (by default) All questions in the dataset are multiple choice, all have a correct answer  Reference Solution 2.00 Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result  Documentation 5.00 Paper available with necessary information  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/in-situ_high-speed_computer_vision/","title":"In-Situ High-Speed Computer Vision","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-12-05</p> <p>Name: In-Situ High-Speed Computer Vision</p> <p>Domain: Fusion/Plasma</p> <p>Focus: Real-time image classification for in-situ plasma diagnostics</p> <p>Task Types: Image Classification</p> <p>Metrics: Accuracy, FPS</p> <p>Models: CNN</p> Keywords plasma in-situ vision real-time ML Citation <ul> <li>Yumou Wei, Ryan F. Forelli, Chris Hansen, Jeffrey P. Levesque, Nhan Tran, Joshua C. Agar, Giuseppe Di Guglielmo, Michael E. Mauel, and Gerald A. Navratil. Low latency optical-based mode tracking with machine learning deployed on fpgas on a tokamak. 2024. URL: https://arxiv.org/abs/2312.00128, arXiv:2312.00128, doi:https://doi.org/10.1063/5.0190354.</li> </ul> <pre><code>@misc{wei2024lowlatencyopticalbasedmode,\n  archiveprefix = {arXiv},\n  author        = {Yumou Wei and Ryan F. Forelli and Chris Hansen and Jeffrey P. Levesque and Nhan Tran and Joshua C. Agar and Giuseppe Di Guglielmo and Michael E. Mauel and Gerald A. Navratil},\n  doi           = {https://doi.org/10.1063/5.0190354},\n  eprint        = {2312.00128},\n  primaryclass  = {physics.plasm-ph},\n  title         = {Low latency optical-based mode tracking with machine learning deployed on FPGAs on a tokamak},\n  url           = {https://arxiv.org/abs/2312.00128},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 1.00 No public implementation or containerized setup released  Specification 3.00 No standardized I/O, latency constraint, or complete framing  Dataset 0.00 Dataset not provided or described in any formal way  Metrics 2.00 Throughput and accuracy mentioned, but not defined or benchmarked  Reference Solution 1.00 Prototype CNNs described; no code, baseline, or training details available  Documentation 2.00 Some insight via papers, but no working repo, setup, or replication path  Average rating: 1.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/intelligent_experiments_through_real-time_ai/","title":"Intelligent experiments through real-time AI","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-01-08</p> <p>Name: Intelligent experiments through real-time AI</p> <p>Domain: Instrumentation and Detectors; Nuclear Physics; Particle Physics</p> <p>Focus: Real-time FPGA-based triggering and detector control for sPHENIX and future EIC</p> <p>Task Types: Trigger classification, Detector control, Real-time inference</p> <p>Metrics: Accuracy (charm and beauty detection), Latency (micros), Resource utilization (LUT/FF/BRAM/DSP)</p> <p>Models: Bipartite Graph Network with Set Transformers (BGN-ST), GarNet (edge-classifier)</p> Keywords FPGA Graph Neural Network hls4ml real-time inference detector control Citation <ul> <li>J. Kvapil, G. Borca-Tasciuc, H. Bossi, K. Chen, Y. Chen, Y. Corrales Morales, H. Da Costa, C. Da Silva, C. Dean, J. Durham, S. Fu, C. Hao, P. Harris, O. Hen, H. Jheng, Y. Lee, P. Li, X. Li, Y. Lin, M. X. Liu, V. Loncar, J. P. Mitrevski, A. Olvera, M. L. Purschke, J. S. Renck, G. Roland, J. Schambach, Z. Shi, N. Tran, N. Wuerfel, B. Xu, D. Yu, and H. Zhang. Intelligent experiments through real-time ai: fast data processing and autonomous detector control for sphenix and future eic detectors. 2025. URL: https://arxiv.org/abs/2501.04845, arXiv:2501.04845.</li> </ul> <pre><code>@misc{kvapil2025intelligentexperimentsrealtimeai,\n  archiveprefix={arXiv},\n  author={J. Kvapil and G. Borca-Tasciuc and H. Bossi and K. Chen and Y. Chen and Y. Corrales Morales and H. Da Costa and C. Da Silva and C. Dean and J. Durham and S. Fu and C. Hao and P. Harris and O. Hen and H. Jheng and Y. Lee and P. Li and X. Li and Y. Lin and M. X. Liu and V. Loncar and J. P. Mitrevski and A. Olvera and M. L. Purschke and J. S. Renck and G. Roland and J. Schambach and Z. Shi and N. Tran and N. Wuerfel and B. Xu and D. Yu and H. Zhang},\n  eprint={2501.04845},\n  primaryclass={physics.ins-det},\n  title={Intelligent experiments through real-time AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors},\n  url={https://arxiv.org/abs/2501.04845},\n  year={2025}\n}</code></pre> Ratings CategoryRating Software 3.00 No containerized or open-source setup provided  Specification 4.00 Architectural/system specifications are incomplete  Dataset 2.00 Dataset is internal and not publicly available or FAIR-compliant  Metrics 3.00 Metrics relevant but not supported by evaluation scripts or baselines  Reference Solution 3.00 No public or reproducible implementation released  Documentation 3.00 No public GitHub or complete pipeline documentation  Average rating: 3.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/irregular_sensor_data_compression/","title":"Irregular Sensor Data Compression","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-05-01</p> <p>Name: Irregular Sensor Data Compression</p> <p>Domain: Particle Physics</p> <p>Focus: Real-time compression of sparse sensor data with autoencoders</p> <p>Task Types: Compression</p> <p>Metrics: MSE, Compression ratio</p> <p>Models: Autoencoder, Quantized autoencoder</p> Keywords compression autoencoder sparse data irregular sampling Citation <ul> <li>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.</li> </ul> <pre><code>@misc{duarte2022fastmlsciencebenchmarksaccelerating2,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Reddi, Vijay Janapa},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}</code></pre> Ratings CategoryRating Software 3.00 Not containerized; Full automation and documentation could be improved  Specification 4.00 Exact latency or resource constraints not numerically specified  Dataset 5.00 All criteria met  Metrics 5.00 All criteria met  Reference Solution 4.00 Not fully documented or automated for reproducibility  Documentation 4.00 Setup for deployment (e.g., FPGA pipeline) requires familiarity with tooling  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/jarvis-leaderboard/","title":"JARVIS-Leaderboard","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-06-20</p> <p>Name: JARVIS-Leaderboard</p> <p>Domain: Materials Science; Benchmarking</p> <p>Focus: Comparative evaluation of materials design methods</p> <p>Task Types: Method benchmarking, Leaderboard ranking</p> <p>Metrics: MAE, RMSE, Accuracy</p> <p>Models: unkown</p> Keywords leaderboards materials methods simulation Citation <ul> <li>Kamal Choudhary, Daniel Wines, Kangming Li, Kevin F. Garrity, Vishu Gupta, Aldo H. Romero, Jaron T. Krogel, Kayahan Saritas, Addis Fuhr, Panchapakesan Ganesh, Paul R. C. Kent, Keqiang Yan, Yuchao Lin, Shuiwang Ji, Ben Blaiszik, Patrick Reiser, Pascal Friederich, Ankit Agrawal, Pratyush Tiwary, Eric Beyerle, Peter Minch, Trevor D. Rhone, Ichiro Takeuchi, Robert B. Wexler, Arun Mannodi-Kanakkithodi, Elif Ertekin, Avanish Mishra, Nithin Mathew, Mitchell Wood, Andrew D. Rohskopf, Jason Hattrick-Simpers, Shih-Han Wang, Luke E. K. Achenie, Hongliang Xin, Maureen Williams, Adam J. Biacchi, and Francesca Tavazza. JARVIS-Leaderboard: a large scale benchmark of materials design methods. npj Computational Materials, 10(1):93, 2024. URL: https://doi.org/10.1038/s41524-024-01259-w, doi:10.1038/s41524-024-01259-w.</li> </ul> <pre><code>@article{choudhary2024jarvis,\n  title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},\nauthor = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},\n  journal = {npj Computational Materials},\n  volume = {10},\n  number = {1},\n  pages = {93},\n  year = {2024},\n  doi = {10.1038/s41524-024-01259-w},\n  url = {https://doi.org/10.1038/s41524-024-01259-w}\n}</code></pre> Ratings CategoryRating Software 1.00 Setup script provided, but no code provided  Specification 1.00 Only dataset format is defined.  Dataset 4.00 Data is public and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks. No splits.  Metrics 5.00 Metrics stated for each benchmark.  Reference Solution 4.00 Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); no constraints specified.  Documentation 1.00 Only the task is specified.  Average rating: 2.67/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/jet_classification/","title":"Jet Classification","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-05-01</p> <p>Name: Jet Classification</p> <p>Domain: Particle Physics</p> <p>Focus: Real-time classification of particle jets using HL-LHC simulation features</p> <p>Task Types: Classification</p> <p>Metrics: Accuracy, AUC</p> <p>Models: Keras DNN, QKeras quantized DNN</p> Keywords classification real-time ML jet tagging QKeras Citation <ul> <li>Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.</li> </ul> <pre><code>@misc{duarte2022fastml,\n  archiveprefix = {arXiv},\n  author        = {Javier Duarte and Nhan Tran and Ben Hawks and Christian Herwig and Jules Muhizi and Shvetank Prakash and Vijay Janapa Reddi},\n  eprint        = {2207.07958},\n  primaryclass  = {cs.LG},\n  title         = {FastML Science Benchmarks: Accelerating Real-Time Scientific Edge Machine Learning},\n  url           = {https://arxiv.org/abs/2207.07958},\n  year          = {2022}\n}</code></pre> Ratings CategoryRating Software 3.00 Not containerized; Setup automation/documentation could be improved  Specification 4.00 System constraints missing  Dataset 5.00 None  Metrics 5.00 None  Reference Solution 4.00 HW/SW requirements missing; Reference not bundled as official starter kit  Documentation 4.00 Full reproducibility requires manual setup  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/lhc_new_physics_dataset/","title":"LHC New Physics Dataset","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2021-07-05</p> <p>Name: LHC New Physics Dataset</p> <p>Domain: Particle Physics; Real-time Triggering</p> <p>Focus: Real-time LHC event filtering for anomaly detection using proton collision data</p> <p>Task Types: Anomaly detection, Event classification</p> <p>Metrics: ROC-AUC, Detection efficiency</p> <p>Models: Autoencoder, Variational autoencoder, Isolation forest</p> Keywords anomaly detection proton collision real-time inference event filtering unsupervised ML Citation <ul> <li>Thea Aarrestad, Ekaterina Govorkova, Jennifer Ngadiuba, Ema Puljak, Maurizio Pierini, and Kinga Anna Wozniak. Unsupervised new physics detection at 40 mhz: training dataset. 2021. URL: https://zenodo.org/record/5046389, doi:10.5281/ZENODO.5046389.</li> </ul> <pre><code>@misc{https://doi.org/10.5281/zenodo.5046389,\n  author    = {Aarrestad, Thea and Govorkova, Ekaterina and Ngadiuba, Jennifer and Puljak, Ema and Pierini, Maurizio and Wozniak, Kinga Anna},\n  copyright = {Creative Commons Attribution 4.0 International},\n  doi       = {10.5281/ZENODO.5046389},\n  publisher = {Zenodo},\n  title     = {Unsupervised New Physics detection at 40 MHz: Training Dataset},\n  url       = {https://zenodo.org/record/5046389},\n  year      = {2021}\n}</code></pre> Ratings CategoryRating Software 3.00 While not formally evaluated in the previous version, Zenodo and paper links suggest available code for baseline models (e.g., autoencoders, GANs), though they are scattered and not unified in a single repository.  Specification 3.00 The task and context are clearly described, but system constraints and formal inputs/outputs are not fully specified.  Dataset 5.00 Large-scale dataset hosted on Zenodo, publicly available, well-documented, with defined train/test structure. Appears to follow at least 4 FAIR principles.  Metrics 4.00 Uses reasonable metrics (ROC-AUC, detection efficiency) that capture performance but lacks full explanation and standard evaluation tools.  Reference Solution 2.00 Baselines are described across multiple papers but lack centralized, reproducible implementations and hardware/software setup details.  Documentation 3.00 Some description in papers and dataset metadata exists, but lacks a unified guide, README, or training setup in a central location.  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/llm-inference-bench/","title":"LLM-Inference-Bench","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-10-31</p> <p>Name: LLM-Inference-Bench</p> <p>Domain: LLM; HPC/inference</p> <p>Focus: Hardware performance benchmarking of LLMs on AI accelerators</p> <p>Task Types: Inference Benchmarking</p> <p>Metrics: Token throughput (tok/s), Latency, Framework-hardware mix performance</p> <p>Models: LLaMA-2-7B, LLaMA-2-70B, Mistral-7B, Qwen-7B</p> Keywords LLM inference benchmarking GPU accelerator throughput Citation <ul> <li>Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: inference benchmarking of large language models on ai accelerators. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, volume, 1362 1379. 2024. doi:10.1109/SCW63240.2024.00178.</li> </ul> <pre><code>@INPROCEEDINGS{10820566,\n  author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},\n  booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, \n  title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, \n  year={2024},\n  volume={},\n  number={},\n  pages={1362-1379},\n  keywords={Performance evaluation;Power demand;Computational modeling;Large language models;Scalability;High performance computing;AI accelerators;Benchmark testing;Propulsion;Throughput;Large Language Models;AI Accelerators;Inference Performance Evaluation;Benchmarking},\n  doi={10.1109/SCW63240.2024.00178}}\n}</code></pre> Ratings CategoryRating Software 5.00 Public GitHub repository (https://github.com/argonne-lcf/LLM-Inference-Bench) under BSD-3 license. Includes scripts, configurations, and dashboards for running and visualizing LLM inference benchmarks across multiple accelerator platforms.  Specification 5.00 Benchmark scope, models, accelerator targets, and supported frameworks are clearly specified. Input configurations and output metrics are standardized across hardware types.  Dataset 2.00 No novel dataset is introduced; benchmark relies on pre-trained LLMs and synthetic inference inputs. Dataset structure and FAIR considerations are minimal.  Metrics 5.00 Hardware-specific metrics (token throughput, latency, utilization) are well-defined, consistently measured, and aggregated in dashboards.  Reference Solution 3.00 Inference configurations and baseline performance results are provided, but there are no full reference training pipelines or model implementations.  Documentation 4.00 GitHub repo provides clear usage instructions, setup guides, and interactive dashboard tooling. Some areas like benchmarking extensions or advanced tuning are less detailed.  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/llms_for_crop_science/","title":"LLMs for Crop Science","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: LLMs for Crop Science</p> <p>Domain: Agricultural Science; NLP</p> <p>Focus: Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts</p> <p>Task Types: Question Answering, Inference</p> <p>Metrics: Accuracy, F1 score</p> <p>Models: GPT-4, LLaMA-2-13B, T5-XXL</p> Keywords crop science prompt engineering domain adaptation question answering Citation <ul> <li>Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, and Enhong Chen. Exploring user retrieval integration towards large language models for cross-domain sequential recommendation. 2024. URL: https://arxiv.org/abs/2406.03085, arXiv:2406.03085.</li> </ul> <pre><code>@misc{shen2024exploringuserretrievalintegration,\n  title={Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation}, \n  author={Tingjia Shen and Hao Wang and Jiaqing Zhang and Sirui Zhao and Liangyue Li and Zulong Chen and Defu Lian and Enhong Chen},\n  year={2024},\n  eprint={2406.03085},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2406.03085}, \n}</code></pre> Ratings CategoryRating Software 0.00 This is a model, not a benchmark.  Specification 0.00 This is a model, not a benchmark.  Dataset 0.00 This is a model, not a benchmark.  Metrics 0.00 This is a model, not a benchmark.  Reference Solution 0.00 This is a model, not a benchmark.  Documentation 0.00 This is a model, not a benchmark.  Average rating: 0.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/massspecgym/","title":"MassSpecGym","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: MassSpecGym</p> <p>Domain: Cheminformatics; Molecular Discovery</p> <p>Focus: Benchmark suite for discovery and identification of molecules via MS/MS</p> <p>Task Types: De novo generation, Retrieval, Simulation</p> <p>Metrics: Structure accuracy, Retrieval precision, Simulation MSE</p> <p>Models: Graph-based generative models, Retrieval baselines</p> Keywords mass spectrometry molecular structure de novo generation retrieval dataset Citation <ul> <li>Roman Bushuiev, Anton Bushuiev, Niek F. de Jonge, Adamo Young, Fleming Kretschmer, Raman Samusevich, Janne Heirman, Fei Wang, Luke Zhang, Kai D\u00fchrkop, Marcus Ludwig, Nils A. Haupt, Apurva Kalia, Corinna Brungs, Robin Schmid, Russell Greiner, Bo Wang, David S. Wishart, Li-Ping Liu, Juho Rousu, Wout Bittremieux, Hannes Rost, Tytus D. Mak, Soha Hassoun, Florian Huber, Justin J.J. van der Hooft, Michael A. Stravs, Sebastian B\u00f6cker, Josef Sivic, and Tom\u00e1\u0161 Pluskal. Massspecgym: a benchmark for the discovery and identification of molecules. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 110010\u2013110027. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_c6c31413,\n  author = {Bushuiev, Roman and Bushuiev, Anton and de Jonge, Niek F. and Young, Adamo and Kretschmer, Fleming and Samusevich, Raman and Heirman, Janne and Wang, Fei and Zhang, Luke and D\\\"{u}hrkop, Kai and Ludwig, Marcus and Haupt, Nils A. and Kalia, Apurva and Brungs, Corinna and Schmid, Robin and Greiner, Russell and Wang, Bo and Wishart, David S. and Liu, Li-Ping and Rousu, Juho and Bittremieux, Wout and Rost, Hannes and Mak, Tytus D. and Hassoun, Soha and Huber, Florian and van der Hooft, Justin J.J. and Stravs, Michael A. and B\\\"{o}cker, Sebastian and Sivic, Josef and Pluskal, Tom\\'{a}\\v{s}},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {110010--110027},\n  publisher = {Curran Associates, Inc.},\n  title = {MassSpecGym: A benchmark for the discovery and identification of molecules},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c6c31413d5c53b7d1c343c1498734b0f-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Open-source GitHub repository available; baseline models and training code partially provided but overall framework maturity is moderate.  Specification 5.00 Clearly defined tasks including molecule generation, retrieval, and spectrum simulation, scoped for MS/MS molecular identification.  Dataset 5.00 Largest public MS/MS dataset with extensive annotations; minor point deducted for lack of explicit train/validation/test splits.  Metrics 5.00 Well-defined metrics such as structure accuracy, retrieval precision, and simulation MSE used consistently.  Reference Solution 3.50 CNN-based baselines are referenced, but pretrained weights and comprehensive training pipelines are not fully documented.  Documentation 1.00 Paper and poster describe benchmark goals and design, but documentation and user guides are minimal and repo status uncertain.  Average rating: 3.75/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/materials_project/","title":"Materials Project","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2011-10-01</p> <p>Name: Materials Project</p> <p>Domain: Materials Science</p> <p>Focus: DFT-based property prediction</p> <p>Task Types: Property prediction</p> <p>Metrics: MAE, R^2</p> <p>Models: Automatminer, Crystal Graph Neural Networks</p> Keywords DFT materials genome high-throughput Citation <ul> <li>Anubhav Jain, Shyue Ping Ong, Geoffroy Hautier, Wei Chen, William Davidson Richards, Stephen Dacek, Shreyas Cholia, Dan Gunter, David Skinner, Gerbrand Ceder, and Kristin A. Persson. The materials project: a materials genome approach. APL Materials, 2013. URL: https://materialsproject.org/, doi:10.1063/1.4812323.</li> </ul> <pre><code>@article{jain2013materials,\n  title={The Materials Project: A materials genome approach},\n  author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},\n  journal={APL Materials},\n  volume    = {1},\n  number    = {1},\n  year={2013},\n  doi       = {10.1063/1.4812323},\n  url={https://materialsproject.org/}\n}</code></pre> Ratings CategoryRating Software 0.00 No instructions available  Specification 1.50 The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.  Dataset 3.00 API key required to access data. No predefined splits.  Metrics 5.00 Uses numerical metrics like MAE and R^2  Reference Solution 2.00 Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no constraints or documentation listed.  Documentation 0.00 No explanations or paper provided  Average rating: 1.92/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/math-/","title":"MATH-500","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-02-15</p> <p>Name: MATH-500</p> <p>Domain: Mathematics</p> <p>Focus: Math reasoning generalization</p> <p>Task Types: Problem solving</p> <p>Metrics: Accuracy</p> <p>Models: unkown</p> Keywords calculus algebra number theory geometry Citation <ul> <li>HuggingFaceH4. Math-500. 2025. URL: https://huggingface.co/datasets/HuggingFaceH4/MATH-500.</li> </ul> <pre><code>@misc{huggingface2025math500,\n  title={MATH-500},\n  author={HuggingFaceH4},\n  year={2025},\n  url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}\n}</code></pre> Ratings CategoryRating Software 0.00 No code provided  Specification 0.00 No method of presentation and evaluation is not stated. No constraints  Dataset 5.00 Problems and solutions are easily downloaded. Could not find a way to download the data  Metrics 2.00 Problem spec states that all of the AI reasoning steps are subject to grading, but no specified way to evaluate the steps  Reference Solution 0.00 Not given  Documentation 0.00 Not given. Implicit instructions to download dataset.  Average rating: 1.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/medqa/","title":"MedQA","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-09-28</p> <p>Name: MedQA</p> <p>Domain: Medical Question Answering</p> <p>Focus: Medical board exam QA</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: Neural reader, Retrieval-based QA systems</p> Keywords USMLE diagnostic QA medical knowledge multilingual Citation <ul> <li>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. 2020. URL: https://arxiv.org/abs/2009.13081, arXiv:2009.13081.</li> </ul> <pre><code>@misc{jin2020diseasedoespatienthave,\n    archiveprefix = {arXiv},\n    author        = {Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},\n    eprint        = {2009.13081},\n    primaryclass  = {cs.CL},\n    title         = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},\n    url           = {https://arxiv.org/abs/2009.13081},\n    year          = {2020}\n  }</code></pre> Ratings CategoryRating Software 5.00 All code available on the github  Specification 3.00 Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.  Dataset 4.00 Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.  Metrics 5.00 Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.  Reference Solution 0.00 No reference solution mentioned.  Documentation 4.00 Paper is available. Evaluation criteria are not mentioned.  Average rating: 3.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/mlcommons_medical_ai/","title":"MLCommons Medical AI","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-07-17</p> <p>Name: MLCommons Medical AI</p> <p>Domain: Healthcare; Medical AI</p> <p>Focus: Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data</p> <p>Task Types: Federated evaluation, Model validation</p> <p>Metrics: ROC AUC, Accuracy, Fairness metrics</p> <p>Models: MedPerf-validated CNNs, GaNDLF workflows</p> Keywords medical AI federated evaluation privacy-preserving fairness healthcare benchmarks Citation <ul> <li>Alexandros Karargyris, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, Prakash Narayana Moorthy, Alexander Chowdhury, Junyi Guo, Sahil Nalawade, Jacob Rosenthal, David Kanter, Maria Xenochristou, Daniel J. Beutel, Verena Chung, Timothy Bergquist, James Eddy, Abubakar Abid, Lewis Tunstall, Omar Sanseviero, Dimitrios Dimitriadis, Yiming Qian, Xinxing Xu, Yong Liu, Rick Siow Mong Goh, Srini Bala, Victor Bittorf, Sreekar Reddy Puchala, Biagio Ricciuti, Soujanya Samineni, Eshna Sengupta, Akshay Chaudhari, Cody Coleman, Bala Desinghu, Gregory Diamos, Debo Dutta, Diane Feddema, Grigori Fursin, Xinyuan Huang, Satyananda Kashyap, Nicholas Lane, Indranil Mallick, Pietro Mascagni, Virendra Mehta, Cassiano Ferro Moraes, Vivek Natarajan, Nikola Nikolov, Nicolas Padoy, Gennady Pekhimenko, Vijay Janapa Reddi, G. Anthony Reina, Pablo Ribalta, Abhishek Singh, Jayaraman J. Thiagarajan, Jacob Albrecht, Thomas Wolf, Geralyn Miller, Huazhu Fu, Prashant Shah, Daguang Xu, Poonam Yadav, David Talby, Mark M. Awad, Jeremy P. Howard, Michael Rosenthal, Luigi Marchionni, Massimo Loda, Jason M. Johnson, Spyridon Bakas, Peter Mattson, FeTS Consortium, BraTS-2020 Consortium, and AI4SafeChole Consortium. Federated benchmarking of medical artificial intelligence with medperf. Nature Machine Intelligence, 5(7):799\u2013810, July 2023. URL: https://doi.org/10.1038/s42256-023-00652-2, doi:10.1038/s42256-023-00652-2.</li> </ul> <pre><code>@article{karargyris2023federated,\n  author = {Karargyris, Alexandros and Umeton, Renato and Sheller, Micah J. and Aristizabal, Alejandro and George, Johnu and Wuest, Anna and Pati, Sarthak and Kassem, Hasan and Zenk, Maximilian and Baid, Ujjwal and Narayana Moorthy, Prakash and Chowdhury, Alexander and Guo, Junyi and Nalawade, Sahil and Rosenthal, Jacob and Kanter, David and Xenochristou, Maria and Beutel, Daniel J. and Chung, Verena and Bergquist, Timothy and Eddy, James and Abid, Abubakar and Tunstall, Lewis and Sanseviero, Omar and Dimitriadis, Dimitrios and Qian, Yiming and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong and Bala, Srini and Bittorf, Victor and Puchala, Sreekar Reddy and Ricciuti, Biagio and Samineni, Soujanya and Sengupta, Eshna and Chaudhari, Akshay and Coleman, Cody and Desinghu, Bala and Diamos, Gregory and Dutta, Debo and Feddema, Diane and Fursin, Grigori and Huang, Xinyuan and Kashyap, Satyananda and Lane, Nicholas and Mallick, Indranil and Mascagni, Pietro and Mehta, Virendra and Moraes, Cassiano Ferro and Natarajan, Vivek and Nikolov, Nikola and Padoy, Nicolas and Pekhimenko, Gennady and Reddi, Vijay Janapa and Reina, G. Anthony and Ribalta, Pablo and Singh, Abhishek and Thiagarajan, Jayaraman J. and Albrecht, Jacob and Wolf, Thomas and Miller, Geralyn and Fu, Huazhu and Shah, Prashant and Xu, Daguang and Yadav, Poonam and Talby, David and Awad, Mark M. and Howard, Jeremy P. and Rosenthal, Michael and Marchionni, Luigi and Loda, Massimo and Johnson, Jason M. and Bakas, Spyridon and Mattson, Peter and FeTS Consortium and BraTS-2020 Consortium and AI4SafeChole Consortium},\n  month = jul,\n  doi = {10.1038/s42256-023-00652-2},\n  journal = {Nature Machine Intelligence},\n  number = {7},\n  pages = {799--810},\n  title = {Federated benchmarking of medical artificial intelligence with MedPerf},\n  url = {https://doi.org/10.1038/s42256-023-00652-2},\n  volume = {5},\n  year = {2023},\n}</code></pre> Ratings CategoryRating Software 5.00 GitHub repository (https://github.com/mlcommons/medical) provides actively maintained open-source tools like MedPerf and GaNDLF for federated medical AI evaluation.  Specification 4.00 The platform defines federated tasks and model evaluation scenarios. Some clinical and system-level constraints are implied but not uniformly formalized across all use cases.  Dataset 4.00 Multi-institutional datasets used in federated settings; real-world data is handled privately onsite, but some FAIR aspects (e.g., accessibility and metadata) are implicit.  Metrics 5.00 Metrics such as ROC AUC, accuracy, and fairness are clearly specified and directly support goals like generalizability and equity.  Reference Solution 3.00 GaNDLF workflows and MedPerf-validated CNNs are referenced, but not all baseline models are centrally documented or easily reproducible.  Documentation 5.00 Extensive documentation, papers, and community support exist. Clear examples and usage instructions are provided in GitHub and publications.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/mlcommons_science/","title":"MLCommons Science","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-06-01</p> <p>Name: MLCommons Science</p> <p>Domain: Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD</p> <p>Focus: AI benchmarks for scientific applications including time-series, imaging, and simulation</p> <p>Task Types: Time-series analysis, Image classification, Simulation surrogate modeling</p> <p>Metrics: MAE, Accuracy, Speedup vs simulation</p> <p>Models: CNN, GNN, Transformer</p> Keywords science AI benchmark MLCommons HPC Citation <ul> <li>Jeyan Thiyagalingam, Gregor von Laszewski, Junqi Yin, Murali Emani, Juri Papay, Gregg Barrett, Piotr Luszczek, Aristeidis Tsaris, Christine Kirkpatrick, Feiyi Wang, Tom Gibbs, Venkatram Vishwanath, Mallikarjun Shankar, Geoffrey Fox, and Tony Hey. Ai benchmarking for science: efforts from the mlcommons science working group. In Hartwig Anzt, Amanda Bienz, Piotr Luszczek, and Marc Baboulin, editors, High Performance Computing. ISC High Performance 2022 International Workshops, 47\u201364. Cham, 2022. Springer International Publishing.</li> </ul> <pre><code>@InProceedings{10.1007/978-3-031-23220-6_4,\n  author=\"Thiyagalingam, Jeyan\n  and von Laszewski, Gregor\n  and Yin, Junqi\n  and Emani, Murali\n  and Papay, Juri\n  and Barrett, Gregg\n  and Luszczek, Piotr\n  and Tsaris, Aristeidis\n  and Kirkpatrick, Christine\n  and Wang, Feiyi\n  and Gibbs, Tom\n  and Vishwanath, Venkatram\n  and Shankar, Mallikarjun\n  and Fox, Geoffrey\n  and Hey, Tony\",\n  editor=\"Anzt, Hartwig\n  and Bienz, Amanda\n  and Luszczek, Piotr\n  and Baboulin, Marc\",\n  title=\"AI Benchmarking for Science: Efforts from the MLCommons Science Working Group\",\n  booktitle=\"High Performance Computing. ISC High Performance 2022 International Workshops\",\n  year=\"2022\",\n  publisher=\"Springer International Publishing\",\n  address=\"Cham\",\n  pages=\"47--64\",\n  abstract=\"With machine learning (ML) becoming a transformative tool for science, the scientific community needs a clear catalogue of ML techniques, and their relative benefits on various scientific problems, if they were to make significant advances in science using AI. Although this comes under the purview of benchmarking, conventional benchmarking initiatives are focused on performance, and as such, science, often becomes a secondary criteria.\",\n  isbn=\"978-3-031-23220-6\"\n}</code></pre> Ratings CategoryRating Software 5.00 Actively maintained GitHub repository available at https://github.com/mlcommons/science with implementations, scripts, and reproducibility support.  Specification 5.00 All five specification aspects are covered: system constraints, task, dataset format, benchmark inputs, and outputs.  Dataset 5.00 Public scientific datasets are used with defined splits. At least 4 FAIR principles are followed.  Metrics 5.00 Clearly defined metrics such as accuracy, training time, and GPU utilization are used. These metrics are explained and effectively capture solution performance.  Reference Solution 5.00 A reference implementation is available, well-documented, trainable/open, and includes full metric evaluation and software/hardware details.  Documentation 5.00 Thorough documentation exists covering the task, background, motivation, evaluation criteria, and includes a supporting paper.  Average rating: 5.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/mlperf_hpc/","title":"MLPerf HPC","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2021-10-20</p> <p>Name: MLPerf HPC</p> <p>Domain: Cosmology, Climate, Protein Structure, Catalysis</p> <p>Focus: Scientific ML training and inference on HPC systems</p> <p>Task Types: Training, Inference</p> <p>Metrics: Training time, Accuracy, GPU utilization</p> <p>Models: CosmoFlow, DeepCAM, OpenCatalyst</p> Keywords HPC training inference scientific ML Citation <ul> <li>Steven Farrell, Murali Emani, Jacob Balma, Lukas Drescher, Aleksandr Drozd, Andreas Fink, Geoffrey Fox, David Kanter, Thorsten Kurth, Peter Mattson, Dawei Mu, Amit Ruhela, Kento Sato, Koichi Shirahata, Tsuguchika Tabaru, Aristeidis Tsaris, Jan Balewski, Ben Cumming, Takumi Danjo, Jens Domke, Takaaki Fukai, Naoto Fukumoto, Tatsuya Fukushi, Balazs Gerofi, Takumi Honda, Toshiyuki Imamura, Akihiko Kasagi, Kentaro Kawakami, Shuhei Kudo, Akiyoshi Kuroda, Maxime Martinasso, Satoshi Matsuoka, Henrique Mendon\u00e7a, Kazuki Minami, Prabhat Ram, Takashi Sawada, Mallikarjun Shankar, Tom St. John, Akihiro Tabuchi, Venkatram Vishwanath, Mohamed Wahib, Masafumi Yamazaki, and Junqi Yin. Mlperf hpc: a holistic benchmark suite for scientific machine learning on hpc systems. 2021. URL: https://arxiv.org/abs/2110.11466, arXiv:2110.11466.</li> </ul> <pre><code>@misc{farrell2021mlperfhpcholisticbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Steven Farrell and Murali Emani and Jacob Balma and Lukas Drescher and Aleksandr Drozd and Andreas Fink and Geoffrey Fox and David Kanter and Thorsten Kurth and Peter Mattson and Dawei Mu and Amit Ruhela and Kento Sato and Koichi Shirahata and Tsuguchika Tabaru and Aristeidis Tsaris and Jan Balewski and Ben Cumming and Takumi Danjo and Jens Domke and Takaaki Fukai and Naoto Fukumoto and Tatsuya Fukushi and Balazs Gerofi and Takumi Honda and Toshiyuki Imamura and Akihiko Kasagi and Kentaro Kawakami and Shuhei Kudo and Akiyoshi Kuroda and Maxime Martinasso and Satoshi Matsuoka and Henrique Mendon\u00e7a and Kazuki Minami and Prabhat Ram and Takashi Sawada and Mallikarjun Shankar and Tom St. John and Akihiro Tabuchi and Venkatram Vishwanath and Mohamed Wahib and Masafumi Yamazaki and Junqi Yin},\n  eprint        = {2110.11466},\n  primaryclass  = {cs.LG},\n  title         = {MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning on HPC Systems},\n  url           = {https://arxiv.org/abs/2110.11466},\n  year          = {2021}\n}</code></pre> Ratings CategoryRating Software 3.00 Reference implementations exist but containerization and environment setup require manual effort across HPC systems.  Specification 4.00 Hardware constraints and I/O formats are not fully defined for all scenarios.  Dataset 5.00 Not all data is independently versioned or comes with standardized FAIR metadata.  Metrics 5.00 None  Reference Solution 4.00 Reproducibility and environment tuning depend on system configuration; baseline models not uniformly bundled.  Documentation 4.00 Central guidance is available but requires domain-specific effort to replicate results across systems.  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/mmlu_massive_multitask_language_understanding/","title":"MMLU (Massive Multitask Language Understanding)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-09-07</p> <p>Name: MMLU  Massive Multitask Language Understanding</p> <p>Domain: Multidomain</p> <p>Focus: Academic knowledge and reasoning across 57 subjects</p> <p>Task Types: Multiple choice</p> <p>Metrics: Accuracy</p> <p>Models: GPT-4o, Gemini 1.5 Pro, o1, DeepSeek-R1</p> Keywords multitask multiple-choice zero-shot few-shot knowledge probing Citation <ul> <li>Dan Hendrycks, Collin Burns, and Saurav Kadavath. Measuring massive multitask language understanding. 2021. URL: https://arxiv.org/abs/2009.03300.</li> </ul> <pre><code>@misc{hendrycks2021measuring,\n  title={Measuring Massive Multitask Language Understanding},\n  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav},\n  journal={arXiv preprint arXiv:2009.03300},\n  year={2021},\n  url={https://arxiv.org/abs/2009.03300}\n}</code></pre> Ratings CategoryRating Software 0.00 No instructions to download or run data given on the site  Specification 4.00 No system constraints  Dataset 5.00 Meets all FAIR principles and properly versioned.  Metrics 5.00 Fully defined, represents a solution's performance.  Reference Solution 2.00 Reference models are available (i.e. GPT-3), but are not trainable or publicly documented  Documentation 5.00 Well-explained in a provided paper.  Average rating: 3.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/molgen/","title":"MOLGEN","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-01-26</p> <p>Name: MOLGEN</p> <p>Domain: Computational Chemistry</p> <p>Focus: Molecular generation and optimization</p> <p>Task Types: Distribution learning, Goal-oriented generation</p> <p>Metrics: Validity%, Novelty%, QED, Docking score</p> <p>Models: MolGen</p> Keywords SELFIES GAN property optimization Citation <ul> <li>Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen. Domain-agnostic molecular generation with chemical feedback. 2024. URL: https://arxiv.org/abs/2301.11259, arXiv:2301.11259.</li> </ul> <pre><code>@misc{fang2024domainagnosticmoleculargenerationchemical,\n  archiveprefix = {arXiv},\n  author        = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},\n  eprint        = {2301.11259},\n  primaryclass  = {cs.LG},\n  title         = {Domain-Agnostic Molecular Generation with Chemical Feedback},\n  url           = {https://arxiv.org/abs/2301.11259},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 0.00 This is a pre-trained model  Specification 0.00 This is a pre-trained model  Dataset 0.00 This is a pre-trained model  Metrics 0.00 This is a pre-trained model  Reference Solution 0.00 This is a pre-trained model  Documentation 0.00 This is a pre-trained model  Average rating: 0.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/neural_architecture_codesign_for_fast_physics_applications/","title":"Neural Architecture Codesign for Fast Physics Applications","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-01-09</p> <p>Name: Neural Architecture Codesign for Fast Physics Applications</p> <p>Domain: Physics; Materials Science; Particle Physics</p> <p>Focus: Automated neural architecture search and hardware-efficient model codesign for fast physics applications</p> <p>Task Types: Classification, Peak finding</p> <p>Metrics: Accuracy, Latency, Resource utilization</p> <p>Models: NAC-based BraggNN, NAC-optimized Deep Sets (jet)</p> Keywords neural architecture search FPGA deployment quantization pruning hls4ml Citation <ul> <li>Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, and Javier Duarte. Neural architecture codesign for fast physics applications. 2025. URL: https://arxiv.org/abs/2501.05515, arXiv:2501.05515.</li> </ul> <pre><code>@misc{weitz2025neuralarchitecturecodesignfast,\n  archiveprefix={arXiv},\n  author={Jason Weitz and Dmitri Demler and Luke McDermott and Nhan Tran and Javier Duarte},\n  eprint={2501.05515},\n  primaryclass={cs.LG},\n  title={Neural Architecture Codesign for Fast Physics Applications},\n  url={https://arxiv.org/abs/2501.05515},\n  year={2025}\n}</code></pre> Ratings CategoryRating Software 3.00 Toolchain (hls4ml, nac-opt) described but not yet containerized or fully packaged  Specification 5.00 Fully specified task with constraints and target deployment; includes hardware context  Dataset 2.00 Simulated datasets referenced but not publicly available or FAIR-compliant  Metrics 5.00 Clear, quantitative metrics aligned with task goals and hardware evaluation  Reference Solution 4.00 Models tested on hardware with source code references; full training pipeline not yet released  Documentation 4.00 Detailed paper and tools described; open repo planned but not yet complete  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/nixtla_neural_forecast_nhits/","title":"Nixtla Neural Forecast NHITS","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-06-01</p> <p>Name: Nixtla Neural Forecast NHITS</p> <p>Domain: Time-series; General ML</p> <p>Focus: Official NHITS implementation for long-horizon time series forecasting</p> <p>Task Types: Time-series forecasting</p> <p>Metrics: RMSE, MAPE</p> <p>Models: NHITS</p> Keywords NHITS long-horizon forecasting neural interpolation time-series Citation <ul> <li>Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, and Artur Dubrawski. Nhits: neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 37, 6989\u20136997. 2023.</li> </ul> <pre><code>@inproceedings{challu2023nhits,\n title={Nhits: Neural hierarchical interpolation for time series forecasting},\n author={Challu, Cristian and Olivares, Kin G and Oreshkin, Boris N and Ramirez, Federico Garza and Canseco, Max Mergenthaler and Dubrawski, Artur},\n booktitle={Proceedings of the AAAI conference on artificial intelligence},\n volume={37},\n number={6},\n pages={6989--6997},\n year={2023}\n }</code></pre> Ratings CategoryRating Software 5.00 Implemented within the open-source NeuralForecast library under Apache 2.0. Includes training, evaluation, and hyperparameter tuning pipelines. Actively maintained.  Specification 5.00 The NHITS forecasting task is clearly defined with structured input/output formats. Model design targets long-horizon accuracy and compute efficiency.  Dataset 3.00 Uses standard benchmark datasets like M4, but does not bundle them directly. FAIR compliance depends on external dataset sources and user setup.  Metrics 5.00 Evaluated using RMSE, MAPE, and other standard forecasting metrics, integrated into training and evaluation APIs.  Reference Solution 4.00 Official NHITS implementation is fully reproducible with training/eval configs, though pretrained weights are not always provided.  Documentation 4.00 Well-documented on GitHub and in AAAI paper, with code examples, training guidance, and usage tutorials. More model-specific docs could improve clarity further.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/nixtla_neural_forecast_timegpt/","title":"Nixtla Neural Forecast TimeGPT","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-10-05</p> <p>Name: Nixtla Neural Forecast TimeGPT</p> <p>Domain: Time-series; General ML</p> <p>Focus: Time-series foundation model \"TimeGPT\" for forecasting and anomaly detection</p> <p>Task Types: Time-series forecasting, Anomaly detection</p> <p>Metrics: RMSE, Anomaly detection metrics</p> <p>Models: TimeGPT</p> Keywords TimeGPT foundation model time-series generative model Citation <ul> <li>Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1. 2024. URL: https://arxiv.org/abs/2310.03589, arXiv:2310.03589.</li> </ul> <pre><code>@misc{garza2024timegpt1,\n  archiveprefix = {arXiv},\n  author        = {Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},\n  eprint        = {2310.03589},\n  primaryclass  = {cs.LG},\n  title         = {TimeGPT-1},\n  url           = {https://arxiv.org/abs/2310.03589},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 4.00 Fully open-source Apache 2.0 implementation integrated in NeuralForecast, supporting training and evaluation via API. Production-grade deployment available via Nixtla API and Azure.  Specification 3.00 Concept and forecasting goals are described, but formal input/output definitions and task constraints are not rigorously specified.  Dataset 3.00 Evaluated on existing open datasets, but consolidated data release, splits, and FAIR metadata are not provided.  Metrics 4.00 Uses standard forecasting metrics such as RMSE, MASE, SMAPE, and anomaly detection metrics consistently across evaluations.  Reference Solution 3.00 TimeGPT implementation is available, but baseline comparisons and additional reference models are limited.  Documentation 3.00 Basic README with installation and usage examples; more detailed API docs and tutorials would improve usability.  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/nixtla_neural_forecast_timellm/","title":"Nixtla Neural Forecast TimeLLM","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-10-03</p> <p>Name: Nixtla Neural Forecast TimeLLM</p> <p>Domain: Time-series; General ML</p> <p>Focus: Reprogramming LLMs for time series forecasting</p> <p>Task Types: Time-series forecasting</p> <p>Metrics: RMSE, MAPE</p> <p>Models: Time-LLM</p> Keywords Time-LLM language model time-series reprogramming Citation <ul> <li>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: time series forecasting by reprogramming large language models. 2024. URL: https://arxiv.org/abs/2310.01728, arXiv:2310.01728.</li> </ul> <pre><code>@misc{jin2024timellmtimeseriesforecasting,\n  title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models}, \n  author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},\n  year={2024},\n  eprint={2310.01728},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2310.01728}, \n}</code></pre> Ratings CategoryRating Software 4.00 Fully open-source under Apache 2.0, integrated into the NeuralForecast library. Includes Time-LLM implementation with example usage and training scripts.  Specification 3.00 High-level framing of forecasting as language modeling is clear, but detailed input/output specifications, constraints, and task formalization are minimal.  Dataset 3.00 Evaluated on standard datasets like M4 and ETT, but dataset splits and versioning are not bundled or explicitly FAIR-compliant.  Metrics 4.00 Standard forecasting metrics such as RMSE, MAPE, and SMAPE are reported. Evaluation is consistent, though deeper metric justification is limited.  Reference Solution 3.00 Time-LLM implementation is open and reproducible, but limited baselines or comparative implementations are included directly.  Documentation 3.00 GitHub README provides installation and quick usage examples, but lacks detailed API docs, training walkthroughs, or extended tutorials.  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/nixtla_neuralforecast/","title":"Nixtla NeuralForecast","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-04-01</p> <p>Name: Nixtla NeuralForecast</p> <p>Domain: Time-series forecasting; General ML</p> <p>Focus: High-performance neural forecasting library with &gt;30 models</p> <p>Task Types: Time-series forecasting</p> <p>Metrics: RMSE, MAPE, CRPS</p> <p>Models: NBEATS, NHITS, TFT, DeepAR</p> Keywords time-series neural forecasting NBEATS, NHITS, TFT probabilistic forecasting usability Citation <ul> <li>Kin G. Olivares, Cristian Chall\u00fa, Federico Garza, Max Mergenthaler Canseco, and Artur Dubrawski. Neuralforecast: user friendly state-of-the-art neural forecasting models. PyCon Salt Lake City, Utah, US 2022, 2022. URL: https://github.com/Nixtla/neuralforecast.</li> </ul> <pre><code>@misc{olivares2022library_neuralforecast,\n  author={Kin G. Olivares and Cristian Chall\u00fa and Federico Garza and Max Mergenthaler Canseco and Artur Dubrawski},\n  title = {NeuralForecast: User friendly state-of-the-art neural forecasting models.},\n  year={2022},\n  howpublished={PyCon Salt Lake City, Utah, US 2022},\n  url={https://github.com/Nixtla/neuralforecast}\n}</code></pre> Ratings CategoryRating Software 5.00 Actively maintained open-source library under Apache 2.0. Offers a clean API, extensive model zoo (&gt;30 models), integration with Ray, Optuna, and supports scalable training and inference workflows.  Specification 5.00 Forecasting task is well-defined with clear input/output structures. Framework supports probabilistic and deterministic forecasting, with unified interfaces and support for batch evaluation.  Dataset 3.00 NeuralForecast does not include its own datasets but supports standard datasets (e.g., M4, M5, ETT). FAIR compliance depends on user-supplied data.  Metrics 5.00 RMSE, MAPE, CRPS, and other domain-relevant metrics are well supported and integrated into the evaluation loop.  Reference Solution 4.00 Includes runnable model baselines and training scripts for all supported models. Some models have pretrained weights, but not all are fully benchmarked out-of-the-box.  Documentation 5.00 Rich documentation with examples, API references, tutorials, notebooks, and CLI support. PyPI, GitHub, and official blog posts offer clear guidance for usage and extension.  Average rating: 4.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/ocp_open_catalyst_project/","title":"OCP (Open Catalyst Project)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-10-20</p> <p>Name: OCP  Open Catalyst Project</p> <p>Domain: Chemistry; Materials Science</p> <p>Focus: Catalyst adsorption energy prediction</p> <p>Task Types: Energy prediction, Force prediction</p> <p>Metrics: MAE (energy), MAE (force)</p> <p>Models: CGCNN, SchNet, DimeNet++, GemNet-OC</p> Keywords DFT relaxations adsorption energy graph neural networks Citation <ul> <li>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. The open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059\u20136072, 2021. URL: https://pubs.acs.org/doi/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.</li> </ul> <pre><code>@article{chanussot2021oc20,\n  title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},\n  author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},\n  journal   = {ACS Catalysis},\n  volume    = {11},\n  number    = {10},\n  pages     = {6059--6072},\n  year      = {2021},\n  doi       = {10.1021/acscatal.0c04525},\n  url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}\n}</code></pre> <ul> <li>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066\u20133084, 2023. URL: https://pubs.acs.org/doi/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.</li> </ul> <pre><code>@article{tran2023oc22,\n  title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},\n  author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, F\u00e9lix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},\n  journal   = {ACS Catalysis},\n  volume    = {13},\n  number    = {5},\n  pages     = {3066--3084},\n  year      = {2023},\n  doi       = {10.1021/acscatal.2c05426},\n  url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}\n}</code></pre> <ul> <li>Lowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst 2020 (oc20) dataset and community challenges. ACS Catalysis, 11(10):6059 6072, 2021. URL: https://doi.org/10.1021/acscatal.0c04525, arXiv:https://doi.org/10.1021/acscatal.0c04525, doi:10.1021/acscatal.0c04525.</li> </ul> <pre><code>@article{doi:10.1021/acscatal.0c04525,\n  author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},\ntitle = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},\n  journal = {ACS Catalysis},\n  volume = {11},\n  number = {10},\n  pages = {6059-6072},\n  year = {2021},\n  doi = {10.1021/acscatal.0c04525},\n  URL = {https://doi.org/10.1021/acscatal.0c04525},eprint = {https://doi.org/10.1021/acscatal.0c04525}}\"</code></pre> <ul> <li>Richard Tran, Janice Lan, Muhammed Shuaibi, Brandon M. Wood, Siddharth Goyal, Abhishek Das, Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, F\u00e9lix Therrien, Jehad Abed, Oleksandr Voznyy, Edward H. Sargent, Zachary Ulissi, and C. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13(5):3066 3084, February 2023. URL: http://dx.doi.org/10.1021/acscatal.2c05426, doi:10.1021/acscatal.2c05426.</li> </ul> <pre><code>@article{tran2023b,\n  title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},\n  volume={13},\n  ISSN={2155-5435},\n  url={http://dx.doi.org/10.1021/acscatal.2c05426},\n  DOI={10.1021/acscatal.2c05426},\n  number={5},\n  journal={ACS Catalysis},\n  publisher={American Chemical Society (ACS)},\n  author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, F\u00e9lix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},\n  year={2023},\n  month=feb, pages={3066-3084} \n}</code></pre> Ratings CategoryRating Software 5.00 Data provided in Github links  Specification 5.00 Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.  Dataset 5.00 Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.  Metrics 5.00 MAE (energy and force) are standard and reproducible.  Reference Solution 4.00 Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated. No hardware listed.  Documentation 1.00 Paper exists, but content is behind a paywall.  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/open_graph_benchmark_ogb_-_biology/","title":"Open Graph Benchmark (OGB) - Biology","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2020-05-02</p> <p>Name: Open Graph Benchmark  OGB  - Biology</p> <p>Domain: Graph ML</p> <p>Focus: Biological graph property prediction</p> <p>Task Types: Node property prediction, Link property prediction, Graph property prediction</p> <p>Metrics: Accuracy, ROC-AUC</p> <p>Models: GCN, GraphSAGE, GAT</p> Keywords node prediction link prediction graph classification Citation <ul> <li>Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: datasets for machine learning on graphs. 2021. URL: https://arxiv.org/abs/2005.00687, arXiv:2005.00687.</li> </ul> <pre><code>@misc{hu2021opengraphbenchmarkdatasets,\n    archiveprefix = {arXiv},\n    author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},\n    eprint        = {2005.00687},\n    primaryclass  = {cs.LG},\n    title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},\n    url           = {https://arxiv.org/abs/2005.00687},\n    year          = {2021}\n}</code></pre> Ratings CategoryRating Software 5.00 All necessary information is provided on the Github  Specification 4.00 Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined. No constraints.  Dataset 5.00 Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.  Metrics 5.00 Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.  Reference Solution 3.00 Multiple baselines implemented and documented (GCN, GAT, GraphSAGE). No contraints.  Documentation 5.00 All necessary information is included in a paper.  Average rating: 4.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/papers_with_code_sota_platform/","title":"Papers With Code (SOTA Platform)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: ongoing</p> <p>Name: Papers With Code  SOTA Platform</p> <p>Domain: General ML; All domains</p> <p>Focus: Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers</p> <p>Task Types: Multiple (Classification, Detection, NLP, etc.)</p> <p>Metrics: Task-specific (Accuracy, F1, BLEU, etc.)</p> <p>Models: All published models with code</p> Keywords leaderboard benchmarking reproducibility open-source Citation <ul> <li>Avrim Blum and Moritz Hardt. The ladder: a reliable leaderboard for machine learning competitions. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, 1006\u20131014. Lille, France, July 2015. PMLR. URL: https://proceedings.mlr.press/v37/blum15.html.</li> </ul> <pre><code>@InProceedings{pmlr-v37-blum15,\n  title =    {The Ladder: A Reliable Leaderboard for Machine Learning Competitions},\n  author =   {Blum, Avrim and Hardt, Moritz},\n  booktitle =        {Proceedings of the 32nd International Conference on Machine Learning},\n  pages =    {1006--1014},\n  year =     {2015},\n  editor =   {Bach, Francis and Blei, David},\n  volume =   {37},\n  series =   {Proceedings of Machine Learning Research},\n  address =          {Lille, France},\n  month =    jul,\n  publisher =    {PMLR},\n  pdf =      {http://proceedings.mlr.press/v37/blum15.pdf},\n  url =      {https://proceedings.mlr.press/v37/blum15.html},\n  abstract =         {The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.}\n}</code></pre> Ratings CategoryRating Software 5.00 Actively maintained open-source platform (https://paperswithcode.com) under Apache 2.0 license; includes automatic integration with GitHub, datasets, and models for reproducibility.  Specification 4.00 Task and benchmark structures are well organized and standardized, but due to its broad coverage, input/output formats vary significantly between tasks and are not always tightly controlled.  Dataset 3.00 Relies on external datasets submitted by the community. While links are available, FAIR compliance is not guaranteed or systematically enforced across all benchmarks.  Metrics 5.00 Tracks state-of-the-art using task-specific metrics like Accuracy, F1, BLEU, etc., with consistent aggregation and historical SOTA tracking.  Reference Solution 3.00 Provides links to implementations of many SOTA models, but no single unified reference baseline is required or maintained per benchmark.  Documentation 4.00 Strong front-end documentation and metadata on benchmarks, tasks, and models; however, some benchmark-specific instructions are sparse or dependent on external paper links.  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/pdebench/","title":"PDEBench","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-10-13</p> <p>Name: PDEBench</p> <p>Domain: CFD; Weather Modeling</p> <p>Focus: Benchmark suite for ML-based surrogates solving time-dependent PDEs</p> <p>Task Types: Supervised Learning</p> <p>Metrics: RMSE, boundary RMSE, Fourier RMSE</p> <p>Models: FNO, U-Net, PINN, Gradient-Based inverse methods</p> Keywords PDEs CFD scientific ML surrogate modeling NeurIPS Citation <ul> <li>Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pfl\u00fcger, and Mathias Niepert. Pdebench: an extensive benchmark for scientific machine learning. 2024. URL: https://arxiv.org/abs/2210.07182, arXiv:2210.07182.</li> </ul> <pre><code>@misc{takamoto2024pdebenchextensivebenchmarkscientific,\n  archiveprefix = {arXiv},\n  author        = {Makoto Takamoto and Timothy Praditia and Raphael Leiteritz and Dan MacKinlay and Francesco Alesiani and Dirk Pfl\u00fcger and Mathias Niepert},\n  eprint        = {2210.07182},\n  primaryclass  = {cs.LG},\n  title         = {PDEBENCH: An Extensive Benchmark for Scientific Machine Learning},\n  url           = {https://arxiv.org/abs/2210.07182},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 5.00 GitHub repository (https://github.com/pdebench/PDEBench) is actively maintained and includes training pipelines, data loaders, and evaluation scripts. Installation and usage are well-documented.  Specification 5.00 Clearly defined tasks for forward and inverse PDE problems, with structured input/output formats, system constraints, and task specifications.  Dataset 5.00 Diverse PDE datasets (synthetic and real-world) hosted on DaRUS with DOIs. Datasets are well-documented, structured, and follow FAIR practices.  Metrics 4.00 Includes RMSE, boundary RMSE, and Fourier-domain RMSE. These are well-suited to PDE problems, though rationale behind metric choices could be expanded in some cases.  Reference Solution 4.00 Baselines (FNO, U-Net, PINN, etc.) are available and documented, but not every model includes full training and evaluation reproducibility out-of-the-box.  Documentation 4.00 Strong documentation on GitHub including examples, configs, and usage instructions. Some model-specific details and tutorials could be further expanded.  Average rating: 4.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/quantum_computing_benchmarks_qml/","title":"Quantum Computing Benchmarks (QML)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-02-22</p> <p>Name: Quantum Computing Benchmarks  QML</p> <p>Domain: Quantum Computing</p> <p>Focus: Quantum algorithm performance evaluation</p> <p>Task Types: Circuit benchmarking, State classification</p> <p>Metrics: Fidelity, Success probability</p> <p>Models: IBM Q, IonQ, AQT@LBNL</p> Keywords quantum circuits state preparation error correction Citation <ul> <li>Florian J. Kiwit, Marwa Marso, Philipp Ross, Carlos A. Riofr\u00edo, Johannes Klepsch, and Andre Luckow. Application-oriented benchmarking of quantum generative learning using quark. In 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), 475 484. IEEE, September 2023. URL: http://dx.doi.org/10.1109/QCE57702.2023.00061, doi:10.1109/qce57702.2023.00061.</li> </ul> <pre><code>@inproceedings{kiwit2023,\n  title={Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK},\n  url={http://dx.doi.org/10.1109/QCE57702.2023.00061},\n  DOI={10.1109/qce57702.2023.00061},\n  booktitle={2023 IEEE International Conference on Quantum Computing and Engineering (QCE)},\n  publisher={IEEE},\n  author={Kiwit, Florian J. and Marso, Marwa and Ross, Philipp and Riofr\u00edo, Carlos A. and Klepsch, Johannes and Luckow, Andre},\n  year={2023},\n  month=sep, pages={475-484}\n}</code></pre> Ratings CategoryRating Software 4.00 Run instructions exist, but are not easy to follow  Specification 3.00 No system constraints. Task clarity and dataset format are not clearly specified.  Dataset 4.00 Datasets are accessible, but not split.  Metrics 3.00 Partially defined, somewhat inferrable metrics. Unknown whether a system's performance is captured.  Reference Solution 0.00 Not provided  Documentation 1.00 Only the task is defined.   Average rating: 2.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/quench_detection/","title":"Quench detection","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-10-15</p> <p>Name: Quench detection</p> <p>Domain: Accelerators and Magnets</p> <p>Focus: Real-time detection of superconducting magnet quenches using ML</p> <p>Task Types: Anomaly detection, Quench localization</p> <p>Metrics: ROC-AUC, Detection latency</p> <p>Models: Autoencoder, RL agents (in development)</p> Keywords quench detection autoencoder anomaly detection real-time Citation <ul> <li>Maira Khan, Steve Krave, Vittorio Marinozzi, Jennifer Ngadiuba, Stoyan Stoynev, and Nhan Tran. Benchmarking and interpreting real time quench detection algorithms. In Fast Machine Learning for Science Conference 2024. Purdue University, IN, October 2024. indico.cern.ch. URL: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf.</li> </ul> <pre><code>@inproceedings{quench2024,\n  author = {Maira Khan and Steve Krave and Vittorio Marinozzi and Jennifer Ngadiuba and Stoyan Stoynev and Nhan Tran},\n  title = {Benchmarking and Interpreting Real Time Quench Detection Algorithms},\n  booktitle = {Fast Machine Learning for Science Conference 2024},\n  year = {2024},\n  month = oct,\n  address = {Purdue University, IN},\n  publisher = {indico.cern.ch},\n  url = {https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf}\n}</code></pre> Ratings CategoryRating Software 1.00 Code not provided; no evidence of documentation or containerization  Specification 4.00 Real-time detection task is clearly described, but exact constraints, inputs/outputs, and evaluation protocol are only partially specified  Dataset 2.00 Dataset URL is missing; FAIR principles largely unmet  Metrics 3.00 ROC-AUC and latency are mentioned, but metric definitions and formal evaluation setup are missing  Reference Solution 1.00 No baseline or reproducible model implementation available  Documentation 2.00 Only a conference slide deck is available; lacks detailed instructions or repository for reproduction  Average rating: 2.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/sabath_sbi-fair/","title":"Sabath (SBI-FAIR)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2021-09-27</p> <p>Name: Sabath  SBI-FAIR</p> <p>Domain: Systems; Metadata</p> <p>Focus: FAIR metadata framework for ML-driven surrogate workflows in HPC systems</p> <p>Task Types: Systems benchmarking</p> <p>Metrics: Metadata completeness, FAIR compliance</p> <p>Models: NA</p> Keywords meta-benchmark metadata HPC surrogate modeling Citation <ul> <li>Piotr Luszczek. Sabath: fair metadata technology for surrogate benchmarks. Technical Report, University of Tennessee, 2021. URL: https://github.com/icl-utk-edu/slip/tree/sabath.</li> </ul> <pre><code>@techreport{luszczek2021sabath,\n  title={SABATH: FAIR Metadata Technology for Surrogate Benchmarks},\n  author={Luszczek, Piotr},\n  year={2021},\n  institution={University of Tennessee},\n  url={https://github.com/icl-utk-edu/slip/tree/sabath}\n}</code></pre> Ratings CategoryRating Software 4.00 Actively maintained GitHub repository (https://github.com/icl-utk-edu/slip/tree/sabath) with BSD-licensed tooling for FAIR metadata capture; integrates with existing surrogate modeling benchmarks.  Specification 4.00 FAIR metadata structure and logging goals are clearly described. Input/output definitions are implied through integrations (e.g., MiniWeatherML), though not always formalized.  Dataset 4.00 Datasets used in surrogate benchmarks are publicly available, well-structured, and FAIR-aligned, but not independently hosted by Sabath itself.  Metrics 4.00 Emphasizes metadata completeness and FAIR compliance. Metrics are clear and well-matched to its metadata-focused benchmarking context.  Reference Solution 3.00 Includes integration with multiple surrogate benchmarks and models, though not all are fully documented or packaged as standardized reference solutions.  Documentation 3.00 Basic instructions and code are provided on GitHub, but more detailed walkthroughs, use-case examples, or tutorials are limited.  Average rating: 3.67/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/satimgnet/","title":"SatImgNet","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-04-23</p> <p>Name: SatImgNet</p> <p>Domain: Remote Sensing</p> <p>Focus: Satellite imagery classification</p> <p>Task Types: Image classification</p> <p>Metrics: Accuracy</p> <p>Models: CLIP, BLIP, ALBEF</p> Keywords land-use zero-shot multi-task Citation <ul> <li>Jonathan Roberts, Kai Han, and Samuel Albanie. Satin: a multi-task metadataset for classifying satellite imagery using vision-language models. 2023. URL: https://huggingface.co/datasets/saral-ai/satimagnet.</li> </ul> <pre><code>@misc{roberts2023satin,\n  title={SATIN: A multi-task metadataset for classifying satellite imagery using vision-language models},\n  author={Roberts, Jonathan and Han, Kai and Albanie, Samuel},\n  journal={arXiv preprint arXiv:2304.11619},\n  year={2023},\n  url={https://huggingface.co/datasets/saral-ai/satimagnet}\n}</code></pre> Ratings CategoryRating Software 0.00 No scripts or environment information provided  Specification 4.00 Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.  Dataset 5.00 Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.  Metrics 5.00 Accuracy of classification is an appropriate metric  Reference Solution 4.00 Baselines like CLIP, BLIP, ALBEF evaluated in the paper; no constraints specified  Documentation 5.00 Paper provides all required information  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/scicode/","title":"SciCode","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-07-18</p> <p>Name: SciCode</p> <p>Domain: Scientific Programming</p> <p>Focus: Scientific code generation and problem solving</p> <p>Task Types: Coding</p> <p>Metrics: Solve rate (%)</p> <p>Models: Claude3.5-Sonnet</p> Keywords code synthesis scientific computing programming benchmark Citation <ul> <li>Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: a research coding benchmark curated by scientists. 2024. URL: https://arxiv.org/abs/2407.13168, arXiv:2407.13168.</li> </ul> <pre><code>@misc{tian2024scicoderesearchcodingbenchmark,\n  archiveprefix = {arXiv},\n  author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},\n  eprint        = {2407.13168},\n  primaryclass  = {cs.AI},\n  title         = {SciCode: A Research Coding Benchmark Curated by Scientists},\n  url           = {https://arxiv.org/abs/2407.13168},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 5.00 Code to run exists on github repo  Specification 4.50 Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.  Dataset 0.00 Paper and website had no link to any dataset. It may still exist somewhere  Metrics 2.00 Metrics stated, but method of grading is not specified  Reference Solution 1.00 Models presented with scores, but none are open or list constraints  Documentation 4.00 Paper containing all needed info except for evlauation criteria  Average rating: 2.75/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/seafloorai/","title":"SeafloorAI","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: SeafloorAI</p> <p>Domain: Marine Science; Vision-Language</p> <p>Focus: Large-scale vision-language dataset for seafloor mapping and geological classification</p> <p>Task Types: Image segmentation, Vision-language QA</p> <p>Metrics: Segmentation pixel accuracy, QA accuracy</p> <p>Models: SegFormer, ViLT-style multimodal models</p> Keywords sonar imagery vision-language seafloor mapping segmentation QA Citation <ul> <li>Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, and Xi Peng. Seafloorai: a large-scale vision-language dataset for seafloor geological survey. 2024. URL: https://arxiv.org/abs/2411.00172, arXiv:2411.00172.</li> </ul> <pre><code>@misc{nguyen2024seafloor,\n  archiveprefix = {arXiv},\n  author = {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},\n  eprint = {2411.00172},\n  primaryclass = {cs.CV},\n  title = {SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},\n  url = {https://arxiv.org/abs/2411.00172},\n  year=2024\n}</code></pre> Ratings CategoryRating Software 3.00 Data processing code is publicly available, but no full benchmark framework or runnable model implementations are provided yet.  Specification 5.00 Tasks (image segmentation and vision-language QA) are clearly defined with geospatial and multimodal objectives well specified.  Dataset 5.00 Large-scale, well-annotated sonar imagery dataset with segmentation masks and natural language descriptions; curated with domain experts.  Metrics 5.00 Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified and appropriate for the tasks.  Reference Solution 4.00 Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but reproducible code or pretrained weights are not fully available yet.  Documentation 4.00 Dataset description and data processing instructions are provided, but tutorials and benchmark usage guides are limited.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/sglang_framework/","title":"SGLang Framework","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-12-12</p> <p>Name: SGLang Framework</p> <p>Domain: LLM Vision</p> <p>Focus: Fast serving framework for LLMs and vision-language models</p> <p>Task Types: Model serving framework</p> <p>Metrics: Tokens/sec, Time-to-first-token, Throughput gain vs baseline</p> <p>Models: LLaVA, DeepSeek, Llama</p> Keywords LLM serving vision-language RadixAttention performance JSON decoding Citation <ul> <li>Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: efficient execution of structured language model programs. 2024. URL: https://arxiv.org/abs/2312.07104, arXiv:2312.07104.</li> </ul> <pre><code>@misc{zheng2024sglangefficientexecutionstructured,\n  archiveprefix = {arXiv},\n  author        = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},\n  eprint        = {2312.07104},\n  primaryclass  = {cs.AI},\n  title         = {SGLang: Efficient Execution of Structured Language Model Programs},\n  url           = {https://arxiv.org/abs/2312.07104},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 5.00 Actively maintained and production-deployed (e.g., xAI, NVIDIA); source code available under Apache 2.0. Includes efficient backends (RadixAttention, quantization, batching) and full serving infrastructure.  Specification 4.00 The framework clearly defines performance targets, serving logic, and model integration. Input/output expectations are consistent, but not all benchmarks are standardized.  Dataset 2.00 Does not introduce new datasets; instead, it evaluates performance using existing model benchmarks. Only configuration files are included.  Metrics 5.00 Serving-related metrics such as tokens/sec, time-to-first-token, and throughput gain vs. baselines are well-defined and consistently applied.  Reference Solution 3.00 Provides benchmark configs and example integrations (e.g., with LLaVA, DeepSeek), but not all models or scripts are runnable out-of-the-box.  Documentation 4.00 Strong GitHub documentation, install guides, and benchmarks. Some advanced topics (e.g., scaling, hardware tuning) could use deeper walkthroughs.  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/single_qubit_readout_on_qick_system/","title":"Single Qubit Readout on QICK System","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2025-01-24</p> <p>Name: Single Qubit Readout on QICK System</p> <p>Domain: Quantum Computing</p> <p>Focus: Real-time single-qubit state classification using FPGA firmware</p> <p>Task Types: Classification</p> <p>Metrics: Accuracy, Latency</p> <p>Models: hls4ml quantized NN</p> Keywords qubit readout hls4ml FPGA QICK Citation <ul> <li>Giuseppe Di Guglielmo, Botao Du, Javier Campos, Alexandra Boltasseva, Akash V. Dixit, Farah Fahim, Zhaxylyk Kudyshev, Santiago Lopez, Ruichao Ma, Gabriel N. Perdue, Nhan Tran, Omer Yesilyurt, and Daniel Bowring. End-to-end workflow for machine learning-based qubit readout with qick and hls4ml. 2025. URL: https://arxiv.org/abs/2501.14663, arXiv:2501.14663.</li> </ul> <pre><code>@misc{diguglielmo2025endtoendworkflowmachinelearningbased,\n  archiveprefix = {arXiv},\n  author        = {Giuseppe Di Guglielmo and Botao Du and Javier Campos and Alexandra Boltasseva and Akash V. Dixit and Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and Ruichao Ma and Gabriel N. Perdue and Nhan Tran and Omer Yesilyurt and Daniel Bowring},\n  eprint        = {2501.14663},\n  primaryclass  = {quant-ph},\n  title         = {End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml},\n  url           = {https://arxiv.org/abs/2501.14663},\n  year          = {2025}\n}</code></pre> Ratings CategoryRating Software 3.00 Code and FPGA firmware available on GitHub; integration with hls4ml demonstrated. Some deployment details and examples are provided but overall software maturity is moderate.  Specification 4.00 Task clearly defined: real-time single-qubit state classification with latency and fidelity constraints. Labeling and ground truth definitions could be more explicit.  Dataset 4.00 Dataset hosted on Zenodo with structured data; however, detailed documentation on image acquisition and labeling pipeline is limited.  Metrics 5.00 Standard classification metrics (accuracy, latency) are used and directly relevant to the quantum readout task.  Reference Solution 1.00 No baseline or starter models with runnable code are linked publicly.  Documentation 4.00 Codabench task page and GitHub repo provide descriptions and usage instructions, but detailed API or deployment tutorials are limited.  Average rating: 3.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/smart_pixels_for_lhc/","title":"Smart Pixels for LHC","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-06-24</p> <p>Name: Smart Pixels for LHC</p> <p>Domain: Particle Physics; Instrumentation and Detectors</p> <p>Focus: On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors</p> <p>Task Types: Image Classification, Data filtering</p> <p>Metrics: Data rejection rate, Power per pixel</p> <p>Models: 2-layer pixel NN</p> Keywords smart pixel on-sensor inference data reduction trigger Citation <ul> <li>Benjamin Parpillon, Chinar Syal, Jieun Yoo, Jennet Dickinson, Morris Swartz, Giuseppe Di Guglielmo, Alice Bean, Douglas Berry, Manuel Blanco Valentin, Karri DiPetrillo, Anthony Badea, Lindsey Gray, Petar Maksimovic, Corrinne Mills, Mark S. Neubauer, Gauri Pradhan, Nhan Tran, Dahai Wen, and Farah Fahim. Smart pixels: in-pixel ai for on-sensor data filtering. 2024. URL: https://arxiv.org/abs/2406.14860, arXiv:2406.14860.</li> </ul> <pre><code>@misc{parpillon2024smartpixelsinpixelai,\n  archiveprefix = {arXiv},\n  author        = {Benjamin Parpillon and Chinar Syal and Jieun Yoo and Jennet Dickinson and Morris Swartz and Giuseppe Di Guglielmo and Alice Bean and Douglas Berry and Manuel Blanco Valentin and Karri DiPetrillo and Anthony Badea and Lindsey Gray and Petar Maksimovic and Corrinne Mills and Mark S. Neubauer and Gauri Pradhan and Nhan Tran and Dahai Wen and Farah Fahim},\n  eprint        = {2406.14860},\n  primaryclass  = {physics.ins-det},\n  title         = {Smart Pixels: In-pixel AI for on-sensor data filtering},\n  url           = {https://arxiv.org/abs/2406.14860},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 2.00 No packaged code or setup scripts available; replication depends on hardware description and paper  Specification 5.00 None  Dataset 2.00 No dataset links; not publicly hosted or FAIR-compliant  Metrics 5.00 None  Reference Solution 3.00 In-pixel 2-layer NN described and evaluated, but reproducibility and source files are not released  Documentation 3.00 Paper contains detailed descriptions, but no repo or external guide for reproducing results  Average rating: 3.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/spiqa_llm/","title":"SPIQA (LLM)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: SPIQA  LLM</p> <p>Domain: Multimodal Scientific QA; Computer Vision</p> <p>Focus: Evaluating LLMs on image-based scientific paper figure QA tasks  LLM Adapter performance</p> <p>Task Types: Multimodal QA</p> <p>Metrics: Accuracy, F1 score</p> <p>Models: LLaVA, MiniGPT-4, Owl-LLM adapter variants</p> Keywords multimodal QA scientific figures image+text chain-of-thought prompting Citation <ul> <li>Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. Spiqa: a dataset for multimodal question answering on scientific papers. 2025. URL: https://arxiv.org/abs/2407.09413, arXiv:2407.09413.</li> </ul> <pre><code>@misc{pramanick2025spiqadatasetmultimodalquestion,\n  title={SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers}, \n  author={Shraman Pramanick and Rama Chellappa and Subhashini Venugopalan},\n  year={2025},\n  eprint={2407.09413},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2407.09413}, \n}</code></pre> Ratings CategoryRating Software 5.00 Well-documented codebase available on Github  Specification 3.50 Task of QA over scientific figures is sufficient but not fully formalized in input/output terms. No hawrdware constraints.  Dataset 5.00 Full dataset available on Hugging Face with train/test/valid splits.  Metrics 4.00 Reports accuracy and F1; fair but no visual reasoning-specific metric.  Reference Solution 4.00 10 LLM adapter baselines; results included without constraints.  Documentation 5.00 Full paper available  Average rating: 4.42/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/spiqa_scientific_paper_image_question_answering/","title":"SPIQA (Scientific Paper Image Question Answering)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-07-12</p> <p>Name: SPIQA  Scientific Paper Image Question Answering</p> <p>Domain: Computer Science</p> <p>Focus: Multimodal QA on scientific figures</p> <p>Task Types: Question answering, Multimodal QA, Chain-of-Thought evaluation</p> <p>Metrics: Accuracy, F1 score</p> <p>Models: Chain-of-Thought models, Multimodal QA systems</p> Keywords multimodal QA figure understanding table comprehension chain-of-thought Citation <ul> <li>Xiaoyan Zhong, Yijian Gao, and Suchin Gururangan. Spiqa: scientific paper image question answering. 2024. URL: https://arxiv.org/abs/2407.09413.</li> </ul> <pre><code>@misc{zhong2024spiqa,\n  title={SPIQA: Scientific Paper Image Question Answering},\n  author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},\n  year={2024},\n  url={https://arxiv.org/abs/2407.09413}\n}</code></pre> Ratings CategoryRating Software 0.00 Not provided  Specification 5.00 Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.  Dataset 4.50 Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.  Metrics 5.00 Uses quantitative metrics (Accuracy, F1) aligned with the task  Reference Solution 2.00 Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.  Documentation 5.00 All information provided in paper  Average rating: 3.58/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/supercond/","title":"SuperCon3D","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: SuperCon3D</p> <p>Domain: Materials Science; Superconductivity</p> <p>Focus: Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures</p> <p>Task Types: Regression (Tc prediction), Generative modeling</p> <p>Metrics: MAE (Tc), Validity of generated structures</p> <p>Models: SODNet, DiffCSP-SC</p> Keywords superconductivity crystal structures equivariant GNN generative models Citation <ul> <li>Pin Chen, Luoxuan Peng, Rui Jiao, Qing Mo, Zhen Wang, Wenbing Huang, Yang Liu, and Yutong Lu. Learning superconductivity from ordered and disordered material structures. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 108902\u2013108928. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_c4e3b55e,\n  author = {Chen, Pin and Peng, Luoxuan and Jiao, Rui and Mo, Qing and Wang, Zhen and Huang, Wenbing and Liu, Yang and Lu, Yutong},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {108902--108928},\n  publisher = {Curran Associates, Inc.},\n  title = {Learning Superconductivity from Ordered and Disordered Material Structures},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Baseline models (SODNet, DiffCSP-SC) are described in the paper; however, fully reproducible code and pretrained models are not publicly available yet.  Specification 5.00 Tasks for regression (Tc prediction) and generative modeling with clear input/output structures and domain constraints are well defined.  Dataset 5.00 Dataset contains 3D crystal structures and associated properties; well-curated but not fully released publicly at this time.  Metrics 4.00 Metrics such as MAE for Tc prediction and validity checks for generated structures are appropriate and clearly described.  Reference Solution 4.00 Paper provides model architecture details and some training insights, but no complete open-source reference implementations yet.  Documentation 4.00 Paper and GitHub provide good metadata and data processing descriptions; tutorials and user guides could be expanded.  Average rating: 4.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/the_well/","title":"The Well","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-03</p> <p>Name: The Well</p> <p>Domain: biological systems, fluid dynamics, acoustic scattering, astrophysical MHD</p> <p>Focus: Foundation model + surrogate dataset spanning 16 physical simulation domains</p> <p>Task Types: Supervised Learning</p> <p>Metrics: Dataset size, Domain breadth</p> <p>Models: FNO baselines, U-Net baselines</p> Keywords surrogate modeling foundation model physics simulations spatiotemporal dynamics Citation <ul> <li>Ruben Ohana, Michael McCabe, Lucas Meyer, Rudy Morel, Fruzsina J. Agocs, Miguel Beneitez, Marsha Berger, Blakesley Burkhart, Stuart B. Dalziel, Drummond B. Fielding, Daniel Fortunato, Jared A. Goldberg, Keiya Hirashima, Yan-Fei Jiang, Rich R. Kerswell, Suryanarayana Maddu, Jonah Miller, Payel Mukhopadhyay, Stefan S. Nixon, Jeff Shen, Romain Watteaux, Bruno R\u00e9galdo-Saint Blancard, Fran\u00e7ois Rozet, Liam H. Parker, Miles Cranmer, and Shirley Ho. The well: a large-scale collection of diverse physics simulations for machine learning. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 44989\u201345037. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_4f9a5acd,\n  author = {Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina J. and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesley and Dalziel, Stuart B. and Fielding, Drummond B. and Fortunato, Daniel and Goldberg, Jared A. and Hirashima, Keiya and Jiang, Yan-Fei and Kerswell, Rich R. and Maddu, Suryanarayana and Miller, Jonah and Mukhopadhyay, Payel and Nixon, Stefan S. and Shen, Jeff and Watteaux, Romain and Blancard, Bruno R\\'{e}galdo-Saint and Rozet, Fran\\c{c}ois and Parker, Liam H. and Cranmer, Miles and Ho, Shirley},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {44989--45037},\n  publisher = {Curran Associates, Inc.},\n  title = {The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 5.00 BSD-licensed software and unified API are available via GitHub and PyPI. Supports loading and manipulating large HDF5 datasets across 16 domains.  Specification 4.00 The benchmark includes clearly defined surrogate modeling tasks, data structure, and metadata. However, constraints and formal task specs vary slightly across domains.  Dataset 5.00 15 TB of ML-ready HDF5 datasets across 16 physics domains. Public, well-structured, richly annotated, and designed with FAIR principles in mind.  Metrics 3.00 Domain breadth and dataset size are emphasized. Standardized quantitative metrics for model evaluation (e.g., RMSE, accuracy) are not uniformly applied across all domains.  Reference Solution 3.00 Includes FNO and U-Net baselines, but does not yet provide fully trained, reproducible models or scripts across all datasets.  Documentation 4.00 The GitHub repo and NeurIPS paper provide detailed guidance on dataset use, structure, and training setup. Tutorials and walkthroughs could be expanded further.  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/ultrafast_jet_classification_at_the_hl-lhc/","title":"Ultrafast jet classification at the HL-LHC","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-07-08</p> <p>Name: Ultrafast jet classification at the HL-LHC</p> <p>Domain: Particle Physics</p> <p>Focus: FPGA-optimized real-time jet origin classification at the HL-LHC</p> <p>Task Types: Classification</p> <p>Metrics: Accuracy, Latency, Resource utilization</p> <p>Models: MLP, Deep Sets, Interaction Network</p> Keywords jet classification FPGA quantization-aware training Deep Sets Interaction Networks Citation <ul> <li>Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, and Thea K. Aarrestad. Ultrafast jet classification on fpgas for the hl-lhc. 2024. URL: https://arxiv.org/abs/2402.01876, arXiv:2402.01876, doi:https://doi.org/10.1088/2632-2153/ad5f10.</li> </ul> <pre><code>@misc{odagiu2024ultrafastjetclassificationfpgas,\n  archiveprefix = {arXiv},\n  author        = {Patrick Odagiu and Zhiqiang Que and Javier Duarte and Johannes Haller and Gregor Kasieczka and Artur Lobanov and Vladimir Loncar and Wayne Luk and Jennifer Ngadiuba and Maurizio Pierini and Philipp Rincke and Arpita Seksaria and Sioni Summers and Andre Sznajder and Alexander Tapper and Thea K. Aarrestad},\n  doi           = {https://doi.org/10.1088/2632-2153/ad5f10},\n  eprint        = {2402.01876},\n  primaryclass  = {hep-ex},\n  title         = {Ultrafast jet classification on FPGAs for the HL-LHC},\n  url           = {https://arxiv.org/abs/2402.01876},\n  year          = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Not containerized; Setup and automation incomplete  Specification 4.00 Hardware constraints are referenced but not fully detailed or standardized  Dataset 4.00 FAIR metadata limited; no clear mention of dataset format or splits  Metrics 3.00 Metrics exist (accuracy, latency, utilization), but formal definitions and evaluation guidance are limited  Reference Solution 2.00 Reference implementations not fully reproducible; no evaluation pipeline or training setup provided  Documentation 3.00 No linked GitHub repo or setup instructions; paper provides partial guidance only  Average rating: 3.17/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/urban_data_layer_udl/","title":"Urban Data Layer (UDL)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: Urban Data Layer  UDL</p> <p>Domain: Urban Computing; Data Engineering</p> <p>Focus: Unified data pipeline for multi-modal urban science research</p> <p>Task Types: Prediction, Classification</p> <p>Metrics: Task-specific accuracy or RMSE</p> <p>Models: Baseline regression/classification pipelines</p> Keywords data pipeline urban science multi-modal benchmark Citation <ul> <li>Yiheng Wang, Tianyu Wang, Yuying Zhang, Hongji Zhang, Haoyu Zheng, Guanjie Zheng, and Linghe Kong. Urbandatalayer: a unified data pipeline for urban science. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 7296\u20137310. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_0db7f135,\n  author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {7296--7310},\n  publisher = {Curran Associates, Inc.},\n  title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Source code is publicly available on GitHub; baseline regression and classification pipelines are included but framework maturity is moderate.  Specification 5.00 Multiple urban science tasks like prediction and classification are well specified with clear input/output and evaluation criteria.  Dataset 5.00 Large, multi-modal urban datasets are open-source, well-documented, and support reproducible research.  Metrics 5.00 Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.  Reference Solution 4.00 Baseline models available but not exhaustive; community adoption and extensions expected.  Documentation 5.00 GitHub repository and conference poster provide comprehensive code and reproducibility instructions.  Average rating: 4.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/vllm_inference_and_serving_engine/","title":"vLLM Inference and Serving Engine","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2023-09-12</p> <p>Name: vLLM Inference and Serving Engine</p> <p>Domain: LLM; HPC/inference</p> <p>Focus: High-throughput, memory-efficient inference and serving engine for LLMs</p> <p>Task Types: Inference Benchmarking</p> <p>Metrics: Tokens/sec, Time to First Token (TTFT), Memory footprint</p> <p>Models: LLaMA, Mixtral, FlashAttention-based models</p> Keywords LLM inference PagedAttention CUDA graph streaming API quantization Citation <ul> <li>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23, 611 626. New York, NY, USA, 2023. Association for Computing Machinery. URL: https://doi.org/10.1145/3600006.3613165, doi:10.1145/3600006.3613165.</li> </ul> <pre><code>@inproceedings{10.1145/3600006.3613165,\n  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},\n  title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  year = {2023},\n  publisher = {Association for Computing Machinery},\n  address = {New York, NY, USA},\n  url = {https://doi.org/10.1145/3600006.3613165},\n  doi = {10.1145/3600006.3613165},\n  abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},\n  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},\n  pages = {611-626},\n  numpages = {16},\n  location = {Koblenz, Germany},\n  series = {SOSP '23}\n}</code></pre> Ratings CategoryRating Software 5.00 Actively maintained open-source project under Apache 2.0. GitHub repo includes full serving engine, benchmarking scripts, CUDA integration, and deployment examples.  Specification 5.00 Inference benchmarks are well-defined with clear input/output formats and platform-specific constraints. Covers multiple models, hardware backends, and batching configurations.  Dataset 3.00 No traditional dataset is included. Instead, it uses structured configs and logs suitable for inference benchmarking. FAIR principles are only partially applicable.  Metrics 5.00 Comprehensive performance metrics like tokens/sec, time-to-first-token (TTFT), and memory footprint are consistently applied and benchmarked across frameworks.  Reference Solution 4.00 Provides runnable scripts and configs for several models (LLaMA, Mixtral, etc.) across platforms. Baselines are reproducible, though not all models are fully wrapped or hosted.  Documentation 4.00 Well-structured GitHub documentation with setup instructions, config examples, benchmarking comparisons, and performance tuning guides.  Average rating: 4.33/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/vllm_performance_dashboard/","title":"vLLM Performance Dashboard","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2022-06-22</p> <p>Name: vLLM Performance Dashboard</p> <p>Domain: LLM; HPC/inference</p> <p>Focus: Interactive dashboard showing inference performance of vLLM</p> <p>Task Types: Performance visualization</p> <p>Metrics: Tokens/sec, TTFT, Memory usage</p> <p>Models: LLaMA-2, Mistral, Qwen</p> Keywords Dashboard Throughput visualization Latency analysis Metric tracking Citation <ul> <li>Simon Mo. Vllm performance dashboard. 2024. URL: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/.</li> </ul> <pre><code>@misc{mo2024vllm_dashboard,\n  title={vLLM Performance Dashboard},\n  author={Mo, Simon},\n  year={2024},\n  url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}\n}</code></pre> Ratings CategoryRating Software 4.00 Interactive dashboard built with ObservableHQ and linked to vLLM benchmarks. Source code is not fully open, but backend integration with vLLM is well-maintained.  Specification 4.00 While primarily a visualization tool, it includes benchmark configurations, metric definitions, and supports comparison across models and hardware.  Dataset 2.00 No datasets are bundled; the dashboard visualizes metrics derived from model inference logs or external endpoints, not a formal dataset.  Metrics 4.00 Tracks tokens/sec, TTFT, memory usage, and platform comparisons. Metrics are clear but focused on visualization rather than statistical robustness.  Reference Solution 3.00 Dashboards include reproducible views of benchmarked models, but do not ship with runnable model code. Relies on external serving infrastructure.  Documentation 4.00 Public dashboard with instructions and tooltips; documentation is clear, though access is restricted (login required) and backend setup is opaque to users.  Average rating: 3.50/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/vocal_call_locator_vcl/","title":"Vocal Call Locator (VCL)","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2024-12-13</p> <p>Name: Vocal Call Locator  VCL</p> <p>Domain: Neuroscience; Bioacoustics</p> <p>Focus: Benchmarking sound-source localization of rodent vocalizations from multi-channel audio</p> <p>Task Types: Sound source localization</p> <p>Metrics: Localization error (cm), Recall/Precision</p> <p>Models: CNN-based SSL models</p> Keywords source localization bioacoustics time-series SSL Citation <ul> <li>Ralph E Peterson, Aramis Tanelus, Christopher Ick, Bartul Mimica, Niegil Francis, Violet J Ivan, Aman Choudhri, Annegret L Falkner, Mala Murthy, David M Schneider, Dan H Sanes, and Alex H Williams. Vocal call locator benchmark (vcl) for localizing rodent vocalizations from multi-channel audio. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, 106370\u2013106382. Curran Associates, Inc., 2024. URL: https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf.</li> </ul> <pre><code>@inproceedings{neurips2024_c00d37d6,\n  author = {Peterson, Ralph E and Tanelus, Aramis and Ick, Christopher and Mimica, Bartul and Francis, Niegil and Ivan, Violet J and Choudhri, Aman and Falkner, Annegret L and Murthy, Mala and Schneider, David M and Sanes, Dan H and Williams, Alex H},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},\n  pages = {106370--106382},\n  publisher = {Curran Associates, Inc.},\n  title = {Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf},\n  volume = {37},\n  year = {2024}\n}</code></pre> Ratings CategoryRating Software 3.00 Some baseline CNN models for sound source localization are reported, but no publicly available or fully integrated runnable codebase yet.  Specification 5.00 Well-defined localization tasks with multiple scenarios and real-world environment conditions; input/output formats clearly described.  Dataset 4.00 Large-scale audio dataset covering real and simulated data with standardized splits, though exact data formats are not fully detailed.  Metrics 5.00 Includes localization error, precision, recall, and other relevant metrics for robust evaluation.  Reference Solution 5.00 Multiple baselines evaluated over diverse models and architectures, supporting reproducibility of benchmark comparisons.  Documentation 1.00 Methodology and paper are thorough, but setup instructions and runnable code are not publicly provided, limiting user onboarding.  Average rating: 3.83/5 Radar plot <p>Edit: edit this entry</p>"},{"location":"md/benchmarks/winogrande/","title":"Winogrande","text":"<p>\u2190 Back to all benchmarks</p> <p>Date: 2019-07-24</p> <p>Name: Winogrande</p> <p>Domain: NLP; Commonsense</p> <p>Focus: Winograd Schema-style pronoun resolution</p> <p>Task Types: Pronoun resolution</p> <p>Metrics: Accuracy, AUC</p> <p>Models: RoBERTa, BERT, GPT-2</p> Keywords adversarial pronoun resolution Citation <ul> <li>Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an adversarial winograd schema challenge at scale. 2019. URL: https://arxiv.org/abs/1907.10641, arXiv:1907.10641.</li> </ul> <pre><code>@misc{sakaguchi2019winograndeadversarialwinogradschema,\n  archiveprefix = {arXiv},\n  author        = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},\n  eprint        = {1907.10641},\n  primaryclass  = {cs.CL},\n  title         = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},\n  url           = {https://arxiv.org/abs/1907.10641},\n  year          = {2019}\n}</code></pre> Ratings CategoryRating Software 0.00 No template code provided  Specification 5.00 Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included.  Dataset 5.00 Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata.  Metrics 5.00 Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations.  Reference Solution 4.00 Baseline results available, requiring users to submit their methods along with their submissions. Constraints are not required in submissions.  Documentation 5.00 Dataset page and paper provide sufficient detail  Average rating: 4.00/5 Radar plot <p>Edit: edit this entry</p>"}]}