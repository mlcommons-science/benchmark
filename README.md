# MLCommons Science Working Group AI Benchmark Collection

This repository holds a list of scientific AI benchmarks maintained by the MLCommons Science Working Group. Benchmark definitions live in YAML, and we provide scripts to validate, summarize, and publish the collection. Contributions from the community are welcome.

## Quick Links

- Benchmark website: <https://mlcommons-science.github.io/benchmark/>
- Ontology sources (current): `source/benchmarks.yaml`, `source/benchmarks-addon.yaml`
- YAML schema and example: `source/benchmarks-format.yaml`, `source/benchmarks-sample.yaml`
- Benchmark format guide: `docs/benchmark-format.md`
- Tooling reference: `docs/tooling.md`
- Rating system: `docs/ratings.md`
- Contribution guide: `CONTRIBUTING.md`

## What Lives in This Repository

- Versioned benchmark metadata expressed in YAML, including citations, FAIR scores, and ratings.
- A Python toolchain (`bin/generate.py` and supporting modules) that validates entries and produces Markdown, LaTeX, and MkDocs outputs.
- Automation for building the public benchmark site in `www/science-ai-benchmarks/`.

All generated artifacts are derived from the YAML sources. Do not edit rendered files manually.

## Quick Start

### Local Python Environment

```bash
git clone https://github.com/mlcommons-science/benchmark.git
cd benchmark
python3 -m venv .venv
source .venv/bin/activate
make install           # installs dependencies from requirements.txt
```

### Docker Workflow

A Docker image is provided with Python, LaTeX, and the repository tooling preconfigured:

```bash
docker build -t benchmark .
docker run --rm -it -v "$PWD":/workspace -e SERVE_HOST=0.0.0.0 -p 8000:8000 benchmark
```

You can execute the Makefile targets and scripts inside the container.

## Add or Update a Benchmark

1. Create a feature branch in your fork (see `CONTRIBUTING.md` for the full workflow).
2. Use `source/benchmarks-sample.yaml` together with the schema in `source/benchmarks-format.yaml` as a reference. Additional YAML source files can be introduced following the same structure.
3. Apply edits with two-space indentation and use the literal string `"unknown"` when information is unavailable.
4. Run the catalogue checks before committing:
   ```bash
   make check          # ensures required fields are present and flags non-ASCII characters
   make check_url      # optional; validates that referenced URLs resolve
   ```
   If a site blocks automated requests (so the URL works in your browser but still fails in the script), follow the “URL verification workflow” in `docs/tooling.md`.
5. Review your diff so the pull request contains only YAML and documentation updates. Avoid committing changes under `content/` or `www/`.
6. Open a pull request. Maintainers will regenerate Markdown, LaTeX, and MkDocs outputs during review before publishing.

Consult `docs/tooling.md` for the full list of Make targets and script options.

## Repository Layout

- `bin/` – validation and publishing scripts.
- `content/` – generated Markdown, TeX, and MkDocs assets that feed the published site.
- `source/` – benchmark catalogue, schema, and MkDocs homepage (`index.md`).
- `tests/` – unit tests for key scripts.
- `www/` – MkDocs output generated by `make mkdocs`; contains the static site published via GitHub Pages and is ignored by Git.

## How to Cite

If you use this repository, the benchmark ontology, or any derived artifacts, please cite all relevant works associated with this project.

### MLCommons Science Working Group AI Benchmarks Collection
Gregor von Laszewski, Ben Hawks, Marco Colombo, Reece Shiraishi, Anjay Krishnan, Nhan Tran, and Geoffrey C. Fox. 2025. *MLCommons Science Working Group AI Benchmarks Collection.* MLCommons Science Working Group. Available at: https://mlcommons-science.github.io/benchmark/benchmarks.pdf

**BibTeX:**
```bibtex
@misc{mlcommons-benchmarks-collection,
  author = {
    Gregor von Laszewski and
    Ben Hawks and
    Marco Colombo and
    Reece Shiraishi and
    Anjay Krishnan and
    Nhan Tran and
    Geoffrey C. Fox
  },
  title = {MLCommons Science Working Group AI Benchmarks Collection},
  url = {https://mlcommons-science.github.io/benchmark/benchmarks.pdf},
  note = {Online Collection: \url{https://mlcommons-science.github.io/benchmark/}},
  month = jun,
  year = 2025,
  howpublished = {GitHub}
}
```

### An MLCommons Scientific Benchmarks Ontology
Ben Hawks, Gregor von Laszewski, Matthew D. Sinclair, Marco Colombo, Shivaram Venkataraman, Rutwik Jain, Yiwei Jiang, Nhan Tran, and Geoffrey Fox. 2025. *An MLCommons Scientific Benchmarks Ontology.* arXiv:2511.05614.

**BibTeX:**
```bibtex
@misc{hawks2025mlcommons_ontology,
  title={An MLCommons Scientific Benchmarks Ontology},
  author={Ben Hawks and Gregor von Laszewski and Matthew D. Sinclair and Marco Colombo and Shivaram Venkataraman and Rutwik Jain and Yiwei Jiang and Nhan Tran and Geoffrey Fox},
  year={2025},
  eprint={2511.05614},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2511.05614}
}
```

### AI Benchmarks Carpentry and Democratization
Authors. 2025. *AI Benchmarks Carpentry and Democratization.* To appear.

**BibTeX:**
```bibtex
@article{mlcommons2025carpentry,
  author = {Authors},
  title = {AI Benchmarks Carpentry and Democratization},
  journal = {To appear},
  year = {2025},
  note = {Forthcoming publication}
}
```

## License

This project is released under the [Apache 2.0 License](LICENSE).

For questions or suggestions, please open an issue or start a discussion on the MLCommons Science Working Group forums.
