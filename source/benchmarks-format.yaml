- date: '2024-05-01'
    # description: | 
    #   The date of availability of the benchmark. If an official release date is 
    #   not available, use the date of adding the entry. In YYYY-MM-DD format.
    # condition: required

  version: 'v1.0'
    # description: The version number of the benchmark
    # condition: optional

  last_updated: '2024-05-01'  # Could also be null
    # description: |
    #   The date when the entry was last updated. The format is YYYY-mm-dd
    # condition: optional

  expired: 'no'  # 'yes' or 'no'
    # description: |
    #   An indication if the benchmark is no longer valid. Either 'yes' or 'no'
    # condition: optional

  valid: true
    # description: |
    #   Identifies if the benchmark is valid at the time of review. Invalidity may be caused by the code or data 
    #   no longer being available.
    # condition: required
  
  valid_date: '2025-01-01'
    # description: |
    #   Date since the benchmark is valid
    # condition: required

  name: 'Jet Classification'
    # description: |
    #   The name of the benchmark. It is best to use a short nameas it is easier to remember 
    #   and used to create a URL.
    # condition: required

  url: 'https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify'
    # description: |
    #   The main URL for this benchmark. We also recommend adding it to the citation.
    # condition: required

  doi: xyz...
    # description: |
    #   A DOI number that may be associated with the benchmark. This could be one obtained by Zendoo.
    #   We also recommend adding this to the citation.
    # condition: optional

  domain:
      - Particle Physics
    # description: |
    #   The scientific domain this benchmark belongs to,  i.e. physics, biology, math, ...
    # condition: ">=1"

  focus: 'Real-time classification of particle jets using HL-LHC simulation features'
    # description: |
    #   A short sentence on the main topics of the benchmark. 
    # condition: required

  keywords:
      - classification
      - real-time ML
      - jet tagging
      - QKeras
    # description: |
    #   List of keywords relevant for the benchmark
    # condition: ">=1"

  summary: |
      This benchmark evaluates ML models for real-time classification of particle jets using high-level features 
      derived from simulated LHC data. It includes both full-precision and quantized models optimized for FPGA deployment.
    # description: |
    #   An easy-to-understand description of the benchmark so that members of the 
    #   community know what the benchmark is about.
    # condition: required

  licensing: Apache 2.0
    # description: |
    #   The license for the benchmark
    # condition: required

  task_types:
      - Classification
    # description: |
    #   The task type defined by this benchmark. Gregor finds that this requires a bit more 
    #   explanation, as it is unclear for most.
    # condition: required

  ai_capability_measured:
      - Real-time inference
      - Model compression performance
    # description: |
    #   The AI capabilities that are measured. Gregor finds this requires the definition of what an AI capability is.
    # condition: ">=1"

  metrics:
      - Accuracy
      - AUC
    # description: |
    #   A list of metrics used by the benchmark
    # condition: ">=1"

  models:
      - Keras DNN
      - QKeras quantized DNN
    # description: |
    #   A list of models that are used by the benchmark.
    # condition: ">=1"

  ml_motif:
    - Real-time
    # description: |
    #   The ML motif tested. TODO: we need a list of motifs to be in a formal document.
    # condition: ">=1"

  type: Benchmark
    # description: |
    #   The type of this benchmark. This could indicate that the effort represents a designed benchmark, or
    #   an application comparison that contains implicit benchmarks. We will be analyzing candidates to identify 
    #   types and make them available here.
    # condition: required

  ml_task:
    - Supervised Learning
    # description: |
    #   A list of machine learning tasks used in this benchmark
    # condition: ">=1"

  solutions: '2'
    # description: |
    #   Obsolete, as results should capture this. Results is a list, thus we can get the number of list elements.
    #   This is easier to manage.
    # condition: deprecated

  notes: Includes both float and quantized models using QKeras
    # description: |
    #   An added note describing the benchmark in more detail or adding some related information.
    # condition: optional

  contact:
      name: Jules Muhizi
      email: unknown
    # description: |
    #   A contact person or organization for this benchmark
    # condition: optional

  cite: 
      - |
        @article{hawks2022fastml,
          title={Fast Machine Learning for Science: Benchmarks and Dataset},
          author={Hawks, Ben and Tran, Nhan and others},
          year={2022},
          url={https://arxiv.org/abs/2207.07958}
        }
    # description: |
    #   A list of bibtex citations. It is important that the citation is in bibtex. This may include 
    #   scholarly articles, web pages, GitHub pages. At least one must be provided.
    # condition: ">=1"

  datasets:
      links:
      - name: OpenML
        url: 'https://www.openml.org/d/42468'
      - name: JetClass
        url: 'https://zenodo.org/record/6619768'
      # description: |
      #   A list of datasets used with the benchmark. 
      # condition: ">=1"

  results:
      links: 
        - name: Gemini LLM Deep Research
          url: https://docs.google.com/document/d/1Mr7J4F8PDAIBXJ2vrfVssxLekEVW7ahJ4wpSe6FN5yw
        - name: ChatGPT LLM
          url: https://docs.google.com/document/d/1runrcij-eoH3_lgGZ8wm2z1YbL1Qf5cSNbVbHyWFDs4
      # description: |
      #   A list of results, ideally published on the web. It could also point to a paper.
      # condition: required

  fair:
      reproducible: true
      benchmark_ready: true  # This field is deprecated as true implies runnable
      # description: |
      #   Attributes important with the fair principle. It is true if fair applies.
      # condition: required

  ratings:

    # description: |
    #   A set of ratings associated with the benchmark. The goal is to rate all benchmarks, but this could
    #   be time-consuming and prevent listing here. To allow addition in the list without 
    #   ratings, we make this section optional. A value of 0 in any of the categories means 
    #   it has not been evaluated yet. Values are allowed from 0 to 5. Note the ratings here are fictional.
    # condition: optional

    software:
      rating: -1
      reason: Not yet evaluated
      # description: |
      #   Evaluates maturity and software engineering aspects used to define this benchmark.
      # condition: optional

    specification:
      rating: 5.0
      reason: |
        Task and format (multiple-choice QA with 5 options) are clearly
        defined; grounded in ConceptNet with consistent structure, though no hardware/system
        constraints are specified.
      # description: |
      #   Evaluates the maturity of the specification and explanations for the existence of this benchmark.
      # condition: optional

    dataset:
      rating: 5.0
      reason: |
        Public, versioned, and FAIR-compliant; includes metadata, splits,
        and licensing; well-integrated with HuggingFace and other ML libraries.
      # description: |
      #   Evaluates the dataset, including attributes such as:
      #     - public availability
      #     - versioning
      #     - metadata
      #     - licensing
      # condition: optional

    metrics:
      rating: 5.0
      reason: | 
        Accuracy is a simple, reproducible metric aligned
        with task goals; no ambiguity in evaluation.
      # description: |
      #   The existence and maturity of an accuracy value relevant for the domain community.
      # condition: optional

    reference_solution:
      rating: 5.0
      reason: |
        Several baseline models (e.g., BERT, RoBerta) are reported
        with scores; implementations exist in public repos, but not bundled as an official
        starter kit.
      # description: |
      #   The availability of a reference solution.
      # condition: optional

    documentation:
      rating: 5.0
      reason: |
        Clear paper, GitHub repo, and integration with HuggingFace
        Datasets; full reproducibility requires manually connecting models to dataset.
      # description: |
      #   The maturity of the documentation, including how to replicate the benchmark.
      # condition: optional
