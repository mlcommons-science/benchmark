- date: '2020-09-07'
  expired:
  valid: 'yes'
  name: MMLU (Massive Multitask Language Understanding)
  url: https://paperswithcode.com/dataset/mmlu
  domain: Multidomain
  focus: Academic knowledge and reasoning across 57 subjects
  keywords:
  - multitask
  - multiple-choice
  - zero-shot
  - few-shot
  - knowledge probing
  description: |-
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  task_types:
  - Multiple choice
  ai_capability_measured: General reasoning, subject-matter understanding
  metrics:
  - Accuracy
  models:
  - GPT-4o
  - Gemini 1.5 Pro
  - o1
  - DeepSeek-R1
  notes: Good
  cite:
  - |-
    @article{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav},
      journal={arXiv preprint arXiv:2009.03300},
      year={2021},
      url={https://arxiv.org/abs/2009.03300}
    }
  ratings:
    specification:
      rating: 6
      reason: TO DO
    dataset:
      rating: 6.5
      reason: TO DO
    metrics:
      rating: 10
      reason: TO DO
    reference_solution:
      rating: 0
      reason: TO DO
    documentation:
      rating: 6
      reason: TO DO
- date: '2023-11-20'
  expired:
  valid: 'yes'
  name: GPQA Diamond
  url: https://arxiv.org/abs/2311.12022
  domain: Science
  focus: Graduate-level scientific reasoning
  keywords:
  - Google-proof
  - graduate-level
  - science QA
  - chemistry
  - physics
  description: |-
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is Google-proof - experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  task_types:
  - Multiple choice
  - Multi-step QA
  ai_capability_measured: Scientific reasoning, deep knowledge
  metrics:
  - Accuracy
  models:
  - o1
  - DeepSeek-R1
  notes: Good
  cite:
  - |-
    @misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
      author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper},
      year={2023},
      url={https://arxiv.org/abs/2311.12022}
    }
  ratings:
    specification:
      rating: 6.5
      reason: |-
        Good description of how the problems are received, but little specification on how the models are tested
    dataset:
      rating: 8.5
      reason: Easily able to access dataset. No labels or train/test/valid split
    metrics:
      rating: 10
      reason: Each question has a correct answer
    reference_solution:
      rating: 7.5
      reason: |-
        Common models such as GPT-3.5 were compared. Reproducibility of results unknown
    documentation:
      rating: 1
      reason: |-
        No reference solution, platform for reproduction, or procedure for replication
- date: '2018-03-14'
  expired:
  valid: 'yes'
  name: ARC-Challenge (Advanced Reasoning Challenge)
  url: https://allenai.org/data/arc
  domain: Science
  focus: Grade-school science with reasoning emphasis
  keywords:
  - grade-school
  - science QA
  - challenge set
  - reasoning
  description: |-
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  task_types:
  - Multiple choice
  ai_capability_measured: Commonsense and scientific reasoning
  metrics:
  - Accuracy
  models:
  - GPT-4
  - Claude
  notes: Good
  cite:
  - |-
    @inproceedings{clark2018think,
      title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
      author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren},
      booktitle={EMNLP 2018},
      pages={237-248},
      year={2018},
      url={https://allenai.org/data/arc}
    }
  ratings:
    specification:
      rating: 9
      reason: |-
        Exact format of data, questions, and answers are specified. No HW constraints
    dataset:
      rating: 10
      reason: |-
        Data accessible, offers instructions on how to download the data via CLI tools
    metrics:
      rating: 10
      reason: |-
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 4.5
      reason: |-
        There are over 300 models listed, but very few, if any, show performance on the dataset
    documentation:
      rating: 4
      reason: |-
        There are easy ways to download the dataset. Documentation quantity and clarity depends on authors of tested models
- date: '2025-01-24'
  expired:
  valid: 'yes'
  name: Humanity's Last Exam
  url: https://arxiv.org/abs/2501.14249
  domain: Multidomain
  focus: Broad cross-domain academic reasoning
  keywords:
  - cross-domain
  - academic exam
  - multiple-choice
  - multidisciplinary
  description: |-
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  task_types:
  - Multiple choice
  ai_capability_measured: Cross-domain academic reasoning
  metrics:
  - Accuracy
  models: []
  notes: Good
  cite:
  - |-
    @misc{phan2025humanitysexam,
      archiveprefix = {arXiv},
      author        = {Long Phan and Alice Gatti and Ziwen Han and Nathaniel Li and Josephina Hu and Hugh Zhang and Chen Bo Calvin Zhang and Mohamed Shaaban and John Ling and Sean Shi and Michael Choi and Anish Agrawal and Arnav Chopra and Adam Khoja and Ryan Kim and Richard Ren and Jason Hausenloy and Oliver Zhang and Mantas Mazeika and Dmitry Dodonov and Tung Nguyen and Jaeho Lee and Daron Anderson and Mikhail Doroshenko and Alun Cennyth Stokes and Mobeen Mahmood and Oleksandr Pokutnyi and Oleg Iskra and Jessica P. Wang and John-Clark Levin and Mstyslav Kazakov and Fiona Feng and Steven Y. Feng and Haoran Zhao and Michael Yu and Varun Gangal and Chelsea Zou and Zihan Wang and Serguei Popov and Robert Gerbicz and Geoff Galgon and Johannes Schmitt and Will Yeadon and Yongki Lee and Scott Sauers and Alvaro Sanchez and Fabian Giska and Marc Roth and Søren Riis and Saiteja Utpala and Noah Burns and Gashaw M. Goshu and Mohinder Maheshbhai Naiya and Chidozie Agu and Zachary Giboney and Antrell Cheatom and Francesco Fournier-Facio and Sarah-Jane Crowson and Lennart Finke and Zerui Cheng and Jennifer Zampese and Ryan G. Hoerr and Mark Nandor and Hyunwoo Park and Tim Gehrunger and Jiaqi Cai and Ben McCarty and Alexis C Garretson and Edwin Taylor and Damien Sileo and Qiuyu Ren and Usman Qazi and Lianghui Li and Jungbae Nam and John B. Wydallis and Pavel Arkhipov and Jack Wei Lun Shi and Aras Bacho and Chris G. Willcocks and Hangrui Cao and Sumeet Motwani and Emily de Oliveira Santos and Johannes Veith and Edward Vendrow and Doru Cojoc and Kengo Zenitani and Joshua Robinson and Longke Tang and Yuqi Li and Joshua Vendrow and Natanael Wildner Fraga and Vladyslav Kuchkin and Andrey Pupasov Maksimov and Pierre Marion and Denis Efremov and Jayson Lynch and Kaiqu Liang and Aleksandar Mikov and Andrew Gritsevskiy and Julien Guillod and Gözdenur Demir and Dakotah Martinez and Ben Pageler and Kevin Zhou and Saeed Soori and Ori Press and Henry Tang and Paolo Rissone and Sean R. Green and Lina Brüssel and Moon Twayana and Aymeric Dieuleveut and Joseph Marvin Imperial and Ameya Prabhu and Jinzhou Yang and Nick Crispino and Arun Rao and Dimitri Zvonkine and Gabriel Loiseau and Mikhail Kalinin and Marco Lukas and Ciprian Manolescu and Nate Stambaugh and Subrata Mishra and Tad Hogg and Carlo Bosio and Brian P Coppola and Julian Salazar and Jaehyeok Jin and Rafael Sayous and Stefan Ivanov and Philippe Schwaller and Shaipranesh Senthilkuma and Andres M Bran and Andres Algaba and Kelsey Van den Houte and Lynn Van Der Sypt and Brecht Verbeken and David Noever and Alexei Kopylov and Benjamin Myklebust and Bikun Li and Lisa Schut and Evgenii Zheltonozhskii and Qiaochu Yuan and Derek Lim and Richard Stanley and Tong Yang and John Maar and Julian Wykowski and Martí Oller and Anmol Sahu and Cesare Giulio Ardito and Yuzheng Hu and Ariel Ghislain Kemogne Kamdoum and Alvin Jin and Tobias Garcia Vilchis and Yuexuan Zu and Martin Lackner and James Koppel and Gongbo Sun and Daniil S. Antonenko and Steffi Chern and Bingchen Zhao and Pierrot Arsene and Joseph M Cavanagh and Daofeng Li and Jiawei Shen and Donato Crisostomi and Wenjin Zhang and Ali Dehghan and Sergey Ivanov and David Perrella and Nurdin Kaparov and Allen Zang and Ilia Sucholutsky and Arina Kharlamova and Daniil Orel and Vladislav Poritski and Shalev Ben-David and Zachary Berger and Parker Whitfill and Michael Foster and Daniel Munro and Linh Ho and Shankar Sivarajan and Dan Bar Hava and Aleksey Kuchkin and David Holmes and Alexandra Rodriguez-Romero and Frank Sommerhage and Anji Zhang and Richard Moat and Keith Schneider and Zakayo Kazibwe and Don Clarke and Dae Hyun Kim and Felipe Meneguitti Dias and Sara Fish and Veit Elser and Tobias Kreiman and Victor Efren Guadarrama Vilchis and Immo Klose and Ujjwala Anantheswaran and Adam Zweiger and Kaivalya Rawal and Jeffery Li and Jeremy Nguyen and Nicolas Daans and Haline Heidinger and Maksim Radionov and Václav Rozhoň and Vincent Ginis and Christian Stump and Niv Cohen and Rafał Poświata and Josef Tkadlec and Alan Goldfarb and Chenguang Wang and Piotr Padlewski and Stanislaw Barzowski and Kyle Montgomery and Ryan Stendall and Jamie Tucker-Foltz and Jack Stade and T. Ryan Rogers and Tom Goertzen and Declan Grabb and Abhishek Shukla and Alan Givré and John Arnold Ambay and Archan Sen and Muhammad Fayez Aziz and Mark H Inlow and Hao He and Ling Zhang and Younesse Kaddar and Ivar Ängquist and Yanxu Chen and Harrison K Wang and Kalyan Ramakrishnan and Elliott Thornley and Antonio Terpin and Hailey Schoelkopf and Eric Zheng and Avishy Carmi and Ethan D. L. Brown and Kelin Zhu and Max Bartolo and Richard Wheeler and Martin Stehberger and Peter Bradshaw and JP Heimonen and Kaustubh Sridhar and Ido Akov and Jennifer Sandlin and Yury Makarychev and Joanna Tam and Hieu Hoang and David M. Cunningham and Vladimir Goryachev and Demosthenes Patramanis and Michael Krause and Andrew Redenti and David Aldous and Jesyin Lai and Shannon Coleman and Jiangnan Xu and Sangwon Lee and Ilias Magoulas and Sandy Zhao and Ning Tang and Michael K. Cohen and Orr Paradise and Jan Hendrik Kirchner and Maksym Ovchynnikov and Jason O. Matos and Adithya Shenoy and Michael Wang and Yuzhou Nie and Anna Sztyber-Betley and Paolo Faraboschi and Robin Riblet and Jonathan Crozier and Shiv Halasyamani and Shreyas Verma and Prashant Joshi and Eli Meril and Ziqiao Ma and Jérémy Andréoletti and Raghav Singhal and Jacob Platnick and Volodymyr Nevirkovets and Luke Basler and Alexander Ivanov and Seri Khoury and Nils Gustafsson and Marco Piccardo and Hamid Mostaghimi and Qijia Chen and Virendra Singh and Tran Quoc Khánh and Paul Rosu and Hannah Szlyk and Zachary Brown and Himanshu Narayan and Aline Menezes and Jonathan Roberts and William Alley and Kunyang Sun and Arkil Patel and Max Lamparth and Anka Reuel and Linwei Xin and Hanmeng Xu and Jacob Loader and Freddie Martin and Zixuan Wang and Andrea Achilleos and Thomas Preu and Tomek Korbak and Ida Bosio and Fereshteh Kazemi and Ziye Chen and Biró Bálint and Eve J. Y. Lo and Jiaqi Wang and Maria Inês S. Nunes and Jeremiah Milbauer and M Saiful Bari and Zihao Wang and Behzad Ansarinejad and Yewen Sun and Stephane Durand and Hossam Elgnainy and Guillaume Douville and Daniel Tordera and George Balabanian and Hew Wolff and Lynna Kvistad and Hsiaoyun Milliron and Ahmad Sakor and Murat Eron and Andrew Favre D. O. and Shailesh Shah and Xiaoxiang Zhou and Firuz Kamalov and Sherwin Abdoli and Tim Santens and Shaul Barkan and Allison Tee and Robin Zhang and Alessandro Tomasiello and G. Bruno De Luca and Shi-Zhuo Looi and Vinh-Kha Le and Noam Kolt and Jiayi Pan and Emma Rodman and Jacob Drori and Carl J Fossum and Niklas Muennighoff and Milind Jagota and Ronak Pradeep and Honglu Fan and Jonathan Eicher and Michael Chen and Kushal Thaman and William Merrill and Moritz Firsching and Carter Harris and Stefan Ciobâcă and Jason Gross and Rohan Pandey and Ilya Gusev and Adam Jones and Shashank Agnihotri and Pavel Zhelnov and Mohammadreza Mofayezi and Alexander Piperski and David K. Zhang and Kostiantyn Dobarskyi and Roman Leventov and Ignat Soroko and Joshua Duersch and Vage Taamazyan and Andrew Ho and Wenjie Ma and William Held and Ruicheng Xian and Armel Randy Zebaze and Mohanad Mohamed and Julian Noah Leser and Michelle X Yuan and Laila Yacar and Johannes Lengler and Katarzyna Olszewska and Claudio Di Fratta and Edson Oliveira and Joseph W. Jackson and Andy Zou and Muthu Chidambaram and Timothy Manik and Hector Haffenden and Dashiell Stander and Ali Dasouqi and Alexander Shen and Bita Golshani and David Stap and Egor Kretov and Mikalai Uzhou and Alina Borisovna Zhidkovskaya and Nick Winter and Miguel Orbegozo Rodriguez and Robert Lauff and Dustin Wehr and Colin Tang and Zaki Hossain and Shaun Phillips and Fortuna Samuele and Fredrik Ekström and Angela Hammon and Oam Patel and Faraz Farhidi and George Medley and Forough Mohammadzadeh and Madellene Peñaflor and Haile Kassahun and Alena Friedrich and Rayner Hernandez Perez and Daniel Pyda and Taom Sakal and Omkar Dhamane and Ali Khajegili Mirabadi and Eric Hallman and Kenchi Okutsu and Mike Battaglia and Mohammad Maghsoudimehrabani and Alon Amit and Dave Hulbert and Roberto Pereira and Simon Weber and Handoko and Anton Peristyy and Stephen Malina and Mustafa Mehkary and Rami Aly and Frank Reidegeld and Anna-Katharina Dick and Cary Friday and Mukhwinder Singh and Hassan Shapourian and Wanyoung Kim and Mariana Costa and Hubeyb Gurdogan and Harsh Kumar and Chiara Ceconello and Chao Zhuang and Haon Park and Micah Carroll and Andrew R. Tawfeek and Stefan Steinerberger and Daattavya Aggarwal and Michael Kirchhof and Linjie Dai and Evan Kim and Johan Ferret and Jainam Shah and Yuzhou Wang and Minghao Yan and Krzysztof Burdzy and Lixin Zhang and Antonio Franca and Diana T. Pham and Kang Yong Loh and Joshua Robinson and Abram Jackson and Paolo Giordano and Philipp Petersen and Adrian Cosma and Jesus Colino and Colin White and Jacob Votava and Vladimir Vinnikov and Ethan Delaney and Petr Spelda and Vit Stritecky and Syed M. Shahid and Jean-Christophe Mourrat and Lavr Vetoshkin and Koen Sponselee and Renas Bacho and Zheng-Xin Yong and Florencia de la Rosa and Nathan Cho and Xiuyu Li and Guillaume Malod and Orion Weller and Guglielmo Albani and Leon Lang and Julien Laurendeau and Dmitry Kazakov and Fatimah Adesanya and Julien Portier and Lawrence Hollom and Victor Souza and Yuchen Anna Zhou and Julien Degorre and Yiğit Yalın and Gbenga Daniel Obikoya and Rai and Filippo Bigi and M. C. Boscá and Oleg Shumar and Kaniuar Bacho and Gabriel Recchia and Mara Popescu and Nikita Shulga and Ngefor Mildred Tanwie and Thomas C. H. Lux and Ben Rank and Colin Ni and Matthew Brooks and Alesia Yakimchyk and Huanxu and Liu and Stefano Cavalleri and Olle Häggström and Emil Verkama and Joshua Newbould and Hans Gundlach and Leonor Brito-Santana and Brian Amaro and Vivek Vajipey and Rynaa Grover and Ting Wang and Yosi Kratish and Wen-Ding Li and Sivakanth Gopi and Andrea Caciolai and Christian Schroeder de Witt and Pablo Hernández-Cámara and Emanuele Rodolà and Jules Robins and Dominic Williamson and Vincent Cheng and Brad Raynor and Hao Qi and Ben Segev and Jingxuan Fan and Sarah Martinson and Erik Y. Wang and Kaylie Hausknecht and Michael P. Brenner and Mao Mao and Christoph Demian and Peyman Kassani and Xinyu Zhang and David Avagian and Eshawn Jessica Scipio and Alon Ragoler and Justin Tan and Blake Sims and Rebeka Plecnik and Aaron Kirtland and Omer Faruk Bodur and D. P. Shinde and Yan Carlos Leyva Labrador and Zahra Adoul and Mohamed Zekry and Ali Karakoc and Tania C. B. Santos and Samir Shamseldeen and Loukmane Karim and Anna Liakhovitskaia and Nate Resman and Nicholas Farina and Juan Carlos Gonzalez and Gabe Maayan and Earth Anderson and Rodrigo De Oliveira Pena and Elizabeth Kelley and Hodjat Mariji and Rasoul Pouriamanesh and Wentao Wu and Ross Finocchio and Ismail Alarab and Joshua Cole and Danyelle Ferreira and Bryan Johnson and Mohammad Safdari and Liangti Dai and Siriphan Arthornthurasuk and Isaac C. McAlister and Alejandro José Moyano and Alexey Pronin and Jing Fan and Angel Ramirez-Trinidad and Yana Malysheva and Daphiny Pottmaier and Omid Taheri and Stanley Stepanic and Samuel Perry and Luke Askew and Raúl Adrián Huerta Rodríguez and Ali M. R. Minissi and Ricardo Lorena and Krishnamurthy Iyer and Arshad Anil Fasiludeen and Ronald Clark and Josh Ducey and Matheus Piza and Maja Somrak and Eric Vergo and Juehang Qin and Benjámin Borbás and Eric Chu and Jack Lindsey and Antoine Jallon and I. M. J. McInnis and Evan Chen and Avi Semler and Luk Gloor and Tej Shah and Marc Carauleanu and Pascal Lauer and Tran Đuc Huy and Hossein Shahrtash and Emilien Duc and Lukas Lewark and Assaf Brown and Samuel Albanie and Brian Weber and Warren S. Vaz and Pierre Clavier and Yiyang Fan and Gabriel Poesia Reis e Silva and Long and Lian and Marcus Abramovitch and Xi Jiang and Sandra Mendoza and Murat Islam and Juan Gonzalez and Vasilios Mavroudis and Justin Xu and Pawan Kumar and Laxman Prasad Goswami and Daniel Bugas and Nasser Heydari and Ferenc Jeanplong and Thorben Jansen and Antonella Pinto and Archimedes Apronti and Abdallah Galal and Ng Ze-An and Ankit Singh and Tong Jiang and Joan of Arc Xavier and Kanu Priya Agarwal and Mohammed Berkani and Gang Zhang and Zhehang Du and Benedito Alves de Oliveira Junior and Dmitry Malishev and Nicolas Remy and Taylor D. Hartman and Tim Tarver and Stephen Mensah and Gautier Abou Loume and Wiktor Morak and Farzad Habibi and Sarah Hoback and Will Cai and Javier Gimenez and Roselynn Grace Montecillo and Jakub Łucki and Russell Campbell and Asankhaya Sharma and Khalida Meer and Shreen Gul and Daniel Espinosa Gonzalez and Xavier Alapont and Alex Hoover and Gunjan Chhablani and Freddie Vargus and Arunim Agarwal and Yibo Jiang and Deepakkumar Patil and David Outevsky and Kevin Joseph Scaria and Rajat Maheshwari and Abdelkader Dendane and Priti Shukla and Ashley Cartwright and Sergei Bogdanov and Niels Mündler and Sören Möller and Luca Arnaboldi and Kunvar Thaman and Muhammad Rehan Siddiqi and Prajvi Saxena and Himanshu Gupta and Tony Fruhauff and Glen Sherman and Mátyás Vincze and Siranut Usawasutsakorn and Dylan Ler and Anil Radhakrishnan and Innocent Enyekwe and Sk Md Salauddin and Jiang Muzhen and Aleksandr Maksapetyan and Vivien Rossbach and Chris Harjadi and Mohsen Bahaloohoreh and Claire Sparrow and Jasdeep Sidhu and Sam Ali and Song Bian and John Lai and Eric Singer and Justine Leon Uro and Greg Bateman and Mohamed Sayed and Ahmed Menshawy and Darling Duclosel and Dario Bezzi and Yashaswini Jain and Ashley Aaron and Murat Tiryakioglu and Sheeshram Siddh and Keith Krenek and Imad Ali Shah and Jun Jin and Scott Creighton and Denis Peskoff and Zienab EL-Wasif and Ragavendran P V and Michael Richmond and Joseph McGowan and Tejal Patwardhan and Hao-Yu Sun and Ting Sun and Nikola Zubić and Samuele Sala and Stephen Ebert and Jean Kaddour and Manuel Schottdorf and Dianzhuo Wang and Gerol Petruzella and Alex Meiburg and Tilen Medved and Ali ElSheikh and S Ashwin Hebbar and Lorenzo Vaquero and Xianjun Yang and Jason Poulos and Vilém Zouhar and Sergey Bogdanik and Mingfang Zhang and Jorge Sanz-Ros and David Anugraha and Yinwei Dai and Anh N. Nhu and Xue Wang and Ali Anil Demircali and Zhibai Jia and Yuyin Zhou and Juncheng Wu and Mike He and Nitin Chandok and Aarush Sinha and Gaoxiang Luo and Long Le and Mickaël Noyé and Michał Perełkiewicz and Ioannis Pantidis and Tianbo Qi and Soham Sachin Purohit and Letitia Parcalabescu and Thai-Hoa Nguyen and Genta Indra Winata and Edoardo M. Ponti and Hanchen Li and Kaustubh Dhole and Jongee Park and Dario Abbondanza and Yuanli Wang and Anupam Nayak and Diogo M. Caetano and Antonio A. W. L. Wong and Maria del Rio-Chanona and Dániel Kondor and Pieter Francois and Ed Chalstrey and Jakob Zsambok and Dan Hoyer and Jenny Reddish and Jakob Hauser and Francisco-Javier Rodrigo-Ginés and Suchandra Datta and Maxwell Shepherd and Thom Kamphuis and Qizheng Zhang and Hyunjun Kim and Ruiji Sun and Jianzhu Yao and Franck Dernoncourt and Satyapriya Krishna and Sina Rismanchian and Bonan Pu and Francesco Pinto and Yingheng Wang and Kumar Shridhar and Kalon J. Overholt and Glib Briia and Hieu Nguyen and David and Soler Bartomeu and Tony CY Pang and Adam Wecker and Yifan Xiong and Fanfei Li and Lukas S. Huber and Joshua Jaeger and Romano De Maddalena and Xing Han Lù and Yuhui Zhang and Claas Beger and Patrick Tser Jern Kon and Sean Li and Vivek Sanker and Ming Yin and Yihao Liang and Xinlu Zhang and Ankit Agrawal and Li S. Yifei and Zechen Zhang and Mu Cai and Yasin Sonmez and Costin Cozianu and Changhao Li and Alex Slen and Shoubin Yu and Hyun Kyu Park and Gabriele Sarti and Marcin Briański and Alessandro Stolfo and Truong An Nguyen and Mike Zhang and Yotam Perlitz and Jose Hernandez-Orallo and Runjia Li and Amin Shabani and Felix Juefei-Xu and Shikhar Dhingra and Orr Zohar and My Chiffon Nguyen and Alexander Pondaven and Abdurrahim Yilmaz and Xuandong Zhao and Chuanyang Jin and Muyan Jiang and Stefan Todoran and Xinyao Han and Jules Kreuer and Brian Rabern and Anna Plassart and Martino Maggetti and Luther Yap and Robert Geirhos and Jonathon Kean and Dingsu Wang and Sina Mollaei and Chenkai Sun and Yifan Yin and Shiqi Wang and Rui Li and Yaowen Chang and Anjiang Wei and Alice Bizeul and Xiaohan Wang and Alexandre Oliveira Arrais and Kushin Mukherjee and Jorge Chamorro-Padial and Jiachen Liu and Xingyu Qu and Junyi Guan and Adam Bouyamourn and Shuyu Wu and Martyna Plomecka and Junda Chen and Mengze Tang and Jiaqi Deng and Shreyas Subramanian and Haocheng Xi and Haoxuan Chen and Weizhi Zhang and Yinuo Ren and Haoqin Tu and Sejong Kim and Yushun Chen and Sara Vera Marjanović and Junwoo Ha and Grzegorz Luczyna and Jeff J. Ma and Zewen Shen and Dawn Song and Cedegao E. Zhang and Zhun Wang and Gaël Gendron and Yunze Xiao and Leo Smucker and Erica Weng and Kwok Hao Lee and Zhe Ye and Stefano Ermon and Ignacio D. Lopez-Miguel and Theo Knights and Anthony Gitter and Namkyu Park and Boyi Wei and Hongzheng Chen and Kunal Pai and Ahmed Elkhanany and Han Lin and Philipp D. Siedler and Jichao Fang and Ritwik Mishra and Károly Zsolnai-Fehér and Xilin Jiang and Shadab Khan and Jun Yuan and Rishab Kumar Jain and Xi Lin and Mike Peterson and Zhe Wang and Aditya Malusare and Maosen Tang and Isha Gupta and Ivan Fosin and Timothy Kang and Barbara Dworakowska and Kazuki Matsumoto and Guangyao Zheng and Gerben Sewuster and Jorge Pretel Villanueva and Ivan Rannev and Igor Chernyavsky and Jiale Chen and Deepayan Banik and Ben Racz and Wenchao Dong and Jianxin Wang and Laila Bashmal and Duarte V. Gonçalves and Wei Hu and Kaushik Bar and Ondrej Bohdal and Atharv Singh Patlan and Shehzaad Dhuliawala and Caroline Geirhos and Julien Wist and Yuval Kansal and Bingsen Chen and Kutay Tire and Atak Talay Yücel and Brandon Christof and Veerupaksh Singla and Zijian Song and Sanxing Chen and Jiaxin Ge and Kaustubh Ponkshe and Isaac Park and Tianneng Shi and Martin Q. Ma and Joshua Mak and Sherwin Lai and Antoine Moulin and Zhuo Cheng and Zhanda Zhu and Ziyi Zhang and Vaidehi Patil and Ketan Jha and Qiutong Men and Jiaxuan Wu and Tianchi Zhang and Bruno Hebling Vieira and Alham Fikri Aji and Jae-Won Chung and Mohammed Mahfoud and Ha Thi Hoang and Marc Sperzel and Wei Hao and Kristof Meding and Sihan Xu and Vassilis Kostakos and Davide Manini and Yueying Liu and Christopher Toukmaji and Jay Paek and Eunmi Yu and Arif Engin Demircali and Zhiyi Sun and Ivan Dewerpe and Hongsen Qin and Roman Pflugfelder and James Bailey and Johnathan Morris and Ville Heilala and Sybille Rosset and Zishun Yu and Peter E. Chen and Woongyeong Yeo and Eeshaan Jain and Ryan Yang and Sreekar Chigurupati and Julia Chernyavsky and Sai Prajwal Reddy and Subhashini Venugopalan and Hunar Batra and Core Francisco Park and Hieu Tran and Guilherme Maximiano and Genghan Zhang and Yizhuo Liang and Hu Shiyu and Rongwu Xu and Rui Pan and Siddharth Suresh and Ziqi Liu and Samaksh Gulati and Songyang Zhang and Peter Turchin and Christopher W. Bartlett and Christopher R. Scotese and Phuong M. Cao and Aakaash Nattanmai and Gordon McKellips and Anish Cheraku and Asim Suhail and Ethan Luo and Marvin Deng and Jason Luo and Ashley Zhang and Kavin Jindel and Jay Paek and Kasper Halevy and Allen Baranov and Michael Liu and Advaith Avadhanam and David Zhang and Vincent Cheng and Brad Ma and Evan Fu and Liam Do and Joshua Lass and Hubert Yang and Surya Sunkari and Vishruth Bharath and Violet Ai and James Leung and Rishit Agrawal and Alan Zhou and Kevin Chen and Tejas Kalpathi and Ziqi Xu and Gavin Wang and Tyler Xiao and Erik Maung and Sam Lee and Ryan Yang and Roy Yue and Ben Zhao and Julia Yoon and Sunny Sun and Aryan Singh and Ethan Luo and Clark Peng and Tyler Osbey and Taozhi Wang and Daryl Echeazu and Hubert Yang and Timothy Wu and Spandan Patel and Vidhi Kulkarni and Vijaykaarti Sundarapandiyan and Ashley Zhang and Andrew Le and Zafir Nasim and Srikar Yalam and Ritesh Kasamsetty and Soham Samal and Hubert Yang and David Sun and Nihar Shah and Abhijeet Saha and Alex Zhang and Leon Nguyen and Laasya Nagumalli and Kaixin Wang and Alan Zhou and Aidan Wu and Jason Luo and Anwith Telluri and Summer Yue and Alexandr Wang and Dan Hendrycks},
      eprint        = {2501.14249},
      primaryclass  = {cs.LG},
      title         = {Humanity's Last Exam},
      url           = {https://arxiv.org/abs/2501.14249},
      year          = {2025}
    }
  ratings:
    specification:
      rating: 8.5
      reason: |-
        Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified
    dataset:
      rating: 6
      reason: |-
        Data accessible through Hugging Face, but requires giving contact information to access
    metrics:
      rating: 10
      reason: |-
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 6
      reason: |-
        Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result
    documentation:
      rating: 0.5
      reason: No specified way to reproduce the reference solution
- date: '2024-11-07'
  expired:
  valid: 'yes'
  name: FrontierMath
  url: https://arxiv.org/abs/2411.04872
  domain: Mathematics
  focus: Challenging advanced mathematical reasoning
  keywords:
  - symbolic reasoning
  - number theory
  - algebraic geometry
  - category theory
  description: |-
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’ 
    ability to solve problems requiring deep abstract reasoning.
  task_types:
  - Problem solving
  ai_capability_measured: Symbolic and abstract mathematical reasoning
  metrics:
  - Accuracy
  models: []
  notes: Good
  cite:
  - |-
    @misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
      archiveprefix = {arXiv},
      author        = {Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli Järviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},
      eprint        = {2411.04872},
      primaryclass  = {cs.AI},
      title         = {FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
      url           = {https://arxiv.org/abs/2411.04872},
      year          = {2024}
    }
  ratings:
    specification:
      rating: 9
      reason: |-
        Well-specified process for asking questions and receiving answers. No HW constraints
    dataset:
      rating: 0.5
      reason: |-
        Paper and website had no link to any dataset. It may still exist somewhere
    metrics:
      rating: 10
      reason: |-
        (by default) All questions in the dataset are multiple choice, all have a correct answer
    reference_solution:
      rating: 9
      reason: Displays result of leading models on the benchmark
    documentation:
      rating: 0.5
      reason: No specified way to reproduce the reference solution
- date: '2024-07-18'
  expired:
  valid: 'yes'
  name: SciCode
  url: https://arxiv.org/abs/2407.13168
  domain: Scientific Programming
  focus: Scientific code generation and problem solving
  keywords:
  - code synthesis
  - scientific computing
  - programming benchmark
  description: |-
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  task_types:
  - Coding
  ai_capability_measured: Program synthesis, scientific computing
  metrics:
  - Solve rate (%)
  models:
  - Claude3.5-Sonnet
  notes: Good
  cite:
  - |-
    @misc{tian2024scicoderesearchcodingbenchmark,
      archiveprefix = {arXiv},
      author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
      eprint        = {2407.13168},
      primaryclass  = {cs.AI},
      title         = {SciCode: A Research Coding Benchmark Curated by Scientists},
      url           = {https://arxiv.org/abs/2407.13168},
      year          = {2024}
    }
  ratings:
    specification:
      rating: 6
      reason: |-
        Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.
    dataset:
      rating: 0.5
      reason: |-
        Paper and website had no link to any dataset. It may still exist somewhere
    metrics:
      rating: 4
      reason: Metrics stated, but not specified in detail
    reference_solution:
      rating: 9
      reason: Models presented with scores
    documentation:
      rating: 0.5
      reason: No specified way to reproduce the reference solution
- date: '2025-03-13'
  expired:
  valid: 'yes'
  name: AIME (American Invitational Mathematics Examination)
  url: |-
    https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions
  domain: Mathematics
  focus: Pre-college advanced problem solving
  keywords:
  - algebra
  - combinatorics
  - number theory
  - geometry
  description: |-
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  task_types:
  - Problem solving
  ai_capability_measured: Mathematical problem-solving and reasoning
  metrics:
  - Accuracy
  models: []
  notes:
  cite:
  - |-
    @misc{www-aime,
      author = {TBD},
      title = {AIME},
      url = {https://www.vals.ai/benchmarks/aime-2025-03-13},
      month = mar,
      year = 2025,
      note = {[Online accessed 2025-06-24]}
    }
  ratings:
    specification:
      rating: 3
      reason: |-
        Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints
    dataset:
      rating: 9
      reason: Easily accessible data with problems and solutions
    metrics:
      rating: 10
      reason: (by default) Answer is correct or it's not
    reference_solution:
      rating: 0
      reason: |-
        Not given. Human performance stats exist, but no mentions of AI performance
    documentation:
      rating: 0
      reason: Not given
- date: '2025-02-15'
  expired:
  valid: 'yes'
  name: MATH-500
  url: https://huggingface.co/datasets/HuggingFaceH4/MATH-500
  domain: Mathematics
  focus: Math reasoning generalization
  keywords:
  - calculus
  - algebra
  - number theory
  - geometry
  description: |-
    MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
    high-school to advanced levels, designed to evaluate LLMs mathematical reasoning and 
    generalization.
  task_types:
  - Problem solving
  ai_capability_measured: Math reasoning and generalization
  metrics:
  - Accuracy
  models: []
  notes: Dataset hosted on Hugging Face
  cite:
  - |-
    @misc{huggingface2025math500,
      title={MATH-500},
      author={HuggingFaceH4},
      year={2025},
      url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}
    }
  ratings:
    specification:
      rating: 3
      reason: |-
        Known what the problems are, but method of presentation and evaluation is not stated. No HW constraints
    dataset:
      rating: 9.9
      reason: |-
        Problems and solutions are easily downloaded. Could not find a way to download the data
    metrics:
      rating: 2
      reason: |-
        Problem spec states that all of the AI reasoning steps are subject to grading, but no specified way to evaluate the steps
    reference_solution:
      rating: 0
      reason: Not given
    documentation:
      rating: 0.5
      reason: Not given. Implicit instructions to download dataset.
- date: '2024-04-02'
  expired:
  valid: 'yes'
  name: |-
    CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)
  url: https://arxiv.org/abs/2404.02029
  domain: Multidomain Science
  focus: Long-context scientific reasoning
  keywords:
  - long-context
  - information extraction
  - multimodal
  description: |-
    CURIE is a benchmark of 580 problems across six scientific disciplines-materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics-
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  task_types:
  - Information extraction
  - Reasoning
  - Concept tracking
  - Aggregation
  - Algebraic manipulation
  - Multimodal comprehension
  ai_capability_measured: Long-context understanding and scientific reasoning
  metrics:
  - Accuracy
  models: []
  notes: Good
  cite:
  - |-
    @misc{curie2024,
      title={Scientific Reasoning Benchmarks from the CURIE Dataset},
      author={TODO: Add authors},
      year={2024},
      url={https://arxiv.org/abs/2404.02029}
    }
  ratings:
    specification:
      rating: 0
      reason: This is an AI model, not a benchmark
    dataset:
      rating: 0
      reason: This is an AI model, not a benchmark
    metrics:
      rating: 0
      reason: This is an AI model, not a benchmark
    reference_solution:
      rating: 0
      reason: This is an AI model, not a benchmark
    documentation:
      rating: 0
      reason: This is an AI model, not a benchmark
- date: '2023-01-26'
  expired:
  valid: 'yes'
  name: FEABench (Finite Element Analysis Benchmark)
  url: https://github.com/alleninstitute/feabench
  domain: Computational Engineering
  focus: FEA simulation accuracy and performance
  keywords:
  - finite element
  - simulation
  - PDE
  description: |-
    FEABench is a suite evaluating finite element analysis tools on standardized 
    PDE-based simulation tasks with complex geometries and boundary conditions, 
    measuring both accuracy and runtime performance.
  task_types:
  - Simulation
  - Performance evaluation
  ai_capability_measured: Numerical simulation accuracy and efficiency
  metrics:
  - Solve time
  - Error norm
  models:
  - FEniCS
  - deal.II
  notes: Good
  cite:
  - |-
    @misc{allen2023feabench,
      title={FEABench: A Finite Element Analysis Benchmark},
      author={Allen Institute},
      year={2023},
      url={https://github.com/alleninstitute/feabench}
    }
  ratings:
    specification:
      rating: 0
      reason: Using the link results in a 404 Not Found error
    dataset:
      rating: 0
      reason: Using the link results in a 404 Not Found error
    metrics:
      rating: 0
      reason: Using the link results in a 404 Not Found error
    reference_solution:
      rating: 0
      reason: Using the link results in a 404 Not Found error
    documentation:
      rating: 0
      reason: Using the link results in a 404 Not Found error
- date: '2024-07-12'
  expired:
  valid: 'yes'
  name: SPIQA (Scientific Paper Image Question Answering)
  url: https://arxiv.org/abs/2407.09413
  domain: Computer Science
  focus: Multimodal QA on scientific figures
  keywords:
  - multimodal QA
  - figure understanding
  - table comprehension
  - chain-of-thought
  description: |-
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  task_types:
  - Question answering
  - Multimodal QA
  - Chain-of-Thought evaluation
  ai_capability_measured: Visual-textual reasoning in scientific contexts
  metrics:
  - Accuracy
  - F1 score
  models:
  - Chain-of-Thought models
  - Multimodal QA systems
  notes: Good
  cite:
  - |-
    @misc{zhong2024spiqa,
      title={SPIQA: Scientific Paper Image Question Answering},
      author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
      year={2024},
      url={https://arxiv.org/abs/2407.09413}
    }
  ratings:
    specification:
      rating: 10
      reason: |-
        Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.
    dataset:
      rating: 9
      reason: |-
        Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.
    metrics:
      rating: 9
      reason: |-
        Uses quantitative metrics (Accuracy, F1) aligned with the task. Well-suited for benchmarking multimodal reasoning.
    reference_solution:
      rating: 5
      reason: |-
        Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.
    documentation:
      rating: 2
      reason: |-
        Dataset and benchmark description provided; code/software mentioned; however, full step-by-step setup or containerized environment not stated.
- date: '2020-09-28'
  expired:
  valid: 'yes'
  name: MedQA
  url: https://arxiv.org/abs/2009.13081
  domain: Medical Question Answering
  focus: Medical board exam QA
  keywords:
  - USMLE
  - diagnostic QA
  - medical knowledge
  - multilingual
  description: |-
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  task_types:
  - Multiple choice
  ai_capability_measured: Medical diagnosis and knowledge retrieval
  metrics:
  - Accuracy
  models:
  - Neural reader
  - Retrieval-based QA systems
  notes: Multilingual (English, Simplified and Traditional Chinese)
  cite:
  - |-
    @misc{jin2020diseasedoespatienthave,
        archiveprefix = {arXiv},
        author        = {Di Jin and Eileen Pan and Nassim Oufattole and Wei-Hung Weng and Hanyi Fang and Peter Szolovits},
        eprint        = {2009.13081},
        primaryclass  = {cs.CL},
        title         = {What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams},
        url           = {https://arxiv.org/abs/2009.13081},
        year          = {2020}
      }
  ratings:
    specification:
      rating: 9
      reason: |-
        Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.
    dataset:
      rating: 8
      reason: |-
        Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.
    metrics:
      rating: 9
      reason: |-
        Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.
    reference_solution:
      rating: 7
      reason: |-
        Model results reported (GPT-4, Med-PaLM, etc.); implementations discussed in papers, but runnable baselines not fully packaged or documented.
    documentation:
      rating: 6
      reason: |-
        Dataset and paper are accessible; instructions on how to use the source code available, but environment setup or full reproducibility workflow is not packaged.
- date: '2025-05-13'
  expired:
  valid: 'yes'
  name: BaisBench (Biological AI Scientist Benchmark)
  url: https://arxiv.org/abs/2505.08341
  domain: Computational Biology
  focus: Omics-driven AI research tasks
  keywords:
  - single-cell annotation
  - biological QA
  - autonomous discovery
  description: |-
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  task_types:
  - Cell type annotation
  - Multiple choice
  ai_capability_measured: Autonomous biological research capabilities
  metrics:
  - Annotation accuracy
  - QA accuracy
  models:
  - LLM-based AI scientist agents
  notes: Underperforms human experts; aims to advance AI-driven discovery
  cite:
  - |-
    @misc{luo2025benchmarkingaiscientistsomics,
      archiveprefix = {arXiv},
      author        = {Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},
      eprint        = {2505.08341},
      primaryclass  = {cs.AI},
      title         = {Benchmarking AI scientists in omics data-driven biological research},
      url           = {https://arxiv.org/abs/2505.08341},
      year          = {2025}
    }
  ratings:
    specification:
      rating: 9
      reason: |-
        Task clearly defined-cell type annotation and biological QA; input/output formats are well-described; system constraints are not deeply quantified.
    dataset:
      rating: 8
      reason: |-
        Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.
    metrics:
      rating: 9
      reason: |-
        Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.
    reference_solution:
      rating: 7
      reason: |-
        Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline with training/eval pipeline confirmed yet.
    documentation:
      rating: 8
      reason: |-
        Dataset and paper accessible; IPYNB files for setup are available on the github repo; further instructions are minimal.
- date: '2023-01-26'
  expired:
  valid: 'yes'
  name: MOLGEN
  url: https://github.com/zjunlp/MolGen
  domain: Computational Chemistry
  focus: Molecular generation and optimization
  keywords:
  - SELFIES
  - GAN
  - property optimization
  description: |-
    MolGen is a pre-trained molecular language model that generates chemically valid
    molecules using SELFIES and reinforcement learning, guided by chemical feedback 
    to optimize properties such as logP, QED, and docking score.
  task_types:
  - Distribution learning
  - Goal-oriented generation
  ai_capability_measured: Generation of valid and optimized molecular structures
  metrics:
  - Validity%
  - Novelty%
  - QED
  - Docking score
  models:
  - MolGen
  notes: This is a model, not a benchmark
  cite:
  - |-
    @misc{fang2024domainagnosticmoleculargenerationchemical,
      archiveprefix = {arXiv},
      author        = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
      eprint        = {2301.11259},
      primaryclass  = {cs.LG},
      title         = {Domain-Agnostic Molecular Generation with Chemical Feedback},
      url           = {https://arxiv.org/abs/2301.11259},
      year          = {2024}
    }
  ratings:
    specification:
      rating: 8
      reason: |-
        The molecular generation task is well-defined, with input/output via SELFIES and chemical properties
    dataset:
      rating: 7
      reason: |-
        Uses standard datasets (ZINC, MOSES, QM9); accessible and widely used, but FAIR metadata, versioning, and splits are not detailed within this specific repo.
    metrics:
      rating: 9
      reason: |-
        Metrics like Validity%, Novelty%, QED, and Docking Score are quantitative, supporting clear model evaluation.
    reference_solution:
      rating: 6
      reason: |-
        Model is released and functional; some training/evaluation code exists, but it's not framed as a reusable baseline in a benchmark context.
    documentation:
      rating: 6.5
      reason: |-
        Code is available and usable; instructions exist, though setup may require domain knowledge or adaptation for different datasets/environments.
- date: '2020-05-02'
  expired:
  valid: 'yes'
  name: Open Graph Benchmark (OGB) - Biology
  url: https://ogb.stanford.edu/docs/home/
  domain: Graph ML
  focus: Biological graph property prediction
  keywords:
  - node prediction
  - link prediction
  - graph classification
  description: |-
    OGB-Biology is a suite of large-scale biological network datasets (protein-protein
    interaction, drug-target, etc.) with standardized splits and evaluation protocols 
    for node, link, and graph property prediction tasks.
  task_types:
  - Node property prediction
  - Link property prediction
  - Graph property prediction
  ai_capability_measured: Scalability and generalization in graph ML for biology
  metrics:
  - Accuracy
  - ROC-AUC
  models:
  - GCN
  - GraphSAGE
  - GAT
  notes: Community-driven updates
  cite:
  - |-
    @misc{hu2021opengraphbenchmarkdatasets,
        archiveprefix = {arXiv},
        author        = {Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
        eprint        = {2005.00687},
        primaryclass  = {cs.LG},
        title         = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
        url           = {https://arxiv.org/abs/2005.00687},
        year          = {2021}
    }
  ratings:
    specification:
      rating: 10
      reason: |-
        Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined.
    dataset:
      rating: 10
      reason: |-
        Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.
    metrics:
      rating: 10
      reason: |-
        Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.
    reference_solution:
      rating: 7
      reason: |-
        Multiple baselines implemented and documented (GCN, GAT, GraphSAGE), though most are provided by 3rd parties.
    documentation:
      rating: 10
      reason: |-
        Full codebase available via GitHub, with documented installation and usage instructions.
- date: '2011-10-01'
  expired:
  valid: 'yes'
  name: Materials Project
  url: https://materialsproject.org/
  domain: Materials Science
  focus: DFT-based property prediction
  keywords:
  - DFT
  - materials genome
  - high-throughput
  description: |-
    The Materials Project provides an open-access database of computed properties for
    inorganic materials via high-throughput density functional theory (DFT), accelerating 
    materials discovery.
  task_types:
  - Property prediction
  ai_capability_measured: Prediction of inorganic material properties
  metrics:
  - MAE
  - R²
  models:
  - Automatminer
  - Crystal Graph Neural Networks
  notes: Core component of the Materials Genome Initiative
  cite:
  - |-
    @article{jain2013materials,
      title={The Materials Project: A materials genome approach},
      author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},
      journal={APL Materials},
      volume    = {1},
      number    = {1},
      year={2013},
      doi       = {10.1063/1.4812323},
      url={https://materialsproject.org/}
    }
  ratings:
    specification:
      rating: 8
      reason: |-
        The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.
    dataset:
      rating: 8
      reason: |-
        Data is versioned, accessible through both UI and API, with rich metadata and citations; widely reused. API key required to access data.
    metrics:
      rating: 10
      reason: Uses numerical metrics like MAE and R²
    reference_solution:
      rating: 6
      reason: |-
        Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no single canonical baseline is tightly integrated into the platform.
    documentation:
      rating: 7
      reason: |-
        Extensive API, code repositories, and user guides exist, but end-to-end benchmarking workflows require additional setup by users. 'Documentation' link did not work.
- date: '2020-10-20'
  expired:
  valid: 'yes'
  name: OCP (Open Catalyst Project)
  url: https://opencatalystproject.org/
  domain: Chemistry; Materials Science
  focus: Catalyst adsorption energy prediction
  keywords:
  - DFT relaxations
  - adsorption energy
  - graph neural networks
  description: |-
    The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate 
    relaxation datasets, challenging ML models to predict energies and forces for 
    renewable energy applications.
  task_types:
  - Energy prediction
  - Force prediction
  ai_capability_measured: Prediction of adsorption energies and forces
  metrics:
  - MAE (energy)
  - MAE (force)
  models:
  - CGCNN
  - SchNet
  - DimeNet++
  - GemNet-OC
  notes: Public leaderboards; active community development
  cite:
  - |-
    @article{chanussot2021oc20,
      title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},
      author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
      journal   = {ACS Catalysis},
      volume    = {11},
      number    = {10},
      pages     = {6059--6072},
      year      = {2021},
      doi       = {10.1021/acscatal.0c04525},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}
    }
  - |-
    @article{tran2023oc22,
      title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
      author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
      journal   = {ACS Catalysis},
      volume    = {13},
      number    = {5},
      pages     = {3066--3084},
      year      = {2023},
      doi       = {10.1021/acscatal.2c05426},
      url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}
    }
  - |-
    @article{doi:10.1021/acscatal.0c04525,
      author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
    title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
      journal = {ACS Catalysis},
      volume = {11},
      number = {10},
      pages = {6059-6072},
      year = {2021},
      doi = {10.1021/acscatal.0c04525},
      URL = {https://doi.org/10.1021/acscatal.0c04525},eprint = {https://doi.org/10.1021/acscatal.0c04525}}"
  - |-
    @article{tran2023b,
      title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
      volume={13},
      ISSN={2155-5435},
      url={http://dx.doi.org/10.1021/acscatal.2c05426},
      DOI={10.1021/acscatal.2c05426},
      number={5},
      journal={ACS Catalysis},
      publisher={American Chemical Society (ACS)},
      author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
      year={2023},
      month=feb, pages={3066-3084} 
    }
  ratings:
    specification:
      rating: 10
      reason: |-
        Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.
    dataset:
      rating: 9.5
      reason: |-
        Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.
    metrics:
      rating: 10
      reason: MAE (energy and force) are standard and reproducible.
    reference_solution:
      rating: 9
      reason: |-
        Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated; highly cited with documented performance.
    documentation:
      rating: 9
      reason: |-
        Code, data loaders, usage instructions, and leaderboard available; minor setup effort may still be required for full reproduction.
- date: '2023-06-20'
  expired:
  valid: 'yes'
  name: JARVIS-Leaderboard
  url: https://arxiv.org/abs/2306.11688
  domain: Materials Science; Benchmarking
  focus: Comparative evaluation of materials design methods
  keywords:
  - leaderboards
  - materials methods
  - simulation
  description: |-
    JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
    structure, force-fields, quantum computing, and experimental methods across hundreds
    of materials science tasks.
  task_types:
  - Method benchmarking
  - Leaderboard ranking
  ai_capability_measured: Performance comparison across diverse materials design methods
  metrics:
  - MAE
  - RMSE
  - Accuracy
  models: []
  notes: 1,281 contributions across 274 benchmarks
  cite:
  - |-
    @article{choudhary2024jarvis,
      title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},
    author = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},
      journal = {npj Computational Materials},
      volume = {10},
      number = {1},
      pages = {93},
      year = {2024},
      doi = {10.1038/s41524-024-01259-w},
      url = {https://doi.org/10.1038/s41524-024-01259-w}
    }
  ratings:
    specification:
      rating: 6
      reason: |-
        Tasks are clearly defined; heterogeneity in benchmarks slightly reduces uniformity; I/O format is not specified
    dataset:
      rating: 9
      reason: |-
        Data is versioned, public, and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks.
    metrics:
      rating: 4
      reason: Overall goal is stated, but the exact metric evaluated is not listed
    reference_solution:
      rating: 5
      reason: |-
        Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); documentation is good, but baselines may be hard to find or not available for every individual task.
    documentation:
      rating: 8.5
      reason: |-
        JARVIS-Tools and leaderboard APIs are well-documented and actively maintained; minimal setup burden, though some task-specific workflows may require additional guidance.
- date: '2022-02-22'
  expired:
  valid: 'yes'
  name: Quantum Computing Benchmarks (QML)
  url:
  - https://github.com/XanaduAI/qml-benchmarks
  - https://pennylane.ai/datasets/collection/qml-benchmarks
  domain: Quantum Computing
  focus: Quantum algorithm performance evaluation
  keywords:
  - quantum circuits
  - state preparation
  - error correction
  description: |-
    A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
    preparation, circuit optimization, and error correction across multiple platforms.
  task_types:
  - Circuit benchmarking
  - State classification
  ai_capability_measured: Quantum algorithm performance and fidelity
  metrics:
  - Fidelity
  - Success probability
  models:
  - IBM Q
  - IonQ
  - AQT@LBNL
  notes: |-
    Hardware-agnostic, application-level metrics. The citation may not be correct.
  cite:
  - |-
    @inproceedings{kiwit2023,
      title={Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK},
      url={http://dx.doi.org/10.1109/QCE57702.2023.00061},
      DOI={10.1109/qce57702.2023.00061},
      booktitle={2023 IEEE International Conference on Quantum Computing and Engineering (QCE)},
      publisher={IEEE},
      author={Kiwit, Florian J. and Marso, Marwa and Ross, Philipp and Riofrío, Carlos A. and Klepsch, Johannes and Luckow, Andre},
      year={2023},
      month=sep, pages={475-484}
    }
  ratings:
    specification:
      rating: 9
      reason: |-
        Tasks like fidelity estimation, state preparation, and runtime benchmarking are clearly defined; I/O formats vary slightly across hardware but are consistently framed in PennyLane/Qiskit ecosystems.
    dataset:
      rating: 8
      reason: |-
        Datasets are accessible, structured, and interoperable via PennyLane; however, not all are versioned or richly annotated in conventional ML metadata standards.
    metrics:
      rating: 9
      reason: |-
        Quantitative and well-motivated metrics (e.g., fidelity, success probability) are used, though reproducibility can depend on hardware noise profiles.
    reference_solution:
      rating: 5
      reason: |-
        Reference implementations exist and are integrated into tools like PennyLane, but performance varies per backend; not all benchmarks include reproducible reference runs.
    documentation:
      rating: 8
      reason: |-
        Strong integration with PennyLane and QML ecosystem; guides and code provided, but advanced hardware setup may pose reproducibility hurdles for newcomers.
- date: '2024-10-01'
  expired:
  valid: 'yes'
  name: CFDBench (Fluid Dynamics)
  url: https://arxiv.org/abs/2310.05963
  domain: Fluid Dynamics; Scientific ML
  focus: Neural operator surrogate modeling
  keywords:
  - neural operators
  - CFD
  - FNO
  - DeepONet
  description: |-
    CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
    assessing neural operators' ability to generalize to unseen PDE parameters and domains.
  task_types:
  - Surrogate modeling
  ai_capability_measured: Generalization of neural operators for PDEs
  metrics:
  - L2 error
  - MAE
  models:
  - FNO
  - DeepONet
  - U-Net
  notes: 302K frames across 739 cases
  cite:
  - |-
    @misc{luo2024cfdbenchlargescalebenchmarkmachine,
      title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},
      author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},
      year={2024},
      url={https://arxiv.org/abs/2310.05963}
    }
  ratings:
    specification:
      rating: 10
      reason: |-
        Tasks are clearly framed (PDE regression, surrogate modeling), with explicit details on the four canonical CFD problems, input/output structure, and generalization goals.
    dataset:
      rating: 10
      reason: |-
        Publicly available on Zenodo, versioned, with metadata and splits; covers thousands of simulations with proper documentation.
    metrics:
      rating: 9
      reason: |-
        Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.
    reference_solution:
      rating: 8
      reason: |-
        Baseline models like FNO and DeepONet are implemented, but full reproduction pipelines or eval scripts may require additional user configuration.
    documentation:
      rating: 6
      reason: |-
        GitHub and Zenodo provide data and code, but setup for evaluating across all 739 cases requires moderate user effort and technical fluency with PyTorch-based frameworks. Reproducibility depends on full implementation details.
- date:
  expired:
  valid: 'yes'
  name: SatImgNet
  url:
  domain: Remote Sensing
  focus: Satellite imagery classification
  keywords:
  - land-use
  - zero-shot
  - multi-task
  description: |-
    SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
    imagery classification datasets evaluating zero-shot transfer of vision-language models
    across diverse remote sensing tasks.
  task_types:
  - Image classification
  ai_capability_measured: Zero-shot land-use classification
  metrics:
  - Accuracy
  models: []
  notes: Public leaderboard available
  cite:
  - |-
    @misc{roberts2023satinmultitaskmetadatasetclassifying,
      title={SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models}, 
      author={Jonathan Roberts and Kai Han and Samuel Albanie},
      year={2023},
      eprint={2304.11619},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.11619}, 
    }
  ratings:
    specification:
      rating: 9
      reason: |-
        Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.
    dataset:
      rating: 9
      reason: |-
        Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.
    metrics:
      rating: 9
      reason: |-
        Standard quantitative metrics (Accuracy, Top-1 Accuracy) aligned with classification tasks; consistent across models, with leaderboard results available.
    reference_solution:
      rating: 7
      reason: |-
        Baselines like CLIP, BLIP, ALBEF evaluated in the paper; full inference pipelines or training code may need reconstruction from paper or GitHub references.
    documentation:
      rating: 7
      reason: |-
        Good usage guidance via Hugging Face and paper; example scripts and evaluation tools exist, but end-to-end reproducibility may require manual integration of model checkpoints and preprocessing.
- date: '2023-07-19'
  expired:
  valid: 'yes'
  name: ClimateLearn
  url: https://arxiv.org/abs/2307.01909
  domain: Climate Science; Forecasting
  focus: ML for weather and climate modeling
  keywords:
  - medium-range forecasting
  - ERA5
  - data-driven
  description: |-
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  task_types:
  - Forecasting
  ai_capability_measured: Global weather prediction (3-5 days)
  metrics:
  - RMSE
  - Anomaly correlation
  models:
  - CNN baselines
  - ResNet variants
  notes: |-
    Includes physical and ML baselines. Appears to be the same as the SatImgNet entry
  cite:
  - |-
    @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
      title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
      author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
      year={2023}, eprint={2307.01909}, 
      archivePrefix={arXiv}, 
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.01909}
    }
  ratings:
    specification:
      rating: 10
      reason: |-
        Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    dataset:
      rating: 10
      reason: |-
        Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    metrics:
      rating: 9
      reason: |-
        ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    reference_solution:
      rating: 8
      reason: |-
        Multiple baselines (e.g., FourCastNet, ClimaX) are provided and evaluated; implementations are available but may require tuning or GPU-specific configuration.
    documentation:
      rating: 8.5
      reason: |-
        Comprehensive setup via GitHub, including data loaders, training scripts, config files, and reproducibility protocols; minor complexity in large-scale data preprocessing.
- date: '2022-06-09'
  expired:
  valid: 'yes'
  name: BIG-Bench (Beyond the Imitation Game Benchmark)
  url: https://github.com/google/BIG-bench
  domain: NLP; AI Evaluation
  focus: Diverse reasoning and generalization tasks
  keywords:
  - few-shot
  - multi-task
  - bias analysis
  description: |-
    BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning, 
    knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
  task_types:
  - Few-shot evaluation
  - Multi-task evaluation
  ai_capability_measured: Reasoning and generalization across diverse tasks
  metrics:
  - Accuracy
  - Task-specific metrics
  models:
  - GPT-3
  - Dense Transformers
  - Sparse Transformers
  notes: Human baselines included
  cite:
  - |-
    @misc{srivastava2023imitationgamequantifyingextrapolating,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.04615}, 
    }
  ratings:
    specification:
      rating: 9.0
      reason: |-
        Tasks are diverse and clearly described; input/output formats are usually defined but vary widely, and system constraints are not standardized.
    dataset:
      rating: 9.0
      reason: |-
        Public, versioned, and well-documented; FAIR overall, though consistency and metadata completeness vary across tasks.
    metrics:
      rating: 8.0
      reason: |-
        Many tasks use standard quantitative metrics (accuracy, BLEU, F1), but others involve subjective ratings (e.g., Likert), which reduces cross-task comparability.
    reference_solution:
      rating: 7.0
      reason: |-
        Human baselines and LLM performance results are included; however, runnable reference solutions are limited and setup is not fully turnkey.
    documentation:
      rating: 8.0
      reason: |-
        Excellent GitHub documentation with usage examples, task templates, and tooling; task diversity may require manual task-by-task execution setup.
- date: '2019-11-20'
  expired:
  valid: 'yes'
  name: CommonSenseQA
  url: |-
    https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge
  domain: NLP; Commonsense
  focus: Commonsense question answering
  keywords:
  - ConceptNet
  - multiple-choice
  - adversarial
  description: |-
    CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
    requiring models to apply commonsense knowledge to select the correct answer 
    among five choices.
  task_types:
  - Multiple choice
  ai_capability_measured: Commonsense reasoning and knowledge integration
  metrics:
  - Accuracy
  models:
  - BERT-large
  - RoBERTa
  - GPT-3
  notes: Baseline 56%, human 89%
  cite:
  - |-
    @misc{talmor2019commonsenseqaquestionansweringchallenge,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00937}, 
    }
  ratings:
    specification:
      rating: 9.0
      reason: |-
        Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified.
    dataset:
      rating: 9.0
      reason: |-
        Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries.
    metrics:
      rating: 9.0
      reason: |-
        Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation.
    reference_solution:
      rating: 8.0
      reason: |-
        Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not bundled as an official starter kit.
    documentation:
      rating: 7.0
      reason: |-
        Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually connecting models to dataset.
- date: '2019-07-24'
  expired:
  valid: 'yes'
  name: Winogrande
  url: https://leaderboard.allenai.org/winogrande/submissions/public
  domain: NLP; Commonsense
  focus: Winograd Schema-style pronoun resolution
  keywords:
  - adversarial
  - pronoun resolution
  description: |-
    WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
    questions with reduced bias using AFLite, serving as both a benchmark and transfer 
    learning resource.
  task_types:
  - Pronoun resolution
  ai_capability_measured: Robust commonsense reasoning
  metrics:
  - Accuracy
  - AUC
  models:
  - RoBERTa
  - BERT
  - GPT-2
  notes: Human ~94%
  cite:
  - |-
    @misc{sakaguchi2019winograndeadversarialwinogradschema,
      archiveprefix = {arXiv},
      author        = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      eprint        = {1907.10641},
      primaryclass  = {cs.CL},
      title         = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
      url           = {https://arxiv.org/abs/1907.10641},
      year          = {2019}
    }
    
  ratings:
    specification:
      rating: 9.0
      reason: |-
        Task (pronoun/coreference resolution) is clearly defined in Winograd Schema style, with consistent input/output format; no system constraints included.
    dataset:
      rating: 9.0
      reason: |-
        Public, versioned, and FAIR-compliant with AFLite-generated splits to reduce annotation artifacts; hosted by AllenAI with good metadata.
    metrics:
      rating: 9.0
      reason: |-
        Accuracy and AUC are quantitative and well-aligned with disambiguation goals; standardized across evaluations.
    reference_solution:
      rating: 8.0
      reason: |-
        Baseline results for BERT, RoBERTa, GPT-2, etc., are published, but official runnable baselines require setup via AllenNLP or other frameworks.
    documentation:
      rating: 6.0
      reason: |-
        Dataset page and paper provide sufficient detail; usage with HuggingFace is smooth, but full reproducibility for training requires configuration effort.
