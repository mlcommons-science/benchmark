# SciCode

<p><a class="md-button back-link" href="../">‚Üê Back to all benchmarks</a></p>
<div class="info-block meta-block">
  <p class="meta-row"><span class="meta-label">Date</span><span class="meta-sep">:</span> <span class="meta-value">2024-07-18</span></p>
  <p class="meta-row"><span class="meta-label">Name</span><span class="meta-sep">:</span> <span class="meta-value">SciCode</span></p>
  <p class="meta-row"><span class="meta-label">Domain</span><span class="meta-sep">:</span> <span class="meta-value">Scientific Programming</span></p>
  <p class="meta-row"><span class="meta-label">Focus</span><span class="meta-sep">:</span> <span class="meta-value">Scientific code generation and problem solving</span></p>
  <p class="meta-row"><span class="meta-label">Task Types</span><span class="meta-sep">:</span> <span class="meta-value">Coding</span></p>
  <p class="meta-row"><span class="meta-label">Metrics</span><span class="meta-sep">:</span> <span class="meta-value">Solve rate (%)</span></p>
  <p class="meta-row"><span class="meta-label">Models</span><span class="meta-sep">:</span> <span class="meta-value">Claude3.5-Sonnet</span></p>
</div>
<h3>Keywords</h3>

<div class="chips"><a class="chip chip-link" href="../#kw=code%20synthesis">code synthesis</a> <a class="chip chip-link" href="../#kw=scientific%20computing">scientific computing</a> <a class="chip chip-link" href="../#kw=programming%20benchmark">programming benchmark</a> </div>
<h3>Citation</h3>

- Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: a research coding benchmark curated by scientists. 2024. URL: https://arxiv.org/abs/2407.13168, arXiv:2407.13168.

<pre><code class="language-bibtex">@misc{tian2024scicoderesearchcodingbenchmark,
  archiveprefix = {arXiv},
  author        = {Minyang Tian and Luyu Gao and Shizhuo Dylan Zhang and Xinan Chen and Cunwei Fan and Xuefei Guo and Roland Haas and Pan Ji and Kittithat Krongchon and Yao Li and Shengyan Liu and Di Luo and Yutao Ma and Hao Tong and Kha Trinh and Chenyu Tian and Zihan Wang and Bohao Wu and Yanyu Xiong and Shengzhu Yin and Minhui Zhu and Kilian Lieret and Yanxin Lu and Genglin Liu and Yufeng Du and Tianhua Tao and Ofir Press and Jamie Callan and Eliu Huerta and Hao Peng},
  eprint        = {2407.13168},
  primaryclass  = {cs.AI},
  title         = {SciCode: A Research Coding Benchmark Curated by Scientists},
  url           = {https://arxiv.org/abs/2407.13168},
  year          = {2024}
}</code></pre>
<h3>Ratings</h3>
<div class="ratings-grid">
  <div class="ratings-head ratings-cell"><span>Category</span><span>Rating</span></div>
  <div class="rating-item">  <div class="rating-cat">Software</div>  <div class="rating-badge">5.00</div>  <div class="rating-bar"><span style="width:100%"></span></div>  <div class="rating-reason">Code to run exists on github repo
</div></div><div class="rating-item">  <div class="rating-cat">Specification</div>  <div class="rating-badge">4.50</div>  <div class="rating-bar"><span style="width:90%"></span></div>  <div class="rating-reason">Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.
</div></div><div class="rating-item">  <div class="rating-cat">Dataset</div>  <div class="rating-badge">0.00</div>  <div class="rating-bar"><span style="width:0%"></span></div>  <div class="rating-reason">Paper and website had no link to any dataset. It may still exist somewhere
</div></div><div class="rating-item">  <div class="rating-cat">Metrics</div>  <div class="rating-badge">2.00</div>  <div class="rating-bar"><span style="width:40%"></span></div>  <div class="rating-reason">Metrics stated, but method of grading is not specified
</div></div><div class="rating-item">  <div class="rating-cat">Reference Solution</div>  <div class="rating-badge">1.00</div>  <div class="rating-bar"><span style="width:20%"></span></div>  <div class="rating-reason">Models presented with scores, but none are open or list constraints
</div></div><div class="rating-item">  <div class="rating-cat">Documentation</div>  <div class="rating-badge">4.00</div>  <div class="rating-bar"><span style="width:80%"></span></div>  <div class="rating-reason">Paper containing all needed info except for evlauation criteria
</div></div>
</div>
<div class="avg-rating">  <strong>Average rating:</strong> <span class="badge badge--bad badge--sm">2.75/5</span></div><h3>Radar plot</h3>

<div class="radar-wrap"><img class="radar-img" alt="SciCode radar" src="../../../tex/images/scicode_radar.png" /></div>

<p><strong>Edit:</strong> <a href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit this entry</a></p>
