- date: "2020-09-07"
  expired: null
  valid: 'yes'
  name: MMLU (Massive Multitask Language Understanding)
  url: https://paperswithcode.com/dataset/mmlu
  domain: Multidomain
  focus: Academic knowledge and reasoning across 57 subjects
  keywords:
    - multitask
    - multiple-choice
    - zero-shot
    - few-shot
    - knowledge probing
  description: |
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  task_types:
    - Multiple choice
  ai_capability_measured: General reasoning, subject-matter understanding
  metrics:
    - Accuracy
  models:
    - GPT-4o
    - Gemini 1.5 Pro
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - |
      @article{hendrycks2021measuring,
        title={Measuring Massive Multitask Language Understanding},
        author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others},
        journal={arXiv preprint arXiv:2009.03300},
        year={2021},
        url={https://arxiv.org/abs/2009.03300}
      }
  ratings:
    - problem_spec_rating: 6
    - problem_spec_reason: TO DO
    - dataset_rating: 6.5
    - dataset_reason: TO DO
    - metrics_rating: 10
    - metrics_reason: TO DO
    - reference_solution_rating: 0
    - reference_solution_reason: TO DO
    - documentation_rating: 6
    - documentation_reason: TO DO

- date: "2023-11-20"
  expired: null
  valid: 'yes'
  name: GPQA Diamond
  url: https://arxiv.org/abs/2311.12022
  domain: Science
  focus: Graduate-level scientific reasoning
  keywords:
    - Google-proof
    - graduate-level
    - science QA
    - chemistry
    - physics
  description: |
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is “Google-proof”—experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  task_types:
    - Multiple choice
    - Multi-step QA
  ai_capability_measured: Scientific reasoning, deep knowledge
  metrics:
    - Accuracy
  models:
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - |
      @misc{rein2023gpqagraduatelevelgoogleproofqa,
        title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
        author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others},
        year={2023},
        url={https://arxiv.org/abs/2311.12022}
      }
  ratings:
    - problem_spec_rating: 6.5
    - problem_spec_reason: Good description of how the problems are received, but little specification on how the models are tested
    - dataset_rating: 8.5
    - dataset_reason: Easily able to access dataset. No labels or train/test/valid split
    - metrics_rating: 10
    - metrics_reason: Each question has a correct answer
    - reference_solution_rating: 7.5
    - reference_solution_reason: Common models such as GPT-3.5 were compared. Reproducibility of results unknown
    - documentation_rating: 1
    - documentation_reason: No reference solution, platform for reproduction, or procedure for replication

- date: "2018-03-14"
  expired: null
  valid: 'yes'
  name: ARC-Challenge (Advanced Reasoning Challenge)
  url: https://allenai.org/data/arc
  domain: Science
  focus: Grade-school science with reasoning emphasis
  keywords:
    - grade-school
    - science QA
    - challenge set
    - reasoning
  description: |
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense and scientific reasoning
  metrics:
    - Accuracy
  models:
    - GPT-4
    - Claude
  notes: Good
  cite:
    - |
      @inproceedings{clark2018think,
        title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
        author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others},
        booktitle={EMNLP 2018},
        pages={237–248},
        year={2018},
        url={https://allenai.org/data/arc}
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason: Exact format of data, questions, and answers are specified. No HW constraints
    - dataset_rating: 10
    - dataset_reason: Data accessible, offers instructions on how to download the data via CLI tools
    - metrics_rating: 10
    - metrics_reason: (by default) All questions in the dataset are multiple choice, all have a correct answer
    - reference_solution_rating: 4.5
    - reference_solution_reason: There are over 300 models listed, but very few, if any, show performance on the dataset
    - documentation_rating: 4
    - documentation_reason: There are easy ways to download the dataset. Documentation quantity and clarity depends on authors of tested models

- date: "2025-01-24"
  expired: null
  valid: 'yes'
  name: Humanity's Last Exam
  url: https://arxiv.org/abs/2501.14249
  domain: Multidomain
  focus: Broad cross-domain academic reasoning
  keywords:
    - cross-domain
    - academic exam
    - multiple-choice
    - multidisciplinary
  description: |
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  task_types:
    - Multiple choice
  ai_capability_measured: Cross-domain academic reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{phan2025humanitys,
        title={Humanity's Last Exam},
        author={Phan, Long and Gatti, Alice and Han, Ziwen and others},
        year={2025},
        url={https://arxiv.org/abs/2501.14249}
      }
  ratings:
    - problem_spec_rating: 8.5
    - problem_spec_reason: Format of inputs (natural language) and outputs (multiple choice or natural language) specified. No HW constraints specified
    - dataset_rating: 6
    - dataset_reason: Data accessible through Hugging Face, but requires giving contact information to access
    - metrics_rating: 10
    - metrics_reason: (by default) All questions in the dataset are multiple choice, all have a correct answer
    - reference_solution_rating: 6
    - reference_solution_reason: Performance for cutting-edge models listed, but does not specify exact version of the models or how to reproduce the result
    - documentation_rating: 0.5
    - documentation_reason: No specified way to reproduce the reference solution
  

- date: "2024-11-07"
  expired: null
  valid: 'yes'
  name: FrontierMath
  url: https://arxiv.org/abs/2411.04872
  domain: Mathematics
  focus: Challenging advanced mathematical reasoning
  keywords:
    - symbolic reasoning
    - number theory
    - algebraic geometry
    - category theory
  description: |
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’ 
    ability to solve problems requiring deep abstract reasoning.
  task_types:
    - Problem solving
  ai_capability_measured: Symbolic and abstract mathematical reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{glazer2024frontiermath,
        title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
        author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others},
        year={2024},
        url={https://arxiv.org/abs/2411.04872}
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason: Well-specified process for asking questions and receiving answers. No HW constraints
    - dataset_rating: 0.5
    - dataset_reason: Paper and website had no link to any dataset. It may still exist somewhere
    - metrics_rating: 10
    - metrics_reason: (by default) All questions in the dataset are multiple choice, all have a correct answer
    - reference_solution_rating: 9
    - reference_solution_reason: Displays result of leading models on the benchmark
    - documentation_rating: 0.5
    - documentation_reason: No specified way to reproduce the reference solution

- date: "2024-07-18"
  expired: null
  valid: 'yes'
  name: SciCode
  url: https://arxiv.org/abs/2407.13168
  domain: Scientific Programming
  focus: Scientific code generation and problem solving
  keywords:
    - code synthesis
    - scientific computing
    - programming benchmark
  description: |
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  task_types:
    - Coding
  ai_capability_measured: Program synthesis, scientific computing
  metrics:
    - Solve rate (%)
  models:
    - Claude3.5-Sonnet
  notes: Good
  cite:
    - |
      @misc{tian2024scicode,
       title={SciCode: A Research Coding Benchmark Curated by Scientists},
       author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others},
       year={2024},
       url={https://arxiv.org/abs/2407.13168}
      }
  ratings:
    - problem_spec_rating: 6
    - problem_spec_reason: Expected outputs and broad types of inputs stated. Few details on output grading. No HW constraints.
    - dataset_rating: 0.5
    - dataset_reason: Paper and website had no link to any dataset. It may still exist somewhere
    - metrics_rating: 4
    - metrics_reason: Metrics stated, but not specified in detail
    - reference_solution_rating: 9
    - reference_solution_reason: Models presented with scores
    - documentation_rating: 0.5
    - documentation_reason: No specified way to reproduce the reference solution

- date: "2025-03-13"
  expired: null
  valid: 'yes'
  name: AIME (American Invitational Mathematics Examination)
  url: https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions
  domain: Mathematics
  focus: Pre-college advanced problem solving
  keywords:
    - algebra
    - combinatorics
    - number theory
    - geometry
  description: |
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  task_types:
    - Problem solving
  ai_capability_measured: Mathematical problem-solving and reasoning
  metrics:
    - Accuracy
  models: []
  notes:
  cite:
    - |
      @misc{www-aime,
        author = {TBD},
        title = {AIME},
        url = {https://www.vals.ai/benchmarks/aime-2025-03-13},
        month = mar,
        year = 2025,
        note = {[Online accessed 2025-06-24]}
      }
  ratings:
    - problem_spec_rating: 3
    - problem_spec_reason: Obvious what the problems are, but not specified how to administer them to AI models. No HW constraints
    - dataset_rating: 9
    - dataset_reason: Easily accessible data with problems and solutions
    - metrics_rating: 10
    - metrics_reason: (by default) Answer is correct or it's not
    - reference_solution_rating: 0
    - reference_solution_reason:  Not given. Human performance stats exist, but no mentions of AI performance
    - documentation_rating: 0
    - documentation_reason: Not given

- date: "2025-02-15"
  expired: null
  valid: 'yes'
  name: MATH-500
  url: https://huggingface.co/datasets/HuggingFaceH4/MATH-500
  domain: Mathematics
  focus: Math reasoning generalization
  keywords:
    - calculus
    - algebra
    - number theory
    - geometry
  description: |
    MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
    high-school to advanced levels, designed to evaluate LLMs’ mathematical reasoning and 
    generalization.
  task_types:
    - Problem solving
  ai_capability_measured: Math reasoning and generalization
  metrics:
    - Accuracy
  models: []
  notes: Dataset hosted on Hugging Face
  cite:
    - |
      @misc{huggingface2025math500,
        title={MATH-500},
        author={HuggingFaceH4},
        year={2025},
        url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}
      }
  ratings:
    - problem_spec_rating: 3
    - problem_spec_reason: Known what the problems are, but method of presentation and evaluation is not stated. No HW constraints
    - dataset_rating: 9.9
    - dataset_reason: Problems and solutions are easily downloaded. Could not find a way to download the data
    - metrics_rating: 2
    - metrics_reason:  Problem spec states that all of the AI’s reasoning steps are subject to grading, but no specified way to evaluate the steps
    - reference_solution_rating: 0
    - reference_solution_reason:  Not given
    - documentation_rating: 0.5
    - documentation_reason: Not given. Implicit instructions to download dataset.

- date: "2024-04-02"
  expired: null
  valid: 'yes'
  name: CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)
  url: https://arxiv.org/abs/2404.02029
  domain: Multidomain Science
  focus: Long-context scientific reasoning
  keywords:
    - long-context
    - information extraction
    - multimodal
  description: |
    CURIE is a benchmark of 580 problems across six scientific disciplines—materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics—
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  task_types:
    - Information extraction
    - Reasoning
    - Concept tracking
    - Aggregation
    - Algebraic manipulation
    - Multimodal comprehension
  ai_capability_measured: Long-context understanding and scientific reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{curie2024,
        title={Scientific Reasoning Benchmarks from the CURIE Dataset},
        author={TODO: Add authors},
        year={2024},
        url={https://arxiv.org/abs/2404.02029}
      }
  ratings:
    - problem_spec_rating: 0
    - problem_spec_reason: This is an AI model, not a benchmark
    - dataset_rating: 0
    - dataset_reason: This is an AI model, not a benchmark
    - metrics_rating: 0
    - metrics_reason: This is an AI model, not a benchmark
    - reference_solution_rating: 0
    - reference_solution_reason: This is an AI model, not a benchmark
    - documentation_rating: 0
    - documentation_reason: This is an AI model, not a benchmark

- date: "2023-01-26"
  expired: null
  valid: 'yes'
  name: FEABench (Finite Element Analysis Benchmark)
  url: https://github.com/alleninstitute/feabench
  domain: Computational Engineering
  focus: FEA simulation accuracy and performance
  keywords:
    - finite element
    - simulation
    - PDE
  description: |
    FEABench is a suite evaluating finite element analysis tools on standardized 
    PDE-based simulation tasks with complex geometries and boundary conditions, 
    measuring both accuracy and runtime performance.
  task_types:
    - Simulation
    - Performance evaluation
  ai_capability_measured: Numerical simulation accuracy and efficiency
  metrics:
    - Solve time
    - Error norm
  models:
    - FEniCS
    - deal.II
  notes: Good
  cite:
    - |
      @misc{allen2023feabench,
        title={FEABench: A Finite Element Analysis Benchmark},
        author={Allen Institute},
        year={2023},
        url={https://github.com/alleninstitute/feabench}
      }
  ratings:
    - problem_spec_rating: 0
    - problem_spec_reason: Using the link results in a 404 Not Found error
    - dataset_rating: 0
    - dataset_reason: Using the link results in a 404 Not Found error
    - metrics_rating: 0
    - metrics_reason: Using the link results in a 404 Not Found error
    - reference_solution_rating: 0
    - reference_solution_reason: Using the link results in a 404 Not Found error
    - documentation_rating: 0
    - documentation_reason: Using the link results in a 404 Not Found error

- date: "2024-07-12"
  expired: null
  valid: 'yes'
  name: SPIQA (Scientific Paper Image Question Answering)
  url: https://arxiv.org/abs/2407.09413
  domain: Computer Science
  focus: Multimodal QA on scientific figures
  keywords:
    - multimodal QA
    - figure understanding
    - table comprehension
    - chain-of-thought
  description: |
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  task_types:
    - Question answering
    - Multimodal QA
    - Chain-of-Thought evaluation
  ai_capability_measured: Visual-textual reasoning in scientific contexts
  metrics:
    - Accuracy
    - F1 score
  models:
    - Chain-of-Thought models
    - Multimodal QA systems
  notes: Good
  cite:
    - |
      @misc{zhong2024spiqa,
        title={SPIQA: Scientific Paper Image Question Answering},
        author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
        year={2024},
        url={https://arxiv.org/abs/2407.09413}
      }
  ratings:
    - problem_spec_rating: 10
    - problem_spec_reason: Task administration clearly defined; prompt instructions explicitly given, no ambiguity in format or scope.
    - dataset_rating: 9
    - dataset_reason: Dataset is available (via paper/appendix), includes train/test/valid split. FAIR-compliant with minor gaps in versioning or access standardization.
    - metrics_rating: 9
    - metrics_reason: Uses quantitative metrics (Accuracy, F1) aligned with the task. Well-suited for benchmarking multimodal reasoning.
    - reference_solution_rating: 5
    - reference_solution_reason: Multiple model results (e.g., GPT-4V, Gemini) reported; baselines exist, but full runnable code not confirmed for all.
    - documentation_rating: 2
    - documentation_reason: Dataset and benchmark description provided; code/software mentioned; however, full step-by-step setup or containerized environment not stated.

- date: "2020-09-28"
  expired: null
  valid: 'yes'
  name: MedQA
  url: https://arxiv.org/abs/2009.13081
  domain: Medical Question Answering
  focus: Medical board exam QA
  keywords:
    - USMLE
    - diagnostic QA
    - medical knowledge
    - multilingual
  description: |
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  task_types:
    - Multiple choice
  ai_capability_measured: Medical diagnosis and knowledge retrieval
  metrics:
    - Accuracy
  models:
    - Neural reader
    - Retrieval-based QA systems
  notes: Multilingual (English, Simplified and Traditional Chinese)
  cite:
    - |
      @misc{jin2020what,
        title={What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams},
        author={Jin, Di and Li, Ying and Zhang, Yichong and others},
        year={2020},
        url={https://arxiv.org/abs/2009.13081}
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason: Task is clearly defined as multiple-choice QA for medical board exams; input and output formats are explicit; task scope is rigorous and structured. System constraints not specified.
    - dataset_rating: 8
    - dataset_reason: Dataset is publicly available (GitHub, paper, Hugging Face), well-structured. However, versioning and metadata could be more standardized to fully meet FAIR criteria.
    - metrics_rating: 9
    - metrics_reason: Uses clear, quantitative metric (accuracy), standard for multiple-choice benchmarks; easily comparable across models.
    - reference_solution_rating: 7
    - reference_solution_reason: Model results reported (GPT-4, Med-PaLM, etc.); implementations discussed in papers, but runnable baselines not fully packaged or documented.
    - documentation_rating: 6
    - documentation_reason: Dataset and paper are accessible; instructions on how to use the source code available, but environment setup or full reproducibility workflow is not packaged.

- date: "2025-05-13"
  expired: null
  valid: 'yes'
  name: BaisBench (Biological AI Scientist Benchmark)
  url: https://arxiv.org/abs/2505.08341
  domain: Computational Biology
  focus: Omics-driven AI research tasks
  keywords:
    - single-cell annotation
    - biological QA
    - autonomous discovery
  description: |
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  task_types:
    - Cell type annotation
    - Multiple choice
  ai_capability_measured: Autonomous biological research capabilities
  metrics:
    - Annotation accuracy
    - QA accuracy
  models:
    - LLM-based AI scientist agents
  notes: Underperforms human experts; aims to advance AI-driven discovery
  cite:
    - |
      @misc{luo2025benchmarkingaiscientistsomics,
        title={Benchmarking AI scientists in omics data-driven biological research},
        author={Luo, Erpai and Jia, Jinmeng and Xiong, Yifan and others},
        year={2025},
        url={https://arxiv.org/abs/2505.08341}
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason:  Task clearly defined—cell type annotation and biological QA; input/output formats are well-described; system constraints are not deeply quantified.
    - dataset_rating: 8
    - dataset_reason: Uses public scRNA-seq datasets linked in paper appendix; structured and accessible, though versioning and full metadata not formalized per FAIR standards.
    - metrics_rating: 9
    - metrics_reason: Includes precise and interpretable metrics (annotation and QA accuracy); directly aligned with task outputs and benchmarking goals.
    - reference_solution_rating: 7
    - reference_solution_reason: Model evaluations and LLM agent results discussed; however, no fully packaged, runnable baseline with training/eval pipeline confirmed yet.
    - documentation_rating: 8
    - documentation_reason: Dataset and paper accessible; IPYNB files for setup are available on the github repo; further instructions are minimal.


- date: "2023-01-26"
  expired: null
  valid: 'yes'
  name: MOLGEN
  url: https://github.com/zjunlp/MolGen
  domain: Computational Chemistry
  focus: Molecular generation and optimization
  keywords:
    - SELFIES
    - GAN
    - property optimization
  description: |
    MolGen is a pre-trained molecular language model that generates chemically valid
    molecules using SELFIES and reinforcement learning, guided by chemical feedback 
    to optimize properties such as logP, QED, and docking score.
  task_types:
    - Distribution learning
    - Goal-oriented generation
  ai_capability_measured: Generation of valid and optimized molecular structures
  metrics:
    - Validity%
    - Novelty%
    - QED
    - Docking score
  models:
    - MolGen
  notes: This is a model, not a benchmark
  cite:
    - |
      @misc{fang2023domain,
        title={Domain-Agnostic Molecular Generation with Chemical Feedback},
        author={Fang, Yin and Zhang, Ningyu and Chen, Zhuo and others},
        year={2023},
        url={https://arxiv.org/abs/2301.11259}
      }
  ratings:
    - problem_spec_rating: 8
    - problem_spec_reason: The molecular generation task is well-defined, with input/output via SELFIES and chemical properties
    - dataset_rating: 7
    - dataset_reason: Uses standard datasets (ZINC, MOSES, QM9); accessible and widely used, but FAIR metadata, versioning, and splits are not detailed within this specific repo.
    - metrics_rating: 9
    - metrics_reason: Metrics like Validity%, Novelty%, QED, and Docking Score are quantitative, supporting clear model evaluation.
    - reference_solution_rating: 6
    - reference_solution_reason: Model is released and functional; some training/evaluation code exists, but it's not framed as a reusable baseline in a benchmark context.
    - documentation_rating: 6.5
    - documentation_reason: Code is available and usable; instructions exist, though setup may require domain knowledge or adaptation for different datasets/environments.

- date: "2020-05-02"
  expired: null
  valid: 'yes'
  name: Open Graph Benchmark (OGB) - Biology
  url: https://ogb.stanford.edu/docs/home/
  domain: Graph ML
  focus: Biological graph property prediction
  keywords:
    - node prediction
    - link prediction
    - graph classification
  description: |
    OGB-Biology is a suite of large-scale biological network datasets (protein-protein
    interaction, drug-target, etc.) with standardized splits and evaluation protocols 
    for node, link, and graph property prediction tasks.
  task_types:
    - Node property prediction
    - Link property prediction
    - Graph property prediction
  ai_capability_measured: Scalability and generalization in graph ML for biology
  metrics:
    - Accuracy
    - ROC-AUC
  models:
    - GCN
    - GraphSAGE
    - GAT
  notes: Community-driven updates
  cite:
    - |
      @misc{hu2020ogb,
        title={Open Graph Benchmark: Datasets for Machine Learning on Graphs},
        author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and others},
        year={2020},
        url={https://arxiv.org/abs/2005.00687}
      }
  ratings:
    - problem_spec_rating: 10
    - problem_spec_reason: Tasks (node/link/graph property prediction) are clearly specified with input/output formats and standardized protocols; constraints (e.g., splits) are well-defined.
    - dataset_rating: 10
    - dataset_reason: Fully FAIR- datasets are versioned, split, and accessible via a standardized API; extensive metadata and documentation are included.
    - metrics_rating: 10
    - metrics_reason: Reproducible, quantitative metrics (e.g., ROC-AUC, accuracy) that are tightly aligned with the tasks.
    - reference_solution_rating: 7
    - reference_solution_reason: Multiple baselines implemented and documented (GCN, GAT, GraphSAGE), though most are provided by 3rd parties.
    - documentation_rating: 10
    - documentation_reason: Full codebase available via GitHub, with documented installation and usage instructions.

- date: "2011-10-01"
  expired: null
  valid: 'yes'
  name: Materials Project
  url: https://materialsproject.org/
  domain: Materials Science
  focus: DFT-based property prediction
  keywords:
    - DFT
    - materials genome
    - high-throughput
  description: |
    The Materials Project provides an open-access database of computed properties for
    inorganic materials via high-throughput density functional theory (DFT), accelerating 
    materials discovery.
  task_types:
    - Property prediction
  ai_capability_measured: Prediction of inorganic material properties
  metrics:
    - MAE
    - R²
  models:
    - Automatminer
    - Crystal Graph Neural Networks
  notes: Core component of the Materials Genome Initiative
  cite:
    - |
      @article{jain2013materials,
        title={The Materials Project: A materials genome approach},
        author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},
        journal={APL Materials},
        volume    = {1},
        number    = {1},
        year={2013},
        doi       = {10.1063/1.4812323},
        url={https://materialsproject.org/}
      }
  ratings:
    - problem_spec_rating: 8
    - problem_spec_reason: The platform offers a wide range of material property prediction tasks, but task framing and I/O formats vary by API use and are not always standardized across use cases.
    - dataset_rating: 8
    - dataset_reason: Data is versioned, accessible through both UI and API, with rich metadata and citations; widely reused. API key required to access data.
    - metrics_rating: 10
    - metrics_reason: Uses numerical metrics like MAE and R²
    - reference_solution_rating: 6
    - reference_solution_reason: Numerous models (e.g., Automatminer, CGCNN) trained on the database, but no single canonical baseline is tightly integrated into the platform.
    - documentation_rating: 7
    - documentation_reason: Extensive API, code repositories, and user guides exist, but end-to-end benchmarking workflows require additional setup by users. 'Documentation' link did not work.

- date: "2020-10-20"
  expired: null
  valid: 'yes'
  name: OCP (Open Catalyst Project)
  url: https://opencatalystproject.org/
  domain: Chemistry; Materials Science
  focus: Catalyst adsorption energy prediction
  keywords:
    - DFT relaxations
    - adsorption energy
    - graph neural networks
  description: |
    The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate 
    relaxation datasets, challenging ML models to predict energies and forces for 
    renewable energy applications.
  task_types:
    - Energy prediction
    - Force prediction
  ai_capability_measured: Prediction of adsorption energies and forces
  metrics:
    - MAE (energy)
    - MAE (force)
  models:
    - CGCNN
    - SchNet
    - DimeNet++
    - GemNet-OC
  notes: Public leaderboards; active community development
  cite:
    - |
      @article{chanussot2021oc20,
        title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},
        author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
        journal   = {ACS Catalysis},
        volume    = {11},
        number    = {10},
        pages     = {6059--6072},
        year      = {2021},
        doi       = {10.1021/acscatal.0c04525},
        url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}
      }
    - |
      @article{tran2023oc22,
        title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
        author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
        journal   = {ACS Catalysis},
        volume    = {13},
        number    = {5},
        pages     = {3066--3084},
        year      = {2023},
        doi       = {10.1021/acscatal.2c05426},
        url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}
      }
    - |
      @article{doi:10.1021/acscatal.0c04525,
        author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
      title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
        journal = {ACS Catalysis},
        volume = {11},
        number = {10},
        pages = {6059-6072},
        year = {2021},
        doi = {10.1021/acscatal.0c04525},
        URL = {https://doi.org/10.1021/acscatal.0c04525},eprint = {https://doi.org/10.1021/acscatal.0c04525}}"
    - |
      @article{tran2023b,
        title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
        volume={13},
        ISSN={2155-5435},
        url={http://dx.doi.org/10.1021/acscatal.2c05426},
        DOI={10.1021/acscatal.2c05426},
        number={5},
        journal={ACS Catalysis},
        publisher={American Chemical Society (ACS)},
        author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
        year={2023},
        month=feb, pages={3066-3084} 
      }
  ratings:
    - problem_spec_rating: 10
    - problem_spec_reason: Tasks (energy and force prediction) are clearly defined with explicit I/O specifications, constraints, and physical relevance for renewable energy.
    - dataset_rating: 9.5
    - dataset_reason: Fully FAIR- OC20, per-adsorbate trajectories, and OC22 are versioned; datasets come with standardized splits, metadata, and are downloadable.
    - metrics_rating: 10
    - metrics_reason: MAE (energy and force) are standard and reproducible.
    - reference_solution_rating: 9
    - reference_solution_reason: Multiple baselines (GemNet-OC, DimeNet++, etc.) implemented and evaluated; highly cited with documented performance.
    - documentation_rating: 9
    - documentation_reason: Code, data loaders, usage instructions, and leaderboard available; minor setup effort may still be required for full reproduction.

- date: "2023-06-20"
  expired: null
  valid: 'yes'
  name: JARVIS-Leaderboard
  url: https://arxiv.org/abs/2306.11688
  domain: Materials Science; Benchmarking
  focus: Comparative evaluation of materials design methods
  keywords:
    - leaderboards
    - materials methods
    - simulation
  description: |
    JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
    structure, force-fields, quantum computing, and experimental methods across hundreds
    of materials science tasks.
  task_types:
    - Method benchmarking
    - Leaderboard ranking
  ai_capability_measured: Performance comparison across diverse materials design methods
  metrics:
    - MAE
    - RMSE
    - Accuracy
  models: []
  notes: 1,281 contributions across 274 benchmarks
  cite:
    - |
      @article{choudhary2024jarvis,
        title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},
      author = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},
        journal = {npj Computational Materials},
        volume = {10},
        number = {1},
        pages = {93},
        year = {2024},
        doi = {10.1038/s41524-024-01259-w},
        url = {https://doi.org/10.1038/s41524-024-01259-w}
      }
  ratings:
    - problem_spec_rating: 6
    - problem_spec_reason: Tasks are clearly defined; heterogeneity in benchmarks slightly reduces uniformity; I/O format is not specified
    - dataset_rating: 9
    - dataset_reason: Data is versioned, public, and adheres to FAIR principles across the NIST-hosted infrastructure; however, metadata completeness varies slightly across benchmarks.
    - metrics_rating: 4
    - metrics_reason: Overall goal is stated, but the exact metric evaluated is not listed
    - reference_solution_rating: 5
    - reference_solution_reason: Many baselines across tasks (CGCNN, ALIGNN, M3GNet, etc.); documentation is good, but baselines may be hard to find or not available for every individual task.
    - documentation_rating: 8.5
    - documentation_reason: JARVIS-Tools and leaderboard APIs are well-documented and actively maintained; minimal setup burden, though some task-specific workflows may require additional guidance.

- date: "2022-02-22"
  expired: null
  valid: 'yes'
  name: Quantum Computing Benchmarks (QML)
  url:
    - https://github.com/XanaduAI/qml-benchmarks
    - https://pennylane.ai/datasets/collection/qml-benchmarks
  domain: Quantum Computing
  focus: Quantum algorithm performance evaluation
  keywords:
    - quantum circuits
    - state preparation
    - error correction
  description: |
    A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
    preparation, circuit optimization, and error correction across multiple platforms.
  task_types:
    - Circuit benchmarking
    - State classification
  ai_capability_measured: Quantum algorithm performance and fidelity
  metrics:
    - Fidelity
    - Success probability
  models:
    - IBM Q
    - IonQ
    - AQT@LBNL
  notes: Hardware-agnostic, application-level metrics. The citation may not be correct.
  cite:
    - |
      @inproceedings{kiwit2023,
        title={Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK},
        url={http://dx.doi.org/10.1109/QCE57702.2023.00061},
        DOI={10.1109/qce57702.2023.00061},
        booktitle={2023 IEEE International Conference on Quantum Computing and Engineering (QCE)},
        publisher={IEEE},
        author={Kiwit, Florian J. and Marso, Marwa and Ross, Philipp and Riofrío, Carlos A. and Klepsch, Johannes and Luckow, Andre},
        year={2023},
        month=sep, pages={475-484}
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason: Tasks like fidelity estimation, state preparation, and runtime benchmarking are clearly defined; I/O formats vary slightly across hardware but are consistently framed in PennyLane/Qiskit ecosystems.
    - dataset_rating: 8
    - dataset_reason: Datasets are accessible, structured, and interoperable via PennyLane; however, not all are versioned or richly annotated in conventional ML metadata standards.
    - metrics_rating: 9
    - metrics_reason: Quantitative and well-motivated metrics (e.g., fidelity, success probability) are used, though reproducibility can depend on hardware noise profiles.
    - reference_solution_rating: 5
    - reference_solution_reason: Reference implementations exist and are integrated into tools like PennyLane, but performance varies per backend; not all benchmarks include reproducible reference runs.
    - documentation_rating: 8
    - documentation_reason: Strong integration with PennyLane and QML ecosystem; guides and code provided, but advanced hardware setup may pose reproducibility hurdles for newcomers.

- date: "2024-10-01"
  expired: null
  valid: 'yes'
  name: CFDBench (Fluid Dynamics)
  url: https://arxiv.org/abs/2310.05963
  domain: Fluid Dynamics; Scientific ML
  focus: Neural operator surrogate modeling
  keywords:
    - neural operators
    - CFD
    - FNO
    - DeepONet
  description: |
    CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
    assessing neural operators' ability to generalize to unseen PDE parameters and domains.
  task_types:
    - Surrogate modeling
  ai_capability_measured: Generalization of neural operators for PDEs
  metrics:
    - L2 error
    - MAE
  models:
    - FNO
    - DeepONet
    - U-Net
  notes: 302K frames across 739 cases
  cite:
    - |
      @misc{luo2024cfdbenchlargescalebenchmarkmachine,
        title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},
        author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},
        year={2024},
        url={https://arxiv.org/abs/2310.05963}
      }
  ratings:
    - problem_spec_rating: 10
    - problem_spec_reason: Tasks are clearly framed (PDE regression, surrogate modeling), with explicit details on the four canonical CFD problems, input/output structure, and generalization goals.
    - dataset_rating: 10
    - dataset_reason: Publicly available on Zenodo, versioned, with metadata and splits; covers thousands of simulations with proper documentation.
    - metrics_rating: 9
    - metrics_reason: Quantitative metrics (L2 error, MAE, relative error) are clearly defined and align with regression task objectives.
    - reference_solution_rating: 8
    - reference_solution_reason: Baseline models like FNO and DeepONet are implemented, but full reproduction pipelines or eval scripts may require additional user configuration.
    - documentation_rating: 6
    - documentation_reason: GitHub and Zenodo provide data and code, but setup for evaluating across all 739 cases requires moderate user effort and technical fluency with PyTorch-based frameworks. Reproducibility depends on full implementation details.

- date: null
  expired: null
  valid: 'yes'
  name: SatImgNet
  url: null
  domain: Remote Sensing
  focus: Satellite imagery classification
  keywords:
    - land-use
    - zero-shot
    - multi-task
  description: |
    SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
    imagery classification datasets evaluating zero-shot transfer of vision-language models
    across diverse remote sensing tasks.
  task_types:
    - Image classification
  ai_capability_measured: Zero-shot land-use classification
  metrics:
    - Accuracy
  models: []
  notes: Public leaderboard available
  cite:
    - |
      @misc{roberts2023satinmultitaskmetadatasetclassifying,
        title={SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models}, 
        author={Jonathan Roberts and Kai Han and Samuel Albanie},
        year={2023},
        eprint={2304.11619},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2304.11619}, 
      }
  ratings:
    - problem_spec_rating: 9
    - problem_spec_reason: Tasks (image classification across 27 satellite datasets) are clearly defined with multi-task and zero-shot framing; input/output structure is mostly standard but some task-specific nuances require interpretation.
    - dataset_rating: 9
    - dataset_reason: Hosted on Hugging Face, versioned, FAIR-compliant with rich metadata; covers many well-known remote sensing datasets unified under one metadataset, though documentation depth varies slightly across tasks.
    - metrics_rating: 9
    - metrics_reason: Standard quantitative metrics (Accuracy, Top-1 Accuracy) aligned with classification tasks; consistent across models, with leaderboard results available.
    - reference_solution_rating: 7
    - reference_solution_reason: Baselines like CLIP, BLIP, ALBEF evaluated in the paper; full inference pipelines or training code may need reconstruction from paper or GitHub references.
    - documentation_rating: 7
    - documentation_reason: Good usage guidance via Hugging Face and paper; example scripts and evaluation tools exist, but end-to-end reproducibility may require manual integration of model checkpoints and preprocessing.

- date: "2023-07-19"
  expired: null
  valid: 'yes'
  name: ClimateLearn
  url: https://arxiv.org/abs/2307.01909
  domain: Climate Science; Forecasting
  focus: ML for weather and climate modeling
  keywords:
    - medium-range forecasting
    - ERA5
    - data-driven
  description: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  task_types:
    - Forecasting
  ai_capability_measured: Global weather prediction (3-5 days)
  metrics:
    - RMSE
    - Anomaly correlation
  models:
    - CNN baselines
    - ResNet variants
  notes: Includes physical and ML baselines. Appears to be the same as the SatImgNet entry
  cite:
    - |
      @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
        title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
        author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
        year={2023}, eprint={2307.01909}, 
        archivePrefix={arXiv}, 
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2307.01909}
      }
  ratings:
    - problem_spec_rating: 10
    - problem_spec_reason: Task framing (medium-range climate forecasting), input/output formats, and evaluation windows are clearly defined; benchmark supports both physical and learned models with detailed constraints.
    - dataset_rating: 10
    - dataset_reason: Provides standardized access to ERA5 and other reanalysis datasets, with ML-ready splits, metadata, and Xarray-compatible formats; versioned and fully FAIR-compliant.
    - metrics_rating: 9
    - metrics_reason: ACC and RMSE are standard, quantitative, and appropriate for climate forecasting; well-integrated into the benchmark, though interpretation across domains may vary.
    - reference_solution_rating: 8
    - reference_solution_reason: Multiple baselines (e.g., FourCastNet, ClimaX) are provided and evaluated; implementations are available but may require tuning or GPU-specific configuration.
    - documentation_rating: 8.5
    - documentation_reason: Comprehensive setup via GitHub, including data loaders, training scripts, config files, and reproducibility protocols; minor complexity in large-scale data preprocessing.

- date: "2022-06-09"
  expired: null
  valid: 'yes'
  name: BIG-Bench (Beyond the Imitation Game Benchmark)
  url: https://github.com/google/BIG-bench
  domain: NLP; AI Evaluation
  focus: Diverse reasoning and generalization tasks
  keywords:
    - few-shot
    - multi-task
    - bias analysis
  description: |
    BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning, 
    knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
  task_types:
    - Few-shot evaluation
    - Multi-task evaluation
  ai_capability_measured: Reasoning and generalization across diverse tasks
  metrics:
    - Accuracy
    - Task-specific metrics
  models:
    - GPT-3
    - Dense Transformers
    - Sparse Transformers
  notes: Human baselines included
  cite:
    - |
      @misc{srivastava2023imitationgamequantifyingextrapolating,
        title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
        author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
        year={2023},
        eprint={2206.04615},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2206.04615}, 
      }
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Tasks are diverse and clearly described; input/output formats
      are usually defined but vary widely, and system constraints are not standardized.
  - dataset_rating: 9.0
  - dataset_reason: Public, versioned, and well-documented; FAIR overall, though consistency
      and metadata completeness vary across tasks.
  - metrics_rating: 8.0
  - metrics_reason: Many tasks use standard quantitative metrics (accuracy, BLEU,
      F1), but others involve subjective ratings (e.g., Likert), which reduces cross-task
      comparability.
  - reference_solution_rating: 7.0
  - reference_solution_reason: Human baselines and LLM performance results are included;
      however, runnable reference solutions are limited and setup is not fully turnkey.
  - documentation_rating: 8.0
  - documentation_reason: Excellent GitHub documentation with usage examples, task
      templates, and tooling; task diversity may require manual task-by-task execution
      setup.

- date: "2019-11-20"
  expired: null
  valid: 'yes'
  name: CommonSenseQA
  url: https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge
  domain: NLP; Commonsense
  focus: Commonsense question answering
  keywords:
    - ConceptNet
    - multiple-choice
    - adversarial
  description: |
    CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
    requiring models to apply commonsense knowledge to select the correct answer 
    among five choices.
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense reasoning and knowledge integration
  metrics:
    - Accuracy
  models:
    - BERT-large
    - RoBERTa
    - GPT-3
  notes: Baseline 56%, human 89%
  cite:
    - |
      @misc{talmor2019commonsenseqaquestionansweringchallenge,
        title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
        author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
        year={2019},
        eprint={1811.00937},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/1811.00937}, 
      }
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Task and format (multiple-choice QA with 5 options) are clearly
      defined; grounded in ConceptNet with consistent structure, though no hardware/system
      constraints are specified.
  - dataset_rating: 9.0
  - dataset_reason: Public, versioned, and FAIR-compliant; includes metadata, splits,
      and licensing; well-integrated with HuggingFace and other ML libraries.
  - metrics_rating: 9.0
  - metrics_reason: Accuracy is a simple, reproducible metric aligned with task goals;
      no ambiguity in evaluation.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Several baseline models (e.g., BERT, RoBERTa) are reported
      with scores; implementations exist in public repos, but not bundled as an official
      starter kit.
  - documentation_rating: 7.0
  - documentation_reason: Clear paper, GitHub repo, and integration with HuggingFace
      Datasets; full reproducibility requires manually connecting models to dataset.

- date: "2019-07-24" 
  expired: null
  valid: 'yes'
  name: Winogrande
  url: https://leaderboard.allenai.org/winogrande/submissions/public
  domain: NLP; Commonsense
  focus: Winograd Schema-style pronoun resolution
  keywords:
    - adversarial
    - pronoun resolution
  description: |
    WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
    questions with reduced bias using AFLite, serving as both a benchmark and transfer 
    learning resource.
  task_types:
    - Pronoun resolution
  ai_capability_measured: Robust commonsense reasoning
  metrics:
    - Accuracy
    - AUC
  models:
    - RoBERTa
    - BERT
    - GPT-2
  notes: Human ~94%
  cite:
    - |
      @misc{sakaguchi2019winograndeadversarialwinogradschema,
        title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
        author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
        year={2019},
        eprint={1907.10641},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/1907.10641}, 
      }
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Task (pronoun/coreference resolution) is clearly defined
      in Winograd Schema style, with consistent input/output format; no system constraints
      included.
  - dataset_rating: 9.0
  - dataset_reason: Public, versioned, and FAIR-compliant with AFLite-generated splits
      to reduce annotation artifacts; hosted by AllenAI with good metadata.
  - metrics_rating: 9.0
  - metrics_reason: Accuracy and AUC are quantitative and well-aligned with disambiguation
      goals; standardized across evaluations.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Baseline results for BERT, RoBERTa, GPT-2, etc., are
      published, but official runnable baselines require setup via AllenNLP or other
      frameworks.
  - documentation_rating: 6.0
  - documentation_reason: Dataset page and paper provide sufficient detail; usage
      with HuggingFace is smooth, but full reproducibility for training requires configuration
      effort.