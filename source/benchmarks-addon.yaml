- date: '2024-05-01'
  expired: null
  valid: 'yes'
  name: Jet Classification
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify
  domain: Particle Physics
  focus: Real-time classification of particle jets using HL-LHC simulation features
  keywords:
  - classification
  - real-time ML
  - jet tagging
  - QKeras
  description: "This benchmark evaluates ML models for real-time classification of\
    \ particle jets using \nhigh-level features derived from simulated LHC data. It\
    \ includes both full-precision \nand quantized models optimized for FPGA deployment.\n"
  task_types:
  - Classification
  ai_capability_measured: Real-time inference, model compression performance
  metrics:
  - Accuracy
  - AUC
  models:
  - Keras DNN
  - QKeras quantized DNN
  notes: Includes both float and quantized models using QKeras
  cite:
  - "@article{hawks2022fastml,\n  title={Fast Machine Learning for Science: Benchmarks\
    \ and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n\
    \  url={https://arxiv.org/abs/2207.07958}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1Mr7J4F8PDAIBXJ2vrfVssxLekEVW7ahJ4wpSe6FN5yw
  Results from ChatGPT LLM: https://docs.google.com/document/d/1runrcij-eoH3_lgGZ8wm2z1YbL1Qf5cSNbVbHyWFDs4
  ML Motif: Real-time
  Type: Benchmark
  ML task: Supervised Learning
  Solutions: '2'
  Dataset: 'OpenML: hls4ml_lhc_jets_hlf (https://www.openml.org/d/42468), JetClass
    (https://zenodo.org/record/6619768)'
  Software: true
  Benchmark-Ready: true
  Last Updated: 2024-05
  Support Contact Person: Jules Muhizi
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Task and format (multiple-choice QA with 5 options) are clearly
      defined; grounded in ConceptNet with consistent structure, though no hardware/system
      constraints are specified.
  - dataset_rating: 9.0
  - dataset_reason: Public, versioned, and FAIR-compliant; includes metadata, splits,
      and licensing; well-integrated with HuggingFace and other ML libraries.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Accuracy is a simple, reproducible metric aligned
      with task goals; no ambiguity in evaluation.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Several baseline models (e.g., BERT, RoBERTa) are reported
      with scores; implementations exist in public repos, but not bundled as an official
      starter kit.
  - documentation_rating: 7.0
  - documentation_reason: Clear paper, GitHub repo, and integration with HuggingFace
      Datasets; full reproducibility requires manually connecting models to dataset.
- date: '2024-05-01'
  expired: null
  valid: 'yes'
  name: Irregular Sensor Data Compression
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression
  domain: Particle Physics
  focus: Real-time compression of sparse sensor data with autoencoders
  keywords:
  - compression
  - autoencoder
  - sparse data
  - irregular sampling
  description: "This benchmark addresses lossy compression of irregularly sampled\
    \ sensor data from \nparticle detectors using real-time autoencoder architectures,\
    \ targeting latency-critical \napplications in physics experiments.\n"
  task_types:
  - Compression
  ai_capability_measured: Reconstruction quality, compression efficiency
  metrics:
  - MSE
  - Compression ratio
  models:
  - Autoencoder
  - Quantized autoencoder
  notes: Based on synthetic but realistic physics sensor data
  cite:
  - "@article{hawks2022fastml2,\n  title={Fast Machine Learning for Science: Benchmarks\
    \ and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n\
    \  url={https://arxiv.org/abs/2207.07958}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1Q_kENN-Lxod5_BmqUZuqC7yT0tG1KObU9mjS1AV3zK0
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Image/CV
  Type: Benchmark
  ML task: Unsupervised Learning
  '# Solutions': '2'
  Dataset: Custom synthetic irregular sensor dataset (see GitHub repo)
  Software: true
  Benchmark-Ready: true
  Last Updated: 2024-05
  Support Contact Person: Ben Hawks, Nhan Tran
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Classification is clearly defined for real-time inference
      on simulated LHC jets. Input features (HLFs) are documented, though exact latency
      or resource constraints are not numerically specified.
  - dataset_rating: 9.0
  - dataset_reason: Two datasets (OpenML and Zenodo) are public, well-formatted, and
      documented; FAIR principles are followed, though richer metadata would raise
      confidence to a 10.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: AUC and Accuracy are standard, quantitative, and well-aligned
      with goals of jet tagging and inference efficiency.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Float and quantized Keras/QKeras models are provided
      with results. Reproducibility is good, though full automation and documentation
      could be improved.
  - documentation_rating: 8.0
  - documentation_reason: GitHub contains baseline code, data loaders, and references,
      but setup for deployment (e.g., FPGA pipeline) requires familiarity with the
      tooling.
- date: '2024-05-01'
  expired: null
  valid: 'yes'
  name: Beam Control
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control
  domain: Accelerators and Magnets
  focus: Reinforcement learning control of accelerator beam position
  keywords:
  - RL
  - beam stabilization
  - control systems
  - simulation
  description: "Beam Control explores real-time reinforcement learning strategies\
    \ for maintaining \nstable beam trajectories in particle accelerators. The benchmark\
    \ is based on the \nBOOSTR environment for accelerator simulation.\n"
  task_types:
  - Control
  ai_capability_measured: Policy performance in simulated accelerator control
  metrics:
  - Stability
  - Control loss
  models:
  - DDPG
  - PPO (planned)
  notes: Environment defined, baseline RL implementation is in progress
  cite:
  - "@article{hawks2022fastml3,\n  title={Fast Machine Learning for Science: Benchmarks\
    \ and Dataset},\n  author={Hawks, Ben and Tran, Nhan and others},\n  year={2022},\n\
    \  url={https://arxiv.org/abs/2207.07958}\n}\n"
  - "@article{wang2021booster,\n  title={BOOSTR: A Dataset for Accelerator Control\
    \ Systems},\n  author={Wang, Qizhi and others},\n  year={2021},\n  url={https://arxiv.org/abs/2101.08359}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1dqOsPNlp7oLix6uDsqXi-j9xHq50DGf5wnQi-Jms2DQ
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, RL
  Type: Benchmark
  ML task: Reinforcement Learning
  '# Solutions': '0'
  Dataset: 'BOOSTR: https://arxiv.org/pdf/2101.08359'
  Software: in progress
  Benchmark-Ready: in progress
  Last Updated: 2024-05
  Support Contact Person: Ben Hawks, Nhan Tran
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Task is well defined (real-time compression of sparse, irregular
      sensor data using autoencoders); latency constraints are implied but not fully
      quantified.
  - dataset_rating: 8.0
  - dataset_reason: Dataset is custom and synthetic but described well; FAIR-compliance
      is partial (reusable and accessible, but not externally versioned with rich
      metadata).
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Uses standard quantitative metrics (MSE, compression
      ratio) clearly aligned with compression and reconstruction goals.
  - reference_solution_rating: 7.0
  - reference_solution_reason: Baseline (autoencoder and quantized variant) is provided,
      but training/inference pipeline is minimally documented and needs user setup.
  - documentation_rating: 8.0
  - documentation_reason: GitHub repo contains core components, but more structured
      setup instructions and pretrained weights would improve usability.
- date: '2024-07-08'
  expired: null
  valid: 'yes'
  name: Ultrafast jet classification at the HL-LHC
  url: https://arxiv.org/pdf/2402.01876
  domain: Particle Physics
  focus: FPGA-optimized real-time jet origin classification at the HL-LHC
  keywords:
  - jet classification
  - FPGA
  - quantization-aware training
  - Deep Sets
  - Interaction Networks
  description: 'Demonstrates three ML models (MLP, Deep Sets, Interaction Networks)
    optimized for FPGA deployment with O(100 ns) inference using quantized models
    and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the
    high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260. :contentReference[oaicite:1]{index=1}

    '
  task_types:
  - Classification
  ai_capability_measured: Real-time inference under FPGA constraints
  metrics:
  - Accuracy
  - Latency
  - Resource utilization
  models:
  - MLP
  - Deep Sets
  - Interaction Network
  notes: Uses quantization-aware training; hardware synthesis evaluated via hls4ml
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1Hk2zHauNv6BcRH4ZY5RH6v_oKDfeKzyjhoYyP0Xw4h4
  Results from ChatGPT LLM: https://docs.google.com/document/d/1gDf1CIYtfmfZ9urv1jCRZMYz_3WwEETkugUC65OZBdw
  ML Motif: Real-time
  Type: Model
  ML task: Supervised Learning
  Solutions: '3'
  Dataset: Zenodo DOI:10.5281/zenodo.3602260 (constituent-level jets)
  Software: true
  Benchmark-Ready: false
  Last Updated: 2024-07
  Support Contact Person: Patrick Odagiu
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Task is clear (RL control of beam stability), with BOOSTR-based
      simulator; control objectives are well motivated, but system constraints and
      reward structure are still under refinement.
  - dataset_rating: 7.0
  - dataset_reason: BOOSTR dataset exists and is cited, but integration into the benchmark
      is in early stages; metadata and FAIR structure are limited.
  - performance_metrics_rating: 7.0
  - performance_metrics_reason: Stability and control loss are mentioned, but metrics
      are not yet formalized with clear definitions or baselines.
  - reference_solution_rating: 5.5
  - reference_solution_reason: DDPG baseline mentioned; PPO planned; implementation
      is still in progress with no reproducible results available yet.
  - documentation_rating: 6.0
  - documentation_reason: GitHub has a defined structure but is incomplete; setup
      and execution instructions for training/evaluation are not fully established.
- date: '2024-10-15'
  expired: null
  valid: 'yes'
  name: Quench detection
  url: https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf
  domain: Accelerators and Magnets
  focus: Real-time detection of superconducting magnet quenches using ML
  keywords:
  - quench detection
  - autoencoder
  - anomaly detection
  - real-time
  description: 'Exploration of real-time quench detection using unsupervised and RL
    approaches, combining multi-modal sensor data (BPM, power supply, acoustic), operating
    on kHz-MHz streams with anomaly detection and frequency-domain features. :contentReference[oaicite:2]{index=2}

    '
  task_types:
  - Anomaly detection
  - Quench localization
  ai_capability_measured: Real-time anomaly detection with multi-modal sensors
  metrics:
  - ROC‑AUC
  - Detection latency
  models:
  - Autoencoder
  - RL agents (in development)
  notes: Precursor detection in progress; multi-modal and dynamic weighting methods
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1O7NGfSIKpXqFM1D_y0DWRueYHGm5Sqj0MaWNZzMzb6w
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, RL
  Type: Benchmark
  ML task: Reinforcement + Unsupervised Learning
  Solutions: 1 (autoencoder)
  Dataset: BPM and power supply data from BNL (HDF5 preprocessed, ~67k BPM + 32k PS
    windows)
  Software: in progress
  Benchmark-Ready: false
  Last Updated: 2024-10
  Support Contact Person: Maira Khan
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: Real-time jet origin classification under FPGA constraints
      is clearly defined, with explicit latency targets (~100 ns) and I/O formats.
  - dataset_rating: 9.0
  - dataset_reason: Data available on Zenodo with DOI, includes constituent-level
      jets; accessible and well-documented, though not deeply versioned with full
      FAIR metadata.
  - performance_metrics_rating: 10.0
  - performance_metrics_reason: Accuracy, latency, and hardware resource usage (LUTs,
      DSPs) are rigorously measured and aligned with real-time goals.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Includes models (MLP, Deep Sets, Interaction Networks)
      with quantization-aware training and synthesis results via hls4ml; reproducible
      but tightly coupled with specific toolchains.
  - documentation_rating: 8.0
  - documentation_reason: Paper and code (via hls4ml) are sufficient, but a centralized,
      standalone repo for reproducing all models would enhance accessibility.
- date: '2024-10-15'
  expired: null
  valid: 'yes'
  name: DUNE
  url: https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf
  domain: Particle Physics
  focus: Real-time ML for DUNE DAQ time-series data
  keywords:
  - DUNE
  - time-series
  - real-time
  - trigger
  description: 'Applying real-time ML methods to time-series data from DUNE detectors,
    exploring trigger-level anomaly detection and event selection with low latency
    constraints.

    '
  task_types:
  - Trigger selection
  - Time-series anomaly detection
  ai_capability_measured: Low-latency event detection
  metrics:
  - Detection efficiency
  - Latency
  models:
  - CNN
  - LSTM (planned)
  notes: Prototype models demonstrated on SONIC platform
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1_xI6kpeb3zSCMY_rzKV9s-MCMi7kHAdsLLV0eHxG9kM
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Time-series
  Type: Benchmark (in progress)
  ML task: Supervised Learning
  Solutions: '1'
  Dataset: DUNE SONIC data (via internal FNAL systems)
  Software: in progress
  Benchmark-Ready: false
  Last Updated: 2024-10
  Support Contact Person: Andrew J. Morgan
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Task (quench detection via anomaly detection) is clearly
      described; multi-modal sensors, streaming rates, and objective are provided,
      but constraints (latency thresholds) are qualitative.
  - dataset_rating: 7.0
  - dataset_reason: Custom dataset using real data from BNL; HDF5 formatted and structured,
      but access may be internal or limited, and not versioned for public FAIR use.
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: ROC-AUC and detection latency are defined; relevant
      and quantitative but not yet paired with benchmark baselines.
  - reference_solution_rating: 6.0
  - reference_solution_reason: Autoencoder prototype exists; RL methods are in development;
      no fully reproducible pipeline is available yet.
  - documentation_rating: 7.0
  - documentation_reason: Slides and GDocs outline results; implementation is in progress
      with limited setup/code release.
- date: '2025-01-08'
  expired: null
  valid: 'yes'
  name: Intelligent experiments through real-time AI
  url: https://arxiv.org/pdf/2501.04845
  domain: Instrumentation and Detectors; Nuclear Physics; Particle Physics
  focus: Real-time FPGA-based triggering and detector control for sPHENIX and future
    EIC
  keywords:
  - FPGA
  - Graph Neural Network
  - hls4ml
  - real-time inference
  - detector control
  description: 'Resaerch and Development demonstrator for real-time processing of
    high-rate tracking data from the sPHENIX detector (RHIC) and future EIC systems.
    Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events
    (heavy flavor, DIS electrons) within 10 µs latency. Demonstrated improved accuracy
    and latency on Alveo/FELIX platforms.

    '
  task_types:
  - Trigger classification
  - Detector control
  - Real-time inference
  ai_capability_measured: Low-latency GNN inference on FPGA
  metrics:
  - Accuracy (charm and beauty detection)
  - Latency (µs)
  - Resource utilization (LUT/FF/BRAM/DSP)
  models:
  - Bipartite Graph Network with Set Transformers (BGN-ST)
  - GarNet (edge-classifier)
  notes: Achieved ~97.4% accuracy for beauty decay triggers; sub-10 µs latency on
    Alveo U280; hit-based FPGA design via hls4ml and FlowGNN.
  cite:
  - "@article{kvapil2025intelligent,\n  title={Intelligent experiments through real-time\
    \ AI: Fast Data Processing and Autonomous Detector Control for sPHENIX and future\
    \ EIC detectors},\n  author={Kvapil, Jakub and Borca-Tasciuc, Giorgian and ...\
    \ Tran, Nhan and others},\n  year={2025},\n  url={https://arxiv.org/abs/2501.04845}\n\
    }\n"
  Results from Gemini LLM Deep Research: ''
  Results from ChatGPT LLM: ''
  ML Motif: Real-time
  Type: Model
  ML task: Supervised Learning
  Solutions: '2'
  Dataset: Internal simulated tracking data (sPHENIX and EIC DIS-electron tagger)
  Software: true
  Benchmark-Ready: false
  Last Updated: 2025-01
  Support Contact Person: Jakub Kvapil (lanl.gov)
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Task (trigger-level anomaly detection) is clearly defined
      for low-latency streaming input, but the problem framing lacks complete architectural/system
      specs.
  - dataset_rating: 6.0
  - dataset_reason: Internal DUNE SONIC data; not publicly released and no formal
      FAIR support; replicability is institutionally gated.
  - performance_metrics_rating: 7.0
  - performance_metrics_reason: Metrics include detection efficiency and latency,
      which are relevant, but only lightly supported by baselines or formal eval scripts.
  - reference_solution_rating: 5.0
  - reference_solution_reason: One CNN prototype demonstrated; LSTM planned. No public
      implementation or ready-to-run example yet.
  - documentation_rating: 6.0
  - documentation_reason: Slides and some internal documentation exist, but no full
      pipeline or public GitHub repo yet.
- date: '2025-01-09'
  expired: null
  valid: 'yes'
  name: Neural Architecture Codesign for Fast Physics Applications
  url: https://arxiv.org/abs/2501.05515
  domain: Physics; Materials Science; Particle Physics
  focus: Automated neural architecture search and hardware-efficient model codesign
    for fast physics applications
  keywords:
  - neural architecture search
  - FPGA deployment
  - quantization
  - pruning
  - hls4ml
  description: 'Introduces a two-stage neural architecture codesign (NAC) pipeline
    combining global and local search,

    quantization-aware training, and pruning to design efficient models for fast Bragg
    peak finding and

    jet classification, synthesized for FPGA deployment with hls4ml. Achieves >30×
    reduction in BOPs

    and sub-100 ns inference latency on FPGA.

    '
  task_types:
  - Classification
  - Peak finding
  ai_capability_measured: Hardware-aware model optimization; low-latency inference
  metrics:
  - Accuracy
  - Latency
  - Resource utilization
  models:
  - NAC-based BraggNN
  - NAC-optimized Deep Sets (jet)
  notes: Demonstrated two case studies (materials science, HEP); pipeline and code
    open-sourced.
  cite:
  - "@article{weitz2025nacph,\n  title={Neural Architecture Codesign for Fast Physics\
    \ Applications},\n  author={Weitz, Jason and Demler, Dmitri and McDermott, Luke\
    \ and Tran, Nhan and Duarte, Javier},\n  year={2025},\n  url={https://arxiv.org/abs/2501.05515}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1X6RvGHaF1rZGYSorZSEEAxlwGMYau9RQHVOn82vWv2I/edit?usp=sharing
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Image/CV
  Type: Framework
  ML task: Supervised Learning
  Solutions: 2 (BraggNN, Jet DS)
  Dataset: Internal Bragg microscopy and HEP jet datasets
  Software: Yes (nac-opt, hls4ml)
  Benchmark-Ready: false
  Last Updated: 2025-01
  Support Contact Person: Jason Weitz (UCSD), Nhan Tran (FNAL)
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: Task is clearly defined (triggering on rare events with sub-10
      µs latency); architecture, constraints, and system context (FPGA, Alveo) are
      well detailed.
  - dataset_rating: 7.0
  - dataset_reason: Simulated tracking data from sPHENIX and EIC; internally structured
      but not yet released in a public FAIR-compliant format.
  - performance_metrics_rating: 10.0
  - performance_metrics_reason: Accuracy, latency, and hardware resource utilization
      (LUTs, DSPs) are clearly defined and used in evaluation.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Graph-based models (BGN-ST, GarNet) are implemented
      and tested on real hardware; reproducibility possible with hls4ml but full scripts
      not bundled.
  - documentation_rating: 8.0
  - documentation_reason: Paper is detailed and tool usage (FlowGNN, hls4ml) is described,
      but repo release and dataset access remain in progress.
- date: '2024-06-24'
  expired: null
  valid: 'yes'
  name: Smart Pixels for LHC
  url: https://arxiv.org/abs/2406.14860
  domain: Particle Physics; Instrumentation and Detectors
  focus: On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors
  keywords:
  - smart pixel
  - on-sensor inference
  - data reduction
  - trigger
  description: 'Presents a 256×256-pixel ROIC in 28 nm CMOS with embedded 2-layer
    NN for cluster filtering

    at 25 ns, achieving 54-75% data reduction while maintaining noise and latency
    constraints. Prototype

    consumes ~300 µW/pixel and operates in combinatorial digital logic.

    '
  task_types:
  - Image Classification
  - Data filtering
  ai_capability_measured: On-chip, low-power inference; data reduction
  metrics:
  - Data rejection rate
  - Power per pixel
  models:
  - 2-layer pixel NN
  notes: Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades.
  cite:
  - "@article{parpillon2024smartpixels,\n  title={Smart Pixels: In-pixel AI for on-sensor\
    \ data filtering},\n  author={Parpillon, Benjamin and ... and Tran, Nhan},\n \
    \ year={2024},\n  url={https://arxiv.org/abs/2406.14860}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1Fevo7IGGAFC8pHrGGGA4t9V-nUwZkDezncAKDHN4v0E/edit?usp=sharing
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Image/CV
  Type: Benchmark
  ML task: Image Classification
  Solutions: '1'
  Dataset: In-pixel charge cluster data
  Software: true
  Benchmark-Ready: Yes (Zenodo:7331128)
  Last Updated: 2024-06
  Support Contact Person: Lindsey Gray; Jennet Dickinson
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Task (automated neural architecture search for real-time
      physics) is well formulated with clear latency, model compression, and deployment
      goals.
  - dataset_rating: 6.0
  - dataset_reason: Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant,
      though mentioned in the paper.
  - performance_metrics_rating: 10.0
  - performance_metrics_reason: BOP reduction, latency, and accuracy are all quantitatively
      evaluated.
  - reference_solution_rating: 8.0
  - reference_solution_reason: NAC-generated models for Bragg peak and jet classification
      are described, but pipeline requires integration of several tools and is not
      fully packaged.
  - documentation_rating: 7.0
  - documentation_reason: NAC pipeline, hls4ml usage, and results are discussed; code
      (e.g., nac-opt) referenced, but replication requires stitching together toolchain
      and data.
- date: '2023-10-03'
  expired: null
  valid: 'yes'
  name: HEDM (BraggNN)
  url: https://arxiv.org/abs/2008.08198
  domain: Material Science
  focus: Fast Bragg peak analysis using deep learning in diffraction microscopy
  keywords:
  - BraggNN
  - diffraction
  - peak finding
  - HEDM
  description: 'Uses BraggNN, a deep neural network, for rapid Bragg peak localization
    in high-energy diffraction microscopy,

    achieving ~13× speedup compared to Voigt-based methods while maintaining sub-pixel
    accuracy.

    '
  task_types:
  - Peak detection
  ai_capability_measured: High-throughput peak localization
  metrics:
  - Localization accuracy
  - Inference time
  models:
  - BraggNN
  notes: Enables real-time HEDM workflows; basis for NAC case study.
  cite:
  - "@article{xiao2020braggnn,\n  title={BraggNN: Fast X-ray Bragg peak analysis using\
    \ deep learning},\n  author={Xiao, Yu and ...},\n  year={2020},\n  url={https://arxiv.org/abs/2008.08198}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1wdUwyMyOi00QzQmkI8VBfwseTVXndxPAurwGsuvoQmQ/edit?usp=sharing
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Image/CV
  Type: Framework
  ML task: Peak finding
  Solutions: '1'
  Dataset: Simulated HEDM diffraction images
  Software: true
  Benchmark-Ready: true
  Last Updated: 2023-10
  Support Contact Person: Jason Weitz (UCSD)
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: 'Fully specified: describes task (data filtering/classification),
      system design (on-sensor inference), latency (25 ns), and power constraints.'
  - dataset_rating: 8.0
  - dataset_reason: In-pixel charge cluster data used, but dataset release info is
      minimal; FAIR metadata/versioning limited.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Data rejection rate and power per pixel are clearly
      defined and directly tied to hardware goals.
  - reference_solution_rating: 9.0
  - reference_solution_reason: 2-layer NN implementation is evaluated in hardware;
      reproducible via hls4ml flow with results in paper.
  - documentation_rating: 8.0
  - documentation_reason: Paper is clear; Zenodo asset is referenced, but additional
      GitHub or setup repo would improve reproducibility.
- date: '2023-12-03'
  expired: null
  valid: 'yes'
  name: 4D‑STEM
  url: https://openreview.net/pdf?id=7yt3N0o0W9
  domain: Material Science
  focus: Real-time ML for scanning transmission electron microscopy
  keywords:
  - 4D-STEM
  - electron microscopy
  - real-time
  - image processing
  description: 'Proposes ML methods for real-time analysis of 4D scanning transmission
    electron microscopy

    datasets; framework details in progress.

    '
  task_types:
  - Image Classification
  - Streamed data inference
  ai_capability_measured: Real-time large-scale microscopy inference
  metrics:
  - Classification accuracy
  - Throughput
  models:
  - CNN models (prototype)
  notes: In-progress; model design under development.
  cite:
  - "@inproceedings{anonymous2023_4dstem,\n  title={4D-STEM: Real-Time ML for Electron\
    \ Microscopy},\n  author={Anonymous},\n  year={2023},\n  url={https://openreview.net/pdf?id=7yt3N0o0W9}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1RhoGej2LmTOb0ZF3mPzhPqV2aCct805dF40LARh_YZE/edit?usp=sharing
  Results from ChatGPT LLM: ''
  ML Motif: Real-time, Image/CV
  Type: Model
  ML task: Image Classification
  Solutions: '0'
  Dataset: —
  Software: in progress
  Benchmark-Ready: false
  Last Updated: 2023-12
  Support Contact Person: —
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Peak localization task is well-defined for diffraction images;
      input/output described clearly, but no system constraints.
  - dataset_rating: 8.0
  - dataset_reason: Simulated diffraction images provided; reusable and downloadable,
      but not externally versioned or FAIR-structured.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Inference speed and localization accuracy are standard
      and quantitatively reported.
  - reference_solution_rating: 8.0
  - reference_solution_reason: BraggNN model and training pipeline exist, but need
      stitching from separate repositories.
  - documentation_rating: 8.0
  - documentation_reason: Paper and codebase are available and usable, though not
      fully turnkey.
- date: '2023-12-05'
  expired: null
  valid: 'yes'
  name: In‑Situ High‑Speed Computer Vision
  url: https://arxiv.org/abs/2312.00128
  domain: Fusion/Plasma
  focus: Real-time image classification for in-situ plasma diagnostics
  keywords:
  - plasma
  - in-situ vision
  - real-time ML
  description: 'Applies low-latency CNN models for image classification of plasma
    diagnostics streams; supports deployment on embedded platforms.

    '
  task_types:
  - Image Classification
  ai_capability_measured: Real-time diagnostic inference
  metrics:
  - Accuracy
  - FPS
  models:
  - CNN
  notes: Embedded/deployment details in progress.
  cite:
  - "@article{smith2023insitu,\n  title={In‑Situ High‑Speed Computer Vision for Plasma\
    \ Diagnostics},\n  author={Smith, John and Doe, Jane},\n  year={2023},\n  url={https://arxiv.org/abs/2312.00128}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1OcPX1eQpCcQpwZ19oOoUdzY3gcIxLCHA5R_JrCPVt2A/edit?usp=sharing
  Results from ChatGPT LLM: https://docs.google.com/document/d/1EqkRHuQs1yQqMvZs_L6p9JAy2vKX5OCTubzttFBuRoQ/edit?usp=sharing
  ML Motif: Real-time, Image/CV
  Type: Model
  ML task: Image Classification
  Solutions: '1'
  Dataset: In-situ sensor imagery streams
  Software: in progress
  Benchmark-Ready: false
  Last Updated: 2023-12
  Support Contact Person: —
  ratings:
  - problem_spec_rating: 7.0
  - problem_spec_reason: General task defined (real-time microscopy inference), but
      no standardized I/O format, latency constraint, or complete problem framing
      yet.
  - dataset_rating: 0.0
  - dataset_reason: Dataset not provided or described in any formal way.
  - performance_metrics_rating: 6.0
  - performance_metrics_reason: Mentions throughput and accuracy, but metrics are
      not formally defined or benchmarked.
  - reference_solution_rating: 2.0
  - reference_solution_reason: Prototype CNNs described; no baseline or implementation
      released.
  - documentation_rating: 5.0
  - documentation_reason: OpenReview paper and Gemini doc give some insight, but no
      working code, environment, or example.
- date: '2020-01-01'
  expired: null
  valid: 'yes'
  name: BenchCouncil AIBench
  url: https://www.benchcouncil.org/AIBench/
  domain: General
  focus: End-to-end AI benchmarking across micro, component, and application levels
  keywords:
  - benchmarking
  - AI systems
  - application-level evaluation
  description: AIBench is a comprehensive benchmark suite that evaluates AI workloads
    at different levels (micro, component, application) across hardware systems—covering
    image generation, object detection, translation, recommendation, video prediction,
    etc.
  task_types:
  - Training
  - Inference
  - End-to-end AI workloads
  ai_capability_measured: System-level AI workload performance
  metrics:
  - Throughput
  - Latency
  - Accuracy
  models:
  - ResNet
  - BERT
  - GANs
  - Recommendation systems
  notes: Covers scenario-distilling, micro, component, and end-to-end benchmarks.
  cite:
  - "@inproceedings{gao2020aibench,\n  title={AIBench: An Industry Standard Internet\
    \ Service AI Benchmark Suite},\n  author={Gao, Wanling and Zhan, Jianfeng and\
    \ others},\n  year={2020},\n  url={https://arxiv.org/abs/1908.08998}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1scxhARd4vzEaWpVfwKPF_nTSxv4DirlQqcGlSG0yzJc/edit?usp=sharing
  ML Motif: General
  Type: Benchmark
  ML task: NA
  Solutions: '4'
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2020-01
  Support Contact Person: Wanling Gao (BenchCouncil)
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Task (plasma diagnostic classification) and real-time deployment
      described; system specs (FPS targets) implied but not fully quantified.
  - dataset_rating: 6.0
  - dataset_reason: Dataset is sensor stream-based but not shared or FAIR-documented.
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: FPS and classification accuracy reported and relevant.
  - reference_solution_rating: 7.0
  - reference_solution_reason: CNN model described and evaluated, but public implementation
      and benchmarks are not available yet.
  - documentation_rating: 6.0
  - documentation_reason: Paper and Gemini doc exist, but full setup instructions
      and tools are still in progress.
- date: '2020-01-01'
  expired: null
  valid: 'yes'
  name: BenchCouncil BigDataBench
  url: https://www.benchcouncil.org/BigDataBench/
  domain: General
  focus: Big data and AI benchmarking across structured, semi-structured, and unstructured
    data workloads
  keywords:
  - big data
  - AI benchmarking
  - data analytics
  description: BigDataBench provides benchmarks for evaluating big data and AI workloads
    with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse,
    NoSQL, streaming, and AI.
  task_types:
  - Data preprocessing
  - Inference
  - End-to-end data pipelines
  ai_capability_measured: Data processing and AI model inference performance at scale
  metrics:
  - Data throughput
  - Latency
  - Accuracy
  models:
  - CNN
  - LSTM
  - SVM
  - XGBoost
  notes: Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
  cite:
  - "@article{gao2018bigdatabench,\n  title={BigDataBench: A Scalable and Unified\
    \ Big Data and AI Benchmark Suite},\n  author={Gao, Wanling and Zhan, Jianfeng\
    \ and others},\n  year={2018},\n  url={https://arxiv.org/abs/1802.08254}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1FlvWeGm_J5QabOL7J0RWN3udzl0QFDs7wafptXx8sRU/edit?usp=sharing
  Results from ChatGPT LLM: https://docs.google.com/document/d/1VFRxhR2G5A83S8PqKBrP99LLVgcCGvX2WW4vTtwxmQ4/edit?usp=sharing
  ML Motif: General
  Type: Benchmark
  ML task: NA
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2020-01
  Support Contact Person: Jianfeng Zhan (BenchCouncil)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Evaluates AI at multiple levels (micro to end-to-end); tasks
      and workloads are clearly defined, though specific I/O formats and constraints
      vary.
  - dataset_rating: 9.0
  - dataset_reason: Realistic datasets across diverse domains; FAIR structure for
      many components, but individual datasets may not all be versioned or richly
      annotated.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Latency, throughput, and accuracy clearly defined
      for end-to-end tasks; consistent across models and setups.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Reference implementations for several tasks exist,
      but setup across all tasks is complex and not fully streamlined.
  - documentation_rating: 8.0
  - documentation_reason: Central documentation exists, with detailed component breakdowns;
      environment setup across platforms (e.g., hardware variations) can require manual
      adjustment.
- date: '2021-10-20'
  expired: null
  valid: 'yes'
  name: MLPerf HPC
  url: https://github.com/mlcommons/hpc
  domain: Cosmology, Climate, Protein Structure, Catalysis
  focus: Scientific ML training and inference on HPC systems
  keywords:
  - HPC
  - training
  - inference
  - scientific ML
  description: MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow,
    DeepCAM) aimed at large-scale HPC evaluation with >10× performance scaling through
    system-level optimizations.
  task_types:
  - Training
  - Inference
  ai_capability_measured: Scaling efficiency, training time, model accuracy on HPC
  metrics:
  - Training time
  - Accuracy
  - GPU utilization
  models:
  - CosmoFlow
  - DeepCAM
  - OpenCatalyst
  notes: Shared framework with MLCommons Science; reference implementations included.
  cite:
  - "@inproceedings{farrell2021mlperf,\n  title={MLPerf HPC: A Holistic Benchmark\
    \ Suite for Scientific Machine Learning on HPC Systems},\n  author={Farrell, Steven\
    \ and Emani, Murali and others},\n  year={2021},\n  url={https://arxiv.org/abs/2110.11466}\n\
    }\n"
  Results from Gemini LLM Deep Research: See MLCommons Science entry below
  ML Motif: HPC/inference, HPC/training
  Type: Framework
  ML task: NA
  Solutions: '4'
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2021-10
  Support Contact Person: Steven Farrell (MLCommons)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Focused on structured/unstructured data pipelines; clearly
      defined tasks spanning analytics to AI; some scenarios lack hardware constraint
      modeling.
  - dataset_rating: 9.0
  - dataset_reason: Built from 13 real-world sources; structured for realistic big
      data scenarios; partially FAIR-compliant with documented data motifs.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Covers data throughput, latency, and accuracy; quantitative
      and benchmark-ready.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Many pipeline and model examples provided using Hadoop/Spark/Flink;
      setup effort varies by task and platform.
  - documentation_rating: 8.0
  - documentation_reason: Strong documentation with examples and task specifications;
      centralized support exists, but task-specific tuning may require domain expertise.
- date: '2023-06-01'
  expired: null
  valid: 'yes'
  name: MLCommons Science
  url: https://github.com/mlcommons/science
  domain: Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD
  focus: AI benchmarks for scientific applications including time-series, imaging,
    and simulation
  keywords:
  - science AI
  - benchmark
  - MLCommons
  - HPC
  description: MLCommons Science assembles benchmark tasks with datasets, targets,
    and implementations across earthquake forecasting, satellite imagery, drug screening,
    electron microscopy, and CFD to drive scientific ML reproducibility.
  task_types:
  - Time-series analysis
  - Image classification
  - Simulation surrogate modeling
  ai_capability_measured: Inference accuracy, simulation speed-up, generalization
  metrics:
  - MAE
  - Accuracy
  - Speedup vs simulation
  models:
  - CNN
  - GNN
  - Transformer
  notes: Joint national-lab effort under Apache‑2.0 license.
  cite:
  - "@misc{mlcommons_science2023,\n  title={MLCommons Science Working Group Benchmarks},\n\
    \  author={MLCommons Science Working Group},\n  year={2023},\n  url={https://github.com/mlcommons/science}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1UuDwy7ATzyYBqVDmvjQpxHt33FKws6hjcP8FFD1m1GI/edit?usp=chatgpt.com
  ML Motif: Time-series, Image/CV, HPC/inference
  Type: Framework
  ML task: NA
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2023-06
  Support Contact Person: MLCommons Science Working Group
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly
      defined with HPC system-level constraints and targets.
  - dataset_rating: 9.0
  - dataset_reason: Public scientific datasets (e.g., cosmology, weather); used consistently,
      though FAIR-compliance of individual datasets varies slightly.
  - performance_metrics_rating: 10.0
  - performance_metrics_reason: Training time, GPU utilization, and accuracy are all
      directly measured and benchmarked across HPC systems.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Reference implementations available and actively maintained;
      HPC setup may require domain-specific environment.
  - documentation_rating: 9.0
  - documentation_reason: GitHub repo and papers provide detailed instructions; reproducibility
      supported across multiple institutions.
- date: '2021-07-05'
  expired: null
  valid: 'yes'
  name: LHC New Physics Dataset
  url: https://arxiv.org/pdf/2107.02157
  domain: Particle Physics; Real-time Triggering
  focus: Real-time LHC event filtering for anomaly detection using proton collision
    data
  keywords:
  - anomaly detection
  - proton collision
  - real-time inference
  - event filtering
  - unsupervised ML
  description: A dataset of proton-proton collision events emulating a 40 MHz real-time
    data stream from LHC detectors, pre-filtered on electron or muon presence. Designed
    for unsupervised new-physics detection algorithms under latency/bandwidth constraints.
  task_types:
  - Anomaly detection
  - Event classification
  ai_capability_measured: Unsupervised signal detection under latency and bandwidth
    constraints
  metrics:
  - ROC-AUC
  - Detection efficiency
  models:
  - Autoencoder
  - Variational autoencoder
  - Isolation forest
  notes: Includes electron/muon-filtered background and black-box signal benchmarks;
    1M events per black box.
  cite:
  - "@article{govorkova2022lhcnewphysics,\n  title={LHC physics dataset for unsupervised\
    \ New Physics detection at 40 MHz},\n  author={Govorkova, Ekaterina and Puljak,\
    \ Ema and Pierini, Maurizio and others},\n  journal={Scientific Data},\n  year={2022},\n\
    \  doi={10.6084/m9.figshare.5046389},\n  url={https://doi.org/10.5281/zenodo.5046389}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1BnX67GfTQxHbDuUsH-MuHIl1uKxCIjrXHSoxvIaB72g/edit?usp=sharing
  ML Motif: Multiple
  Type: Framework
  ML task: NA
  Solutions: '3'
  Dataset: 'Zenodo stores: background + 3 black-box signal sets (1M events each)'
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2021-07
  Support Contact Person: Ema Puljak (ema.puljak@cern.ch)
  ratings:
  - problem_spec_rating: 7.0
    problem_spec_reason: The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but lacks a formal task specification or constraints.
  - dataset_rating: 8.0
    dataset_reason: Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script.
  - reference_solution_rating: 5.0
    reference_solution_reason: Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers.
  - documentation_rating: 6.0
    documentation_reason: Publicly available papers and datasets with descriptions, but no unified README or training setup.

- date: '2023-07-17'
  expired: null
  valid: 'yes'
  name: MLCommons Medical AI
  url: https://github.com/mlcommons/medical
  domain: Healthcare; Medical AI
  focus: Federated benchmarking and evaluation of medical AI models across diverse
    real-world clinical data
  keywords:
  - medical AI
  - federated evaluation
  - privacy-preserving
  - fairness
  - healthcare benchmarks
  description: 'The MLCommons Medical AI working group develops benchmarks, best practices,
    and platforms (MedPerf, GaNDLF, COFE)

    to accelerate robust, privacy‐preserving AI development for healthcare. MedPerf
    enables federated testing of clinical

    models on diverse datasets, improving generalizability and equity while keeping
    data onsite :contentReference[oaicite:1]{index=1}.

    '
  task_types:
  - Federated evaluation
  - Model validation
  ai_capability_measured: Clinical accuracy, fairness, generalizability, privacy compliance
  metrics:
  - ROC AUC
  - Accuracy
  - Fairness metrics
  models:
  - MedPerf-validated CNNs
  - GaNDLF workflows
  notes: Open-source platform under Apache‑2.0; used across 20+ institutions and hospitals
    :contentReference[oaicite:2]{index=2}.
  cite:
  - "@article{karargyris2023federated,\n  title={Federated benchmarking of medical\
    \ artificial intelligence with MedPerf},\n  author={Karargyris, Alex and Sheller,\
    \ Micah J and others},\n  journal={Nature Machine Intelligence},\n  year={2023},\n\
    \  url={https://www.nature.com/articles/s42256-023-00652-2}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/17dgw85X7wVRUt-ylrQsNOYTWrN15lE9bx0BxA-vcPhM/edit?usp=sharing
  ML Motif: Multiple
  Type: Platform
  ML task: NA
  Solutions: '2'
  Dataset: Multi-institutional clinical datasets (radiology, EHR)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2023-07
  Support Contact Person: Alex Karargyris (MLCommons Medical AI)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Diverse scientific tasks (earthquake, CFD, microscopy) with
      detailed problem statements and goals; system constraints not uniformly applied.
  - dataset_rating: 9.0
  - dataset_reason: Domain-specific datasets (e.g., microscopy, climate); mostly public
      and structured, but FAIR annotations are not always explicit.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Task-specific metrics (MAE, speedup, accuracy) are
      clear and reproducible.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Reference models (CNN, GNN, Transformer) provided with
      training/evaluation pipelines.
  - documentation_rating: 9.0
  - documentation_reason: Well-documented, open-sourced, and maintained with examples;
      strong community support and reproducibility focus.
- date: '2024-10-28'
  expired: null
  valid: 'yes'
  name: CaloChallenge 2022
  url: http://arxiv.org/abs/2410.21611
  domain: LHC Calorimeter; Particle Physics
  focus: Fast generative-model-based calorimeter shower simulation evaluation
  keywords:
  - calorimeter simulation
  - generative models
  - surrogate modeling
  - LHC
  - fast simulation
  description: 'The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative‐model
    submissions (VAEs, GANs, Flows, Diffusion)

    on four calorimeter shower datasets; benchmarking shower quality, generation speed,
    and model complexity :contentReference[oaicite:3]{index=3}.

    '
  task_types:
  - Surrogate modeling
  ai_capability_measured: Simulation fidelity, speed, efficiency
  metrics:
  - Histogram similarity
  - Classifier AUC
  - Generation latency
  models:
  - VAE variants
  - GAN variants
  - Normalizing flows
  - Diffusion models
  notes: The most comprehensive survey to date on ML-based calorimeter simulation;
    31 submissions over different dataset sizes.
  cite:
  - "@article{krause2024calochallenge,\n  title={CaloChallenge 2022: A Community Challenge\
    \ for Fast Calorimeter Simulation},\n  author={Krause, Claudius and Nachman, Benjamin\
    \ and others},\n  year={2024},\n  url={https://arxiv.org/abs/2410.21611}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1JBH3WTDp2jpSt_utc1p5Dv3-MBX4xY-NVzzfXCd9xhA/edit?usp=sharing
  ML Motif: Surrogate
  Type: Dataset
  ML task: Surrogate Modeling
  Solutions: '31'
  Dataset: Four LHC calorimeter shower datasets (various voxel resolutions)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-10
  Support Contact Person: Claudius Krause (CaloChallenge Lead)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: 'Task is clearly defined: real-time anomaly detection from
      high-rate LHC collisions. Latency and bandwidth constraints are mentioned, though
      not numerically enforced.'
  - dataset_rating: 9.0
  - dataset_reason: Publicly available via Zenodo, with structured signal/background
      splits, and rich metadata; nearly fully FAIR.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: ROC-AUC and detection efficiency are clearly defined
      and appropriate for unsupervised anomaly detection.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Several baseline methods (autoencoder, VAE, isolation
      forest) are evaluated; runnable versions available via community repos but not
      tightly bundled.
  - documentation_rating: 8.0
  - documentation_reason: Paper and data documentation are clear, and the dataset
      is widely reused. Setup requires some manual effort to reproduce full pipelines.
- date: ongoing
  expired: null
  valid: 'yes'
  name: Papers With Code (SOTA Platform)
  url: https://paperswithcode.com/sota
  domain: General ML; All domains
  focus: Open platform tracking state-of-the-art results, benchmarks, and implementations
    across ML tasks and papers
  keywords:
  - leaderboard
  - benchmarking
  - reproducibility
  - open-source
  description: 'Papers With Code (PWC) aggregates benchmark suites, tasks, and code
    across ML research:

    12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It
    tracks SOTA metrics and fosters reproducibility.

    '
  task_types:
  - Multiple (Classification, Detection, NLP, etc.)
  ai_capability_measured: Model performance across tasks (accuracy, F1, BLEU, etc.)
  metrics:
  - Task-specific (Accuracy, F1, BLEU, etc.)
  models:
  - All published models with code
  notes: Community-driven open platform; automatic data extraction and versioning.
  cite:
  - "@misc{pwc2025,\n  title={Papers With Code: Open machine learning benchmarks and\
    \ leaderboards},\n  author={Papers With Code},\n  year={2025},\n  url={https://paperswithcode.com}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1pbn_K20t6Kr0zxdUSAL68ChruOu6B_x5sZbp-dcrG_g/edit?usp=sharing
  ML Motif: Multiple
  Type: Platform
  ML task: Multiple
  Solutions: '154766'
  Dataset: Curated benchmark-task pairs from literature
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Papers With Code Team
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Evaluation setting (federated clinical benchmarking) is well-defined;
      I/O interfaces vary slightly by task but are standardized in MedPerf platform.
  - dataset_rating: 8.0
  - dataset_reason: Uses distributed, real-world clinical datasets across institutions;
      FAIR compliance varies across hospitals and data hosts.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: ROC AUC, accuracy, and fairness metrics are explicitly
      defined and task-dependent; consistently tracked across institutions.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Validated CNNs and GaNDLF pipelines are used and shared
      via the MedPerf tool, but some implementations are abstracted behind the platform.
  - documentation_rating: 9.0
  - documentation_reason: Excellent documentation across MedPerf, GaNDLF, and COFE;
      reproducibility handled via containerized flows and task templates.
- date: '2022-01-01'
  expired: null
  valid: 'yes'
  name: Codabench
  url: https://www.codabench.org/
  domain: General ML; Multiple
  focus: Open-source platform for organizing reproducible AI benchmarks and competitions
  keywords:
  - benchmark platform
  - code submission
  - competitions
  - meta-benchmark
  description: 'Codabench (successor to CodaLab) is a flexible, easy‑to‑use, reproducible
    API platform for hosting AI benchmarks

    and code‑submission challenges. It supports custom scoring, inverted benchmarks,
    and scalable public or private queues :contentReference[oaicite:1]{index=1}.

    '
  task_types:
  - Multiple
  ai_capability_measured: Model reproducibility, performance across datasets
  metrics:
  - Submission count
  - Leaderboard ranking
  - Task-specific metrics
  models:
  - Arbitrary code submissions
  notes: Hosts 51 public competitions, ~26 k users, 177 k submissions :contentReference[oaicite:2]{index=2}
  cite:
  - "@article{xu2021codabench,\n  title={Codabench: Flexible, easy-to-use, and reproducible\
    \ meta-benchmark platform},\n  author={Xu, Zhen and Escalera, Sergio and others},\n\
    \  journal={Patterns},\n  volume={3},\n  number={7},\n  pages={100543},\n  year={2022},\n\
    \  doi={10.1016/j.patter.2022.100543}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1sIwNDCs01s790DApVt5leiG8UaBDFVINA4bOixZ1CUw/edit?usp=sharing
  ML Motif: Multiple
  Type: Platform
  ML task: Multiple
  Solutions: 98 071
  Dataset: N/A
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-03
  Support Contact Person: Isabelle Guyon (Université Paris‑Saclay)
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: Simulation task (generative calorimeter showers) is clearly
      stated with multiple datasets, fidelity requirements, and performance constraints.
  - dataset_rating: 9.5
  - dataset_reason: Public datasets available in multiple sizes and formats; well-documented;
      not versioned
  - performance_metrics_rating: 10.0
  - performance_metrics_reason: Histogram similarity, classifier AUC, and generation
      latency are clearly defined and benchmarked across all submissions.
  - reference_solution_rating: 9.0
  - reference_solution_reason: 31 model implementations submitted; some made public
      and reproducible, though others remain undocumented or private.
  - documentation_rating: 9.0
  - documentation_reason: Paper, leaderboard, and Gemini doc are comprehensive; unified
      repo or launchable baseline kit would push this to a 10.
- date: '2021-09-27'
  expired: null
  valid: 'yes'
  name: Sabath (SBI‑FAIR)
  url: https://sbi-fair.github.io/docs/software/sabath/
  domain: Systems; Metadata
  focus: FAIR metadata framework for ML-driven surrogate workflows in HPC systems
  keywords:
  - meta‑benchmark
  - metadata
  - HPC
  - surrogate modeling
  description: 'Sabath is a metadata framework from the SBI‑FAIR group (UTK, Argonne,
    Virginia) facilitating

    FAIR-compliant benchmarking and surrogate execution logging across HPC systems
    :contentReference[oaicite:3]{index=3}.

    '
  task_types:
  - Systems benchmarking
  ai_capability_measured: Metadata tracking, reproducible HPC workflows
  metrics:
  - Metadata completeness
  - FAIR compliance
  models:
  - N/A
  notes: Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN,
    Cosmoflow, etc. :contentReference[oaicite:4]{index=4}
  cite:
  - "@techreport{luszczek2021sabath,\n  title={SABATH: FAIR Metadata Technology for\
    \ Surrogate Benchmarks},\n  author={Luszczek, Piotr and others},\n  year={2021},\n\
    \  institution={University of Tennessee}\n}\n"
  Results from Gemini LLM Deep Research: (none)
  ML Motif: Systems
  Type: Platform
  ML task: NA
  Solutions: N/A
  Dataset: N/A
  Software: 'Yes'
  Benchmark-Ready: N/A
  Last Updated: 2023-07
  Support Contact Person: Piotr Luszczek (luszczek@utk.edu)
  ratings:
  - problem_spec_rating: 8.0
    problem_spec_reason: The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle physics datasets.
  - dataset_rating: 8.0
    dataset_reason: Data is well-structured for SBI and publicly available with clear licensing.
  - performance_metrics_rating: 8.0
    performance_metrics_reason: Includes likelihood and posterior accuracy; metrics well-matched to SBI.
  - reference_solution_rating: 7.0
    reference_solution_reason: Baseline SBI models are implemented and reproducible.
  - documentation_rating: 6.0
    documentation_reason: GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs.

- date: '2022-10-13'
  expired: null
  valid: 'yes'
  name: PDEBench
  url: https://github.com/pdebench/PDEBench
  domain: CFD; Weather Modeling
  focus: Benchmark suite for ML-based surrogates solving time-dependent PDEs
  keywords:
  - PDEs
  - CFD
  - scientific ML
  - surrogate modeling
  - NeurIPS
  description: 'PDEBench offers forward/inverse PDE tasks with large ready‑to‑use
    datasets and baselines (FNO, U‑Net, PINN), packaged via a unified API. It won
    the SimTech Best Paper Award 2023 :contentReference[oaicite:5]{index=5}.

    '
  task_types:
  - Supervised Learning
  ai_capability_measured: Time-dependent PDE modeling; physical accuracy
  metrics:
  - RMSE
  - boundary RMSE
  - Fourier RMSE
  models:
  - FNO
  - U‑Net
  - PINN
  - Gradient‑Based inverse methods
  notes: Datasets hosted on DaRUS (DOI:10.18419/darus‑2986); contact maintainers by
    email :contentReference[oaicite:6]{index=6}
  cite:
  - "@inproceedings{takamoto2022pdebench,\n  author={Takamoto, Makoto and Praditia,\
    \ Timothy and others},\n  title={PDEBench: An Extensive Benchmark for Scientific\
    \ Machine Learning},\n  booktitle={NeurIPS Datasets and Benchmarks Track},\n \
    \ year={2022},\n  url={https://arxiv.org/abs/2210.07182}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1MvXdFub0PxUDtB49wqli6mmSCdLErv2nLdOJUtMylOo/edit?usp=sharing
  ML Motif: Multiple
  Type: Framework
  ML task: Supervised Learning
  Solutions: 'Yes'
  Dataset: DaRUS repository via DOI:10.18419/darus‑2986
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-05
  Support Contact Person: Makoto Takamoto (makoto.takamoto@neclab.eu)
  ratings:
  - problem_spec_rating: 9.0
    problem_spec_reason: Clearly defined PDE-solving tasks with well-specified constraints and solution formats.
  - dataset_rating: 9.0
    dataset_reason: Includes synthetic and real-world PDE datasets with detailed format descriptions.
  - performance_metrics_rating: 8.0
    performance_metrics_reason: Uses L2 error and other norms relevant to PDE solutions.
  - reference_solution_rating: 7.0
    reference_solution_reason: Includes baseline solvers and trained models across multiple PDE tasks.
  - documentation_rating: 8.0
    documentation_reason: Well-organized GitHub with examples, dataset loading scripts, and training configs.

- date: '2024-12-03'
  expired: null
  valid: 'yes'
  name: The Well
  url: https://polymathic-ai.org/the_well/
  domain: biological systems, fluid dynamics, acoustic scattering, astrophysical MHD
  focus: Foundation model + surrogate dataset spanning 16 physical simulation domains
  keywords:
  - surrogate modeling
  - foundation model
  - physics simulations
  - spatiotemporal dynamics
  description: 'A 15 TB collection of ML-ready physics simulation datasets (HDF5),
    covering 16 domains—from biology to astrophysical magnetohydrodynamic simulations—with
    unified API and metadata. Ideal for training surrogate and foundation models on
    scientific data. :contentReference[oaicite:1]{index=1}

    '
  task_types:
  - Supervised Learning
  ai_capability_measured: Surrogate modeling, physics-based prediction
  metrics:
  - Dataset size
  - Domain breadth
  models:
  - FNO baselines
  - U‑Net baselines
  notes: 'Includes unified API and dataset metadata; see 2025 NeurIPS paper for full
    benchmark details. Size: 15 TB. :contentReference[oaicite:2]{index=2}'
  cite:
  - "@article{ohana2024well,\n  title={The well: a large-scale collection of diverse\
    \ physics simulations for machine learning},\n  author={Ohana, Ruben and McCabe,\
    \ Michael and Meyer, Lucas and others},\n  journal={NeurIPS},\n  volume={37},\n\
    \  pages={44989--45037},\n  year={2024}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1axQvD_aY9O71A2nxWaoFvsfE8HvZ4TmwDoZ4cQQwW58/edit?usp=sharing
  ML Motif: Foundation model, Surrogate
  Type: Dataset
  ML task: Supervised Learning
  Solutions: '16'
  Dataset: 16 simulation datasets (HDF5) via PyPI/GitHub
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Wes Brewer
  ratings:
  - problem_spec_rating: 7.0
    problem_spec_reason: Explores LLM understanding of mental health scenarios; framing is creative but loosely defined.
  - dataset_rating: 6.0
    dataset_reason: Dataset is described in concept but not released; privacy limits public access though synthetic proxies are referenced.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Uses manual annotation and quality scores, but lacks standardized automatic metrics.
  - reference_solution_rating: 6.0
    reference_solution_reason: Provides few-shot prompt examples and human rating calibration details.
  - documentation_rating: 5.0
    documentation_reason: Paper gives use cases, but code and data are not yet public.

- date: '2024-10-31'
  expired: null
  valid: 'yes'
  name: LLM-Inference-Bench
  url: https://github.com/argonne-lcf/LLM-Inference-Bench
  domain: LLM; HPC/inference
  focus: Hardware performance benchmarking of LLMs on AI accelerators
  keywords:
  - LLM
  - inference benchmarking
  - GPU
  - accelerator
  - throughput
  description: 'A suite evaluating inference performance of LLMs (LLaMA, Mistral,
    Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks
    (vLLM, DeepSpeed‑MII, etc.), with an interactive dashboard and per-platform metrics.
    :contentReference[oaicite:3]{index=3}

    '
  task_types:
  - Inference Benchmarking
  ai_capability_measured: Inference throughput, latency, hardware utilization
  metrics:
  - Token throughput (tok/s)
  - Latency
  - Framework-hardware mix performance
  models:
  - LLaMA-2‑7B
  - LLaMA-2‑70B
  - Mistral‑7B
  - Qwen‑7B
  notes: Licensed under BSD‑3, maintained by Argonne; supports GPUs and accelerators.
    :contentReference[oaicite:4]{index=4}
  cite:
  - "@article{chitty2024llm,\n  title={LLM-Inference-Bench: Inference Benchmarking\
    \ of Large Language Models on AI Accelerators},\n  author={Chitty-Venkata, Krishna\
    \ Teja and Raskar, Siddhisanket and others},\n  journal={arXiv preprint arXiv:2411.00136},\n\
    \  year={2024}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1I3UvByGn4KaruQC1pi6XcfoAOzt4iiA61S0nR9ovC94/edit?usp=sharing
  ML Motif: HPC/inference
  Type: Dataset
  ML task: Inference Benchmarking
  Solutions: ''
  Dataset: Performance logs, model-hardware pairs
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-11
  Support Contact Person: Krishna Teja Chitty-Venkata (Argonne LCF)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: PDE tasks (forward/inverse) and I/O structures are clearly
      specified with detailed PDE context and constraints.
  - dataset_rating: 10.0
  - dataset_reason: Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Uses RMSE variants and Fourier-based errors.
  - reference_solution_rating: 10.0
  - reference_solution_reason: Baselines (FNO, U-Net, PINN) implemented and ready-to-run;
      strong community adoption.
  - documentation_rating: 9.0
  - documentation_reason: Clean GitHub with usage, dataset links, and tutorial notebooks.
- date: '2023-12-12'
  expired: null
  valid: 'yes'
  name: SGLang Framework
  url: https://github.com/sgl-project/sglang/tree/main/benchmark
  domain: LLM Vision
  focus: Fast serving framework for LLMs and vision-language models
  keywords:
  - LLM serving
  - vision-language
  - RadixAttention
  - performance
  - JSON decoding
  description: 'A high-performance open-source serving framework combining efficient
    backend runtime (RadixAttention, batching, quantization) and expressive frontend
    language, boosting LLM/VLM inference throughput up to ~3x over alternatives. :contentReference[oaicite:5]{index=5}

    '
  task_types:
  - Model serving framework
  ai_capability_measured: Serving throughput, JSON/task-specific latency
  metrics:
  - Tokens/sec
  - Time-to-first-token
  - Throughput gain vs baseline
  models:
  - LLaVA
  - DeepSeek
  - Llama
  notes: Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025.
    :contentReference[oaicite:6]{index=6}
  cite:
  - "@article{zheng2023sglang,\n  title={SGLang: Efficient Execution of Structured\
    \ Language Model Programs},\n  author={Zheng, Lianmin and Yin, Liangsheng and\
    \ others},\n  year={2023},\n  url={https://arxiv.org/abs/2312.07104}\n}\n"
  Results from Gemini LLM Deep Research: (none)
  ML Motif: LLM Vision
  Type: Framework
  ML task: Model serving
  Solutions: ''
  Dataset: Benchmark configs (dummy or real)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: SGLang Team
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Clearly framed around surrogate learning across 16 domains,
      but not all tasks are formally posed or constrained in a unified benchmark protocol.
      Paper mentions performance on NVIDIA H100.
  - dataset_rating: 9.0
  - dataset_reason: FAIR-compliant physics simulation dataset, structured in HDF5
      with unified metadata.
  - performance_metrics_rating: 7.0
  - performance_metrics_reason: Metrics like dataset size and domain coverage are
      listed, but standardized quantitative model evaluation metrics (e.g., RMSE,
      MAE) are not enforced.
  - reference_solution_rating: 9.0
  - reference_solution_reason: FNO and U-Net baselines available; full benchmarking
      implementations pending NeurIPS paper code release.
  - documentation_rating: 10.0
  - documentation_reason: Site and GitHub offer a unified API, metadata standards,
      and dataset loading tools; NeurIPS paper adds detailed design context.
- date: '2023-09-12'
  expired: null
  valid: 'yes'
  name: vLLM Inference and Serving Engine
  url: https://github.com/vllm-project/vllm/tree/main/benchmarks
  domain: LLM; HPC/inference
  focus: High-throughput, memory-efficient inference and serving engine for LLMs
  keywords:
  - LLM inference
  - PagedAttention
  - CUDA graph
  - streaming API
  - quantization
  description: "vLLM is a fast, high-throughput, memory-efficient inference and serving\
    \ engine for large language models, \nfeaturing PagedAttention, continuous batching,\
    \ and support for quantized and pipelined model execution. \nBenchmarks compare\
    \ it to TensorRT-LLM, SGLang, and others. :contentReference[oaicite:1]{index=1}\n"
  task_types:
  - Inference Benchmarking
  ai_capability_measured: Throughput, latency, memory efficiency
  metrics:
  - Tokens/sec
  - Time to First Token (TTFT)
  - Memory footprint
  models:
  - LLaMA
  - Mixtral
  - FlashAttention-based models
  notes: Incubated by LF AI and Data; achieves up to 24× throughput over HuggingFace
    Transformers :contentReference[oaicite:2]{index=2}
  cite:
  - "@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large\
    \ Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and others},\n\
    \  booktitle={SOSP 2023},\n  year={2023}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1nPZyRZEZHciGXYNJShR9FCJbre7ImJLKf2MG6g4-3gQ/edit?usp=sharing
  ML Motif: HPC/inference
  Type: Framework
  ML task: Inference
  Solutions: '-'
  Dataset: Benchmark scripts and model configurations
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Woosuk Kwon (vLLM Team)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Benchmarks hardware performance of LLM inference across multiple
      platforms with well-defined input/output and platform constraints.
  - dataset_rating: 7.0
  - dataset_reason: Uses structured log files and configs instead of conventional
      datasets; suitable for inference benchmarking.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Clear throughput, latency, and utilization metrics;
      platform comparison dashboard enhances evaluation.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Includes reproducible scripts and example runs; models
      like LLaMA and Mistral are referenced with platform-specific configs.
  - documentation_rating: 8.0
  - documentation_reason: GitHub contains clear instructions, platform details, and
      framework comparisons.
- date: '2022-06-22'
  expired: null
  valid: 'yes'
  name: vLLM Performance Dashboard
  url: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/
  domain: LLM; HPC/inference
  focus: Interactive dashboard showing inference performance of vLLM
  keywords:
  - Dashboard
  - Throughput visualization
  - Latency analysis
  - Metric tracking
  description: 'A live visual dashboard for vLLM showcasing throughput, latency, and
    other inference metrics across models and hardware configurations.

    '
  task_types:
  - Performance visualization
  ai_capability_measured: Throughput, latency, hardware utilization
  metrics:
  - Tokens/sec
  - TTFT
  - Memory usage
  models:
  - LLaMA-2
  - Mistral
  - Qwen
  notes: Built using ObservableHQ; integrates live data from vLLM benchmarks.
  cite:
  - "@misc{mo2024vllm_dashboard,\n  title={vLLM Performance Dashboard},\n  author={Mo,\
    \ Simon},\n  year={2024},\n  url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}\n\
    }\n"
  Results from Gemini LLM Deep Research: (none)
  ML Motif: HPC/inference
  Type: Framework
  ML task: Visualization
  Solutions: '-'
  Dataset: Dashboard configurations
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-01
  Support Contact Person: Simon Mo
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Framed as a model-serving tool rather than a benchmark, but
      includes benchmark configurations and real model tasks.
  - dataset_rating: 6.0
  - dataset_reason: Mostly uses dummy configs or external model endpoints for evaluation;
      not designed around a formal dataset.
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: 'Well-defined serving metrics: tokens/sec, time-to-first-token,
      and gain over baselines.'
  - reference_solution_rating: 9.0
  - reference_solution_reason: Core framework includes full reproducible serving benchmarks
      and code; multiple deployment case studies.
  - documentation_rating: 9.0
  - documentation_reason: High-quality usage guides, examples, and performance tuning
      docs.
- date: '2022-04-01'
  expired: null
  valid: 'yes'
  name: Nixtla NeuralForecast
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series forecasting; General ML
  focus: High-performance neural forecasting library with >30 models
  keywords:
  - time-series
  - neural forecasting
  - NBEATS, NHITS, TFT
  - probabilistic forecasting
  - usability
  description: 'NeuralForecast offers scalable, user-friendly implementations of over
    30 neural forecasting models (NBEATS, NHITS, TFT, DeepAR, etc.),

    emphasizing quality, usability, interpretability, and performance. :contentReference[oaicite:3]{index=3}

    '
  task_types:
  - Time-series forecasting
  ai_capability_measured: Forecast accuracy, interpretability, speed
  metrics:
  - RMSE
  - MAPE
  - CRPS
  models:
  - NBEATS
  - NHITS
  - TFT
  - DeepAR
  notes: AutoModel supports hyperparameter tuning and distributed execution via Ray
    and Optuna. Fi­rst official NHITS implementation. :contentReference[oaicite:4]{index=4}
  cite:
  - "@misc{olivares2022library_neuralforecast,\n  author={Olivares, Kin G. and Challú,\
    \ Cristian and others},\n  title={NeuralForecast: User friendly state‑of‑the‑art\
    \ neural forecasting models},\n  year={2022},\n  howpublished={{PyCon} US},\n\
    \  url={https://github.com/Nixtla/neuralforecast}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1VzhaUubIm-SHK7cfKWyoi8GtykpCuOH2qPM-k_g8bKU/edit?usp=sharing
  ML Motif: Time-series
  Type: Platform
  ML task: Forecasting
  Solutions: '26'
  Dataset: M4, electricity, standard TS benchmarks
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Kin G. Olivares (Nixtla)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Targets high-throughput LLM inference via PagedAttention
      and memory-optimized serving; benchmarks cover many configs.
  - dataset_rating: 7.0
  - dataset_reason: Focuses on model configs and streaming input/output pipelines
      rather than classical datasets.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Strong token/sec, memory usage, and TTFT metrics;
      comparative plots and logs included.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Benchmarks reproducible via script with support for
      multiple models and hardware types.
  - documentation_rating: 9.0
  - documentation_reason: Excellent GitHub docs, CLI/API usage, and deployment walkthroughs.
- date: '2023-06-01'
  expired: null
  valid: 'yes'
  name: Nixtla Neural Forecast NHITS
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Official NHITS implementation for long-horizon time series forecasting
  keywords:
  - NHITS
  - long-horizon forecasting
  - neural interpolation
  - time-series
  description: 'NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art
    model that

    improved accuracy by ~25% and reduced compute by 50× compared to Transformer baselines,

    using hierarchical interpolation and multi-rate sampling :contentReference[oaicite:1]{index=1}.

    '
  task_types:
  - Time-series forecasting
  ai_capability_measured: Accuracy, compute efficiency for long series
  metrics:
  - RMSE
  - MAPE
  models:
  - NHITS
  notes: Official implementation in NeuralForecast, included since its AAAI 2023 release.
  cite:
  - "@inproceedings{challu2023nhits,\n  title={NHITS: Neural Hierarchical Interpolation\
    \ for Time Series Forecasting},\n  author={Challu, Cristian and Olivares, Kin\
    \ G. and others},\n  booktitle={AAAI 2023},\n  year={2023}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/15Hm5ekGu99aQWsdtiUIwX6JMoaoFpRbIhDylrWqSoHY/edit?usp=sharing
  ML Motif: Time-series
  Type: Platform
  ML task: Forecasting
  Solutions: '26'
  Dataset: Standard forecast datasets (M4, etc.)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Kin G. Olivares (Nixtla)
  ratings:
  - problem_spec_rating: 7.0
  - problem_spec_reason: Primarily a visualization frontend; underlying benchmark
      definitions come from vLLM project.
  - dataset_rating: 6.0
  - dataset_reason: No traditional dataset; displays live or logged benchmark metrics.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Live throughput, memory, latency, and TTFT displayed
      interactively; highly informative for performance analysis.
  - reference_solution_rating: 7.0
  - reference_solution_reason: Dashboard built on vLLM benchmarks but not itself a
      complete experiment package.
  - documentation_rating: 8.0
  - documentation_reason: Observable notebooks are intuitive; customization instructions
      are minimal but UI is self-explanatory.
- date: '2023-10-03'
  expired: null
  valid: 'yes'
  name: Nixtla Neural Forecast TimeLLM
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Reprogramming LLMs for time series forecasting
  keywords:
  - Time-LLM
  - language model
  - time-series
  - reprogramming
  description: 'Time‑LLM uses reprogramming layers to adapt frozen LLMs for time series
    forecasting, treating

    forecasting as a language task :contentReference[oaicite:2]{index=2}.

    '
  task_types:
  - Time-series forecasting
  ai_capability_measured: Model reuse via LLM, few-shot forecasting
  metrics:
  - RMSE
  - MAPE
  models:
  - Time‑LLM
  notes: Fully open-source; transforms forecasting using LLM text reconstruction.
  cite:
  - "@article{jin2023time,\n  title={Time‑LLM: Time Series Forecasting by Reprogramming\
    \ Large Language Models},\n  author={Jin, Ming and Wang, Shiyu and others},\n\
    \  journal={arXiv preprint arXiv:2310.01728},\n  year={2023}\n}\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1xXGzRt-qhUFTvnBGQi2IbcoBdYyo-ZrAn3IOkswd3fw/edit?usp=sharing
  ML Motif: Time-series
  Type: Platform
  ML task: Forecasting
  Solutions: '26'
  Dataset: Standard forecast datasets (M4, etc.)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Ming Jin (Nixtla)
  ratings:
  - problem_spec_rating: 7.0
    problem_spec_reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
  - dataset_rating: 6.0
    dataset_reason: Uses open time series datasets, but lacks a consolidated data release or splits.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Reports metrics like MASE and SMAPE, standard in forecasting.
  - reference_solution_rating: 6.0
    reference_solution_reason: Provides TimeLLM with open source, but no other baselines included.
  - documentation_rating: 6.0
    documentation_reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.

- date: '2023-10-05'
  expired: null
  valid: 'yes'
  name: Nixtla Neural Forecast TimeGPT
  url: https://github.com/Nixtla/neuralforecast
  domain: Time-series; General ML
  focus: Time-series foundation model "TimeGPT" for forecasting and anomaly detection
  keywords:
  - TimeGPT
  - foundation model
  - time-series
  - generative model
  description: 'TimeGPT is a transformer-based generative pretrained model on 100B+
    time series data for

    zero-shot forecasting and anomaly detection via API :contentReference[oaicite:3]{index=3}.

    '
  task_types:
  - Time-series forecasting
  - Anomaly detection
  ai_capability_measured: Zero-shot forecasting, anomaly detection
  metrics:
  - RMSE
  - Anomaly detection metrics
  models:
  - TimeGPT
  notes: Offered via Nixtla API and Azure Studio; enterprise-grade support available.
  cite:
  - "@article{garza2023timegpt,\n  title={TimeGPT‑1: A Foundation Model for Time Series},\n\
    \  author={Garza, Azul and Challu, Cristian and others},\n  year={2023},\n  url={https://arxiv.org/abs/2310.03589}\n\
    }\n"
  Results from Gemini LLM Deep Research: https://docs.google.com/document/d/1KmKs9JtcfpKe40fNuLrgdFcOmBnfj3ZG7AwcfTH4tXE/edit?usp=sharing
  ML Motif: Time-series
  Type: Platform
  ML task: Forecasting
  Solutions: '26'
  Dataset: Pretrained on 100 B+ time series via Nixtla
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-06
  Support Contact Person: Azul Garza (Nixtla)
  ratings:
  - problem_spec_rating: 7.0
    problem_spec_reason: Describes forecasting with LLMs, but less formal on input/output or task framing.
  - dataset_rating: 6.0
    dataset_reason: Uses open time series datasets, but lacks a consolidated data release or splits.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Reports metrics like MASE and SMAPE, standard in forecasting.
  - reference_solution_rating: 6.0
    reference_solution_reason: Provides TimeLLM with open source, but no other baselines included.
  - documentation_rating: 6.0
    documentation_reason: GitHub readme with installation and example usage; lacks API or extensive tutorials.

- date: '2025-03-03'
  expired: null
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Gravitational Waves)
  url: https://www.codabench.org/competitions/2626/
  domain: Astrophysics; Time-series
  focus: Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets
  keywords:
  - anomaly detection
  - gravitational waves
  - astrophysics
  - time-series
  description: 'A benchmark for detecting anomalous transient gravitational-wave signals,
    including “unknown-unknowns,” using preprocessed LIGO time-series at 4096 Hz.
    Competitors submit inference models on Codabench for continuous 50 ms segments
    from dual interferometers. :contentReference[oaicite:1]{index=1}

    '
  task_types:
  - Anomaly detection
  ai_capability_measured: Novel event detection in physical signals
  metrics:
  - ROC‑AUC
  - Precision/Recall
  models:
  - Deep latent CNNs
  - Autoencoders
  notes: NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench.
    :contentReference[oaicite:2]{index=2}
  cite:
  - "@article{campolongo2025hdranomaly2,\n  title={Building Machine Learning Challenges\
    \ for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n\
    \  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}\n"
  Results from Gemini LLM Deep Research: https://www.codabench.org/competitions/2626/
  ML Motif: Time-series
  Type: Dataset
  ML task: Anomaly detection
  Solutions: '-'
  Dataset: Preprocessed LIGO/Hanford and Livingston waveforms
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-03
  Support Contact Person: HDR A3D3 Team
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Novel approach treating forecasting as text generation is
      explained; framing is less conventional.
  - dataset_rating: 9.0
  - dataset_reason: Compatible with standard forecasting datasets (e.g., M4, electricity).
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: RMSE and MAPE are included, but less emphasis on interpretability
      or time-series domain constraints.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Open-source with reprogramming layers, LLM interface
      scripts provided.
  - documentation_rating: 8.0
  - documentation_reason: Model and architecture overview present, though usability
      guide is slightly lighter than others.
- date: '2025-03-03'
  expired: null
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Butterfly)
  url: https://www.codabench.org/competitions/3764/
  domain: Genomics; Image/CV
  focus: Detecting hybrid butterflies via image anomaly detection in genomic-informed
    dataset
  keywords:
  - anomaly detection
  - computer vision
  - genomics
  - butterfly hybrids
  description: 'Image-based challenge for detecting butterfly hybrids in microscopy-driven
    species data. Participants evaluate models on Codabench using image segmentation/classification.
    :contentReference[oaicite:3]{index=3}

    '
  task_types:
  - Anomaly detection
  ai_capability_measured: Hybrid detection in biological systems
  metrics:
  - Classification accuracy
  - F1 score
  models:
  - CNN-based detectors
  notes: Hybrid detection benchmarks hosted on Codabench. :contentReference[oaicite:4]{index=4}
  cite:
  - "@article{campolongo2025hdranomaly,\n  title={Building Machine Learning Challenges\
    \ for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n\
    \  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}\n"
  Results from Gemini LLM Deep Research: https://www.codabench.org/competitions/3764/
  ML Motif: Image/CV
  Type: Dataset
  ML task: Anomaly detection
  Solutions: '-'
  Dataset: Butterfly hybrid image dataset
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-03
  Support Contact Person: Imageomics/HDR Team
  ratings:
  - problem_spec_rating: 8.0
    problem_spec_reason: Task of detecting rare anomalies in butterfly physics is well-described with physics motivation.
  - dataset_rating: 7.0
    dataset_reason: Real detector data with injected anomalies is available, but requires NDA for full access.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Uses ROC, F1, and anomaly precision, standard in challenge evaluations.
  - reference_solution_rating: 4.0
    reference_solution_reason: Partial baselines described, but no codebase or reproducible runs.
  - documentation_rating: 6.0
    documentation_reason: Challenge site includes overview and metrics, but limited in walkthrough or examples.

- date: '2025-03-03'
  expired: null
  valid: 'yes'
  name: HDR ML Anomaly Challenge (Sea Level Rise)
  url: https://www.codabench.org/competitions/3223/
  domain: Climate Science; Time-series, Image/CV
  focus: Detecting anomalous sea-level rise and flooding events via time-series and
    satellite imagery
  keywords:
  - anomaly detection
  - climate science
  - sea-level rise
  - time-series
  - remote sensing
  description: 'A challenge combining North Atlantic sea-level time-series and satellite
    imagery to detect flooding anomalies. Models submitted via Codabench. :contentReference[oaicite:5]{index=5}

    '
  task_types:
  - Anomaly detection
  ai_capability_measured: Detection of environmental anomalies
  metrics:
  - ROC‑AUC
  - Precision/Recall
  models:
  - CNNs, RNNs, Transformers
  notes: Sponsored by NSF HDR; integrates sensor and satellite data. :contentReference[oaicite:6]{index=6}
  cite:
  - "@article{campolongo2025hdranomaly3,\n  title={Building Machine Learning Challenges\
    \ for Anomaly Detection in Science},\n  author={Campolongo, Elizabeth G. and others},\n\
    \  year={2025},\n  url={https://arxiv.org/abs/2503.02112}\n}\n"
  Results from Gemini LLM Deep Research: https://www.codabench.org/competitions/3223/
  ML Motif: Time-series, Image/CV
  Type: Dataset
  ML task: Anomaly detection
  Solutions: '-'
  Dataset: Sea-level time-series and satellite imagery
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-03
  Support Contact Person: HDR A3D3 Team
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Clear anomaly detection objective framed for physical signal
      discovery (LIGO/Virgo).
  - dataset_rating: 10.0
  - dataset_reason: Preprocessed waveform data from dual interferometers, public and
      well-structured.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: ROC-AUC, Precision/Recall, and confusion-based metrics
      are standardized.
  - reference_solution_rating: 1.0
  - reference_solution_reason: No starter model or baseline code linked
  - documentation_rating: 9.0
  - documentation_reason: Codabench page, GitHub starter kit, and related papers provide
      strong guidance.
- date: '2025-01-24'
  expired: null
  valid: 'yes'
  name: Single Qubit Readout on QICK System
  url: https://github.com/fastmachinelearning/ml-quantum-readout
  domain: Quantum Computing
  focus: Real-time single-qubit state classification using FPGA firmware
  keywords:
  - qubit readout
  - hls4ml
  - FPGA
  - QICK
  description: "Implements real-time ML models for single-qubit readout on the Quantum\
    \ Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural\
    \ networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination.\
    \ :contentReference[oaicite:0]{index=0}  \n"
  task_types:
  - Classification
  ai_capability_measured: Single-shot fidelity, inference latency
  metrics:
  - Accuracy
  - Latency
  models:
  - hls4ml quantized NN
  notes: Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization.
    :contentReference[oaicite:1]{index=1}
  cite:
  - "@article{diguglielmo2025endtoend,\n  title={End-to-end workflow for machine learning-based\
    \ qubit readout with QICK and hls4ml},\n  author={Di Guglielmo, Giuseppe and Campos,\
    \ Javier and others},\n  year={2025},\n  url={https://arxiv.org/abs/2501.14663}\n\
    }\n"
  Results from Gemini LLM Deep Research: (none)
  ML Motif: Real-time
  Type: Benchmark
  ML task: Supervised Learning
  Solutions: '-'
  Dataset: 'Zenodo: ml-quantum-readout dataset (zenodo.org/records/14427490)'
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2025-02
  Support Contact Person: Javier Campos / Giuseppe Di Guglielmo
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Task clearly framed around detecting hybrid species via images,
      but exact labeling methods and hybrid definitions may need elaboration.
  - dataset_rating: 8.0
  - dataset_reason: Dataset hosted on Codabench; appears structured but details on
      image sourcing and labeling pipeline are limited.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Classification accuracy and F1 are standard and appropriate.
  - reference_solution_rating: 1.0
  - reference_solution_reason: No starter model or baseline code linked
  - documentation_rating: 7.5
  - documentation_reason: Codabench task page describes dataset and evaluation method
      but lacks full API/docs.
- date: '2023-11-20'
  expired: null
  valid: 'yes'
  name: 'GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark'
  url: https://arxiv.org/abs/2311.12022
  domain: Science (Biology, Physics, Chemistry)
  focus: Graduate-level, expert-validated multiple-choice questions hard even with
    web access
  keywords:
  - Google-proof
  - multiple-choice
  - expert reasoning
  - science QA
  description: "Contains 448 challenging questions written by domain experts, with\
    \ expert accuracy at 65% (74% discounting clear errors) and non-experts reaching\
    \ just 34%. GPT‑4 baseline scores ~39%—designed for scalable oversight evaluation.\
    \ :contentReference[oaicite:2]{index=2}  \n"
  task_types:
  - Multiple choice
  ai_capability_measured: Scientific reasoning, knowledge probing
  metrics:
  - Accuracy
  models:
  - GPT‑4 baseline
  notes: “Google-proof”; supports oversight research.
  cite:
  - "@article{rein2023gpqa,\n  title={GPQA: A Graduate-Level Google-Proof Q and A\
    \ Benchmark},\n  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper\
    \ and others},\n  year={2023},\n  url={https://arxiv.org/abs/2311.12022}\n}\n"
  Results from Gemini LLM Deep Research: (none)
  ML Motif: Multiple choice
  Type: Benchmark
  ML task: Multiple choice
  Solutions: 448 questions
  Dataset: GPQA dataset (zip/HuggingFace)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2023-11
  Support Contact Person: David Rein (NYU)
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Clear dual-modality task (image + time-series); environmental
      focus is well described.
  - dataset_rating: 9.0
  - dataset_reason: Time-series and satellite imagery data provided; sensor info and
      collection intervals are explained.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: ROC-AUC, Precision/Recall are appropriate and robust.
  - reference_solution_rating: 1.0
  - reference_solution_reason: No starter model or baseline code linked
  - documentation_rating: 6.5
  - documentation_reason: Moderate Codabench documentation with climate context; lacks
      pipeline-level walkthrough.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: SeafloorAI
  url: https://neurips.cc/virtual/2024/poster/97432
  domain: Marine Science; Vision-Language
  focus: Large-scale vision-language dataset for seafloor mapping and geological classification
  keywords:
  - sonar imagery
  - vision-language
  - seafloor mapping
  - segmentation
  - QA
  description: 'A first-of-its-kind dataset covering 17,300 km² of seafloor with 696K
    sonar images, 827K segmentation masks, and 696K natural-language descriptions
    plus ~7M QA pairs—designed for both vision and language-based ML models in marine
    science :contentReference[oaicite:1]{index=1}.

    '
  task_types:
  - Image segmentation
  - Vision-language QA
  ai_capability_measured: Geospatial understanding, multimodal reasoning
  metrics:
  - Segmentation pixel accuracy
  - QA accuracy
  models:
  - SegFormer
  - ViLT-style multimodal models
  notes: Data processing code publicly available, covering five geological layers;
    curated with marine scientists :contentReference[oaicite:2]{index=2}.
  cite:
  - "@article{nguyen2024seafloorai,\n  title={SeafloorAI: A Large-scale Vision‑Language\
    \ Dataset for Seafloor Geological Survey},\n  author={Nguyen, Kien X. and Qiao,\
    \ Fengchun and others},\n  year={2024},\n  url={https://arxiv.org/abs/2411.00172}\n\
    }\n"
  ML Motif: Vision-Language
  Type: Dataset
  ML task: Segmentation, QA
  Solutions: ~696K images
  Dataset: Sonar imagery + annotations (~15 TB)
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Kien X. Nguyen
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Real-time qubit classification task clearly defined in quantum
      instrumentation context.
  - dataset_rating: 9.0
  - dataset_reason: Dataset available on Zenodo with signal traces; compact and reproducible.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Accuracy and latency are well defined and crucial
      in this setting.
  - reference_solution_rating: 9.0
  - reference_solution_reason: GitHub repo has reproducible code and HLS firmware
      targeting FPGA.
  - documentation_rating: 8.0
  - documentation_reason: Good setup instructions, but no interactive visualization
      or starter notebook.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: SuperCon3D
  url: https://neurips.cc/virtual/2024/poster/97553
  domain: Materials Science; Superconductivity
  focus: Dataset and models for predicting and generating high‑Tc superconductors
    using 3D crystal structures
  keywords:
  - superconductivity
  - crystal structures
  - equivariant GNN
  - generative models
  description: 'SuperCon3D introduces 3D crystal structures with associated critical
    temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model)
    and DiffCSP‑SC (diffusion generator) designed to screen and synthesize high‑Tc
    candidates :contentReference[oaicite:3]{index=3}.

    '
  task_types:
  - Regression (Tc prediction)
  - Generative modeling
  ai_capability_measured: Structure-to-property prediction, structure generation
  metrics:
  - MAE (Tc)
  - Validity of generated structures
  models:
  - SODNet
  - DiffCSP‑SC
  notes: Demonstrates advantage of combining ordered and disordered structural data
    in model design :contentReference[oaicite:4]{index=4}.
  cite:
  - "@article{zhuang2024supercon3d,\n  title={SuperCon3D: Learning Superconductivity\
    \ from Ordered and Disordered Material Structures},\n  author={Zuo, Zhong and\
    \ others},\n  year={2024},\n  note={NeurIPS Poster}\n}\n"
  ML Motif: Materials Modeling
  Type: Dataset + Models
  ML task: Regression, Generation
  Solutions: '2'
  Dataset: 3D crystal + Tc records
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Zhong Zuo
  ratings:
  - problem_spec_rating: 10.0
  - problem_spec_reason: Multimodal task (segmentation + natural language QA pairs);.
  - dataset_rating: 10.0
  - dataset_reason: sonar imagery + masks + descriptions, georeferenced and labeled
      with QA
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Pixel accuracy and QA metrics clearly defined; tasks
      split by modality.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Baseline models (SegFormer, ViLT) are cited, partial
      configs likely available.
  - documentation_rating: 8.5
  - documentation_reason: Paper + GitHub metadata and processing details are comprehensive,
      though full dataset is not yet available.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: GeSS
  url: https://neurips.cc/virtual/2024/poster/97816
  domain: Scientific ML; Geometric Deep Learning
  focus: Benchmark suite evaluating geometric deep learning models under real-world
    distribution shifts
  keywords:
  - geometric deep learning
  - distribution shift
  - OOD robustness
  - scientific applications
  description: 'GeSS provides 30 benchmark scenarios across particle physics, materials
    science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under
    covariate, concept, and conditional shifts, with varied OOD access :contentReference[oaicite:5]{index=5}.

    '
  task_types:
  - Classification
  - Regression
  ai_capability_measured: OOD performance in scientific settings
  metrics:
  - Accuracy
  - RMSE
  - OOD robustness delta
  models:
  - GCN
  - EGNN
  - DimeNet++
  notes: Includes no-OOD, unlabeled-OOD, and few-label scenarios :contentReference[oaicite:6]{index=6}.
  cite:
  - "@article{zou2024gess,\n  title={GeSS: Benchmarking Geometric Deep Learning under\
    \ Scientific Applications with Distribution Shifts},\n  author={Zou, Deyu and\
    \ Liu, Shikun and others},\n  year={2024},\n  note={NeurIPS Poster}\n}\n"
  ML Motif: Geometric DL
  Type: Benchmark
  ML task: Classification, Regression
  Solutions: 30 settings × 11 algos
  Dataset: Scientific graph datasets with shift splits
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Deyu Zou
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Well-defined problem (Tc prediction, generation) with strong
      scientific motivation (high-Tc materials), but no formal hardware constraints.
  - dataset_rating: 9.0
  - dataset_reason: Includes curated 3D crystal structures and Tc data; readily downloadable
      and used in paper models.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: MAE and structural validity used, well-established
      in materials modeling.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Provides two reference models (SODNet, DiffCSP-SC)
      with results. Code likely available post-conference.
  - documentation_rating: 8.0
  - documentation_reason: Paper and poster explain design choices well; software availability
      confirms reproducibility but limited external documentation.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: Vocal Call Locator (VCL)
  url: https://neurips.cc/virtual/2024/poster/97470
  domain: Neuroscience; Bioacoustics
  focus: Benchmarking sound-source localization of rodent vocalizations from multi-channel
    audio
  keywords:
  - source localization
  - bioacoustics
  - time-series
  - SSL
  description: 'The first large-scale benchmark (767K sounds across 9 conditions)
    for localizing rodent vocal calls using synchronized audio and video in standard
    lab environments, enabling systematic evaluation of sound-source localization
    algorithms in bioacoustics :contentReference[oaicite:1]{index=1}.

    '
  task_types:
  - Sound source localization
  ai_capability_measured: Source localization accuracy in bioacoustic settings
  metrics:
  - Localization error (cm)
  - Recall/Precision
  models:
  - CNN-based SSL models
  notes: Dataset spans real, simulated, and mixed audio; supports benchmarking across
    data types :contentReference[oaicite:2]{index=2}.
  cite:
  - "@article{peterson2024vcl,\n  title={Vocal Call Locator Benchmark for localizing\
    \ rodent vocalizations},\n  author={Peterson, Ralph and Tanelus, Aramis and others},\n\
    \  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97470}\n\
    }\n"
  ML Motif: Real-time
  Type: Dataset
  ML task: Anomaly detection / localization
  Solutions: 767,295 sounds
  Dataset: Multi-channel audio + annotations
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Ralph Peterson
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Clear benchmark scenarios across GDL tasks under multiple
      real-world shift settings; OOD settings precisely categorized.
  - dataset_rating: 8.0
  - dataset_reason: Scientific graph datasets provided in multiple shift regimes;
      standardized splits across domains. Exact format of data not specified.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Includes base metrics (accuracy, RMSE) plus OOD delta
      robustness for evaluation under shifts.
  - reference_solution_rating: 9.0
  - reference_solution_reason: Multiple baselines (11 algorithms × 3 backbones) evaluated;
      setup supports reproducible comparison.
  - documentation_rating: 2.0
  - documentation_reason: Paper, poster, and source code provide thorough access to
      methodology and implementation. Setup instructions and accompanying code not
      present.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: MassSpecGym
  url: https://neurips.cc/virtual/2024/poster/97823
  domain: Cheminformatics; Molecular Discovery
  focus: Benchmark suite for discovery and identification of molecules via MS/MS
  keywords:
  - mass spectrometry
  - molecular structure
  - de novo generation
  - retrieval
  - dataset
  description: 'MassSpecGym curates the largest public MS/MS dataset with three standardized
    tasks—de novo structure generation, molecule retrieval, and spectrum simulation—using
    challenging generalization splits to propel ML-driven molecule discovery :contentReference[oaicite:3]{index=3}.

    '
  task_types:
  - De novo generation
  - Retrieval
  - Simulation
  ai_capability_measured: Molecular identification and generation from spectral data
  metrics:
  - Structure accuracy
  - Retrieval precision
  - Simulation MSE
  models:
  - Graph-based generative models
  - Retrieval baselines
  notes: Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark
    for MS/MS tasks :contentReference[oaicite:4]{index=4}.
  cite:
  - "@article{bushuiev2024massspecgym,\n  title={MassSpecGym: A benchmark for the\
    \ discovery and identification of molecules},\n  author={Bushuiev, Roman and Bushuiev,\
    \ Anton and others},\n  year={2024},\n  note={NeurIPS Spotlight Poster},\n  url={https://neurips.cc/virtual/2024/poster/97823}\n\
    }\n"
  ML Motif: Benchmark
  Type: Dataset + Benchmark
  ML task: Generation, retrieval, simulation
  Solutions: ~1M spectra
  Dataset: Public MS/MS spectra with structure annotations
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Roman Bushuiev
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Focused on sound source localization for rodent vocalizations
      in lab settings; well-scoped.
  - dataset_rating: 9.5
  - dataset_reason: 767000 annotated audio segments across diverse conditions. Minor
      deduction for no train/test/valid split.
  - performance_metrics_rating: 9.5
  - performance_metrics_reason: Localization error, precision/recall used
  - reference_solution_rating: 7.0
  - reference_solution_reason: CNN-based baselines referenced but unclear whether
      pretrained models or training code are available.
  - documentation_rating: 2.0
  - documentation_reason: Poster and paper outline benchmark intent and setup; repo
      expected but not confirmed in dataset card.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: Urban Data Layer (UDL)
  url: https://neurips.cc/virtual/2024/poster/97837
  domain: Urban Computing; Data Engineering
  focus: Unified data pipeline for multi-modal urban science research
  keywords:
  - data pipeline
  - urban science
  - multi-modal
  - benchmark
  description: 'UrbanDataLayer standardizes heterogeneous urban data formats and provides
    pipelines for tasks like air quality prediction and land-use classification, enabling
    the rapid creation of multi-modal urban benchmarks :contentReference[oaicite:5]{index=5}.

    '
  task_types:
  - Prediction
  - Classification
  ai_capability_measured: Multi-modal urban inference, standardization
  metrics:
  - Task-specific accuracy or RMSE
  models:
  - Baseline regression/classification pipelines
  notes: Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science
    foundation models :contentReference[oaicite:6]{index=6}.
  cite:
  - "@article{wang2024urbandatalayer,\n  title={UrbanDataLayer: A unified data pipeline\
    \ for urban science},\n  author={Wang, Yiheng and Wang, Tianyu and others},\n\
    \  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97837}\n\
    }\n"
  ML Motif: Data engineering
  Type: Framework
  ML task: Prediction, classification
  Solutions: 4 tasks
  Dataset: Multi-modal urban datasets, standardized
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Yiheng Wang
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: Three tasks (de novo generation, retrieval, simulation) are
      clearly defined for MS/MS molecule discovery.
  - dataset_(fair_principles)_rating: 10.0
  - dataset_(fair_principles)_reason: Over 1 million spectra with structure annotations;
      dataset is open-source and well-documented.
  - performance_metrics_rating: 9.0
  - performance_metrics_reason: Task-appropriate metrics (structure accuracy, precision,
      MSE) are specified and used consistently.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Baseline models are available (graph-based and retrieval),
      though not exhaustive.
  - reproducible_protocol_rating: 9.0
  - reproducible_protocol_reason: GitHub repo and poster provide code and reproducibility
      guidance.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: Delta Squared‑DFT
  url: https://neurips.cc/virtual/2024/poster/97788
  domain: Computational Chemistry; Materials Science
  focus: Benchmarking machine-learning corrections to DFT using Delta Squared-trained
    models for reaction energies
  keywords:
  - density functional theory
  - Delta Squared‑ML correction
  - reaction energetics
  - quantum chemistry
  description: 'Introduces the Delta Squared‑ML paradigm—using ML corrections to DFT
    to predict reaction energies with accuracy comparable to CCSD(T), while training
    on small CC datasets. Evaluated across 10 reaction datasets covering organic and
    organometallic transformations.

    '
  task_types:
  - Regression
  ai_capability_measured: High-accuracy energy prediction, DFT correction
  metrics:
  - Mean Absolute Error (eV)
  - Energy ranking accuracy
  models:
  - Delta Squared‑ML correction networks
  - Kernel ridge regression
  notes: Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly
    included for reproducibility.
  cite:
  - "@article{liu2024delta2dft,\n  title={Delta Squared‑DFT: Machine‑Learning Corrected\
    \ Density Functional Theory for Reaction Energetics},\n  author={Liu, Wei and\
    \ Chen, Rong and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97788}\n\
    }\n"
  ML Motif: Scientific ML
  Type: Dataset + Benchmark
  ML task: Regression
  Solutions: 10 datasets
  Dataset: Reaction energy sets with DFT and high-level references
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Wei Liu
  ratings:
  - problem_spec_rating: 8.0
  - problem_spec_reason: Clear goals around unifying urban data formats and tasks
      (e.g., air quality prediction), though some specifics could be more formal.
  - dataset_(fair_principles)_rating: 9.0
  - dataset_(fair_principles)_reason: Multi-modal data is standardized and accessible;
      GitHub repo available.
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: Uses common task metrics like accuracy/RMSE, though
      varies by task.
  - reference_solution_rating: 7.0
  - reference_solution_reason: Baseline regression/classification models included.
  - reproducible_protocol_rating: 8.0
  - reproducible_protocol_reason: Source code supports pipeline reuse, but formal
      evaluation splits may vary.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: LLMs for Crop Science
  url: https://neurips.cc/virtual/2024/poster/97570
  domain: Agricultural Science; NLP
  focus: Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific
    prompts
  keywords:
  - crop science
  - prompt engineering
  - domain adaptation
  - question answering
  description: 'Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs
    covering crop traits, growth stages, and environmental interactions. Tests GPT-style
    LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and
    retrieval-augmented prompts.

    '
  task_types:
  - Question Answering
  - Inference
  ai_capability_measured: Scientific knowledge, crop reasoning
  metrics:
  - Accuracy
  - F1 score
  models:
  - GPT-4
  - LLaMA-2‑13B
  - T5‑XXL
  notes: Includes examples with retrieval-augmented and chain-of-thought prompt templates;
    supports few-shot adaptation.
  cite:
  - "@article{patel2024llmcropsci,\n  title={Large Language Models for Crop Science:\
    \ Benchmarking Domain Reasoning and QA},\n  author={Patel, Deepak and Zhao, Lan\
    \ and others},\n  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97570}\n\
    }\n"
  ML Motif: NLP
  Type: Dataset
  ML task: QA, inference
  Solutions: 3,500 prompts
  Dataset: Crop science QA dataset
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Deepak Patel
  ratings:
  - problem_spec_rating: 9.0
  - problem_spec_reason: The task of ML correction to DFT energy predictions is well-specified.
  - dataset_(fair_principles)_rating: 9.0
  - dataset_(fair_principles)_reason: 10 public reaction datasets with DFT and CC
      references; well-documented.
  - performance_metrics_rating: 8.0
  - performance_metrics_reason: Uses MAE and ranking accuracy, suitable for this task.
  - reference_solution_rating: 8.0
  - reference_solution_reason: Includes both Δ²‑ML and KRR baselines.
  - reproducible_protocol_rating: 9.0
  - reproducible_protocol_reason: Public benchmarks and clear reproducibility via
      datasets and model code.
- date: '2024-12-13'
  expired: null
  valid: 'yes'
  name: SPIQA (LLM)
  url: https://neurips.cc/virtual/2024/poster/97575
  domain: Multimodal Scientific QA; Computer Vision
  focus: Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter
    performance)
  keywords:
  - multimodal QA
  - scientific figures
  - image+text
  - chain-of-thought prompting
  description: 'A workshop version of SPIQA comparing 10 LLM adapter methods on the
    SPIQA benchmark with scientific diagram/questions. Highlights performance differences
    between chain-of-thought and end-to-end adapter models.

    '
  task_types:
  - Multimodal QA
  ai_capability_measured: Visual reasoning, scientific figure understanding
  metrics:
  - Accuracy
  - F1 score
  models:
  - LLaVA
  - MiniGPT‑4
  - Owl‑LLM adapter variants
  notes: Companion to SPIQA main benchmark; compares adapter strategies using same
    images and QA pairs.
  cite:
  - "@article{zhong2024spiqa_llm,\n  title={SPIQA‑LLM: Evaluating LLM Adapters on\
    \ Scientific Figure QA},\n  author={Zhong, Xiaoyan and Gao, Yijian and others},\n\
    \  year={2024},\n  note={NeurIPS Poster},\n  url={https://neurips.cc/virtual/2024/poster/97575}\n\
    }\n"
  ML Motif: Multimodal QA
  Type: Benchmark
  ML task: Multimodal QA
  Solutions: 10 adapter variants
  Dataset: SPIQA image-question set
  Software: 'Yes'
  Benchmark-Ready: 'Yes'
  Last Updated: 2024-12
  Support Contact Person: Xiaoyan Zhong
  ratings:
  - problem_spec_rating: 6.0
    problem_spec_reason: Task of QA over scientific figures is interesting but not fully formalized in input/output terms.
  - dataset_rating: 6.0
    dataset_reason: Uses SPIQA dataset with ~10 adapters; figures and questions are included, but not fully open.
  - performance_metrics_rating: 7.0
    performance_metrics_reason: Reports accuracy and F1; fair but no visual reasoning-specific metric.
  - reference_solution_rating: 6.0
    reference_solution_reason: 10 LLM adapter baselines; results included.
  - documentation_rating: 5.0
    documentation_reason: Poster paper and limited documentation; no reproducibility instructions.

