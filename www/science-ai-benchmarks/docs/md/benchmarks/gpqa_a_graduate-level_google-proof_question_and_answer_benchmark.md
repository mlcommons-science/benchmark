# GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark

<p><a class="md-button back-link" href="../">‚Üê Back to all benchmarks</a></p>
<div class="info-block meta-block">
  <p class="meta-row"><span class="meta-label">Date</span><span class="meta-sep">:</span> <span class="meta-value">2023-11-20</span></p>
  <p class="meta-row"><span class="meta-label">Name</span><span class="meta-sep">:</span> <span class="meta-value">GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark</span></p>
  <p class="meta-row"><span class="meta-label">Domain</span><span class="meta-sep">:</span> <span class="meta-value">Science  Biology, Physics, Chemistry</span></p>
  <p class="meta-row"><span class="meta-label">Focus</span><span class="meta-sep">:</span> <span class="meta-value">Graduate-level, expert-validated multiple-choice questions hard even with web access</span></p>
  <p class="meta-row"><span class="meta-label">Task Types</span><span class="meta-sep">:</span> <span class="meta-value">Multiple choice</span></p>
  <p class="meta-row"><span class="meta-label">Metrics</span><span class="meta-sep">:</span> <span class="meta-value">Accuracy</span></p>
  <p class="meta-row"><span class="meta-label">Models</span><span class="meta-sep">:</span> <span class="meta-value">GPT-4 baseline</span></p>
</div>
<h3>Keywords</h3>

<div class="chips"><a class="chip chip-link" href="../#kw=Google-proof">Google-proof</a> <a class="chip chip-link" href="../#kw=multiple-choice">multiple-choice</a> <a class="chip chip-link" href="../#kw=expert%20reasoning">expert reasoning</a> <a class="chip chip-link" href="../#kw=science%20QA">science QA</a> </div>
<h3>Citation</h3>

- David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.

<pre><code class="language-bibtex">@misc{rein2023gpqagraduatelevelgoogleproofqa2,
  archiveprefix = {arXiv},
  author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
  eprint        = {2311.12022},
  primaryclass  = {cs.AI},
  title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
  url           = {https://arxiv.org/abs/2311.12022},
  year          = {2023}
}</code></pre>
<h3>Ratings</h3>
<div class="ratings-grid">
  <div class="ratings-head ratings-cell"><span>Category</span><span>Rating</span></div>
  <div class="rating-item">  <div class="rating-cat">Software</div>  <div class="rating-badge">3.00</div>  <div class="rating-bar"><span style="width:60%"></span></div>  <div class="rating-reason">Dataset and benchmark materials are publicly available via HuggingFace and GitHub,
but no integrated runnable code or software framework is provided.
</div></div><div class="rating-item">  <div class="rating-cat">Specification</div>  <div class="rating-badge">5.00</div>  <div class="rating-bar"><span style="width:100%"></span></div>  <div class="rating-reason">Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning.
Input/output formats and evaluation criteria are well described.
</div></div><div class="rating-item">  <div class="rating-cat">Dataset</div>  <div class="rating-badge">5.00</div>  <div class="rating-bar"><span style="width:100%"></span></div>  <div class="rating-reason">The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits.
</div></div><div class="rating-item">  <div class="rating-cat">Metrics</div>  <div class="rating-badge">5.00</div>  <div class="rating-bar"><span style="width:100%"></span></div>  <div class="rating-reason">Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA.
</div></div><div class="rating-item">  <div class="rating-cat">Reference Solution</div>  <div class="rating-badge">1.00</div>  <div class="rating-bar"><span style="width:20%"></span></div>  <div class="rating-reason">No baseline implementations or starter code are linked or provided for reproduction.
</div></div><div class="rating-item">  <div class="rating-cat">Documentation</div>  <div class="rating-badge">3.00</div>  <div class="rating-bar"><span style="width:60%"></span></div>  <div class="rating-reason">Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines.
</div></div>
</div>
<div class="avg-rating">  <strong>Average rating:</strong> <span class="badge badge--meh badge--sm">3.67/5</span></div><h3>Radar plot</h3>

<div class="radar-wrap"><img class="radar-img" alt="GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark radar" src="../../../tex/images/gpqa_a_graduate-level_google-proof_question_and_answer_benchmark_radar.png" /></div>

<p><strong>Edit:</strong> <a href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit this entry</a></p>
